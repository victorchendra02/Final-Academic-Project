{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import perf_counter\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import matplotlib.pyplot as plt\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "\n",
    "kalimat = \"An n times n matrix whose entries come from the set S = {1, 2, ldots , 2n - 1 } is called a silver matrix if, for each i = 1, 2, ldots , n, the i-th row and the i-th column together contain all elements of S. Show that: (a) there is no silver matrix for n = 1997; (b) silver matrices exist for infinitely many values of n.\"\n",
    "print(len(kalimat))\n",
    "print(len(kalimat.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'bert.embeddings.position_ids', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('tbs17/MathBERT', output_hidden_states=True)\n",
    "model = TFBertModel.from_pretrained(\"tbs17/MathBERT\", from_pt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return tokenizer.encode(text, padding='max_length', max_length=512, truncation=True, return_tensors='tf')\n",
    "\n",
    "res = tokenize_text(kalimat)\n",
    "# res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(res['input_ids'], attention_mask=res['attention_mask'])\n",
    "model(\n",
    "    input_ids=tf.keras.Input(shape=(512,), dtype=tf.int32, name=''), \n",
    "    attention_mask=tf.keras.Input(shape=(512,), dtype=tf.int32, name=''),\n",
    "    token_type_ids=tf.keras.Input(shape=(512,), dtype=tf.int32, name='')\n",
    ")\n",
    "# model_ = hub.KerasLayer(model)\n",
    "model(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'bert.embeddings.position_ids', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "class MathBertRegressorModel:\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('tbs17/MathBERT', output_hidden_states=True)\n",
    "        self.encoder = TFBertModel.from_pretrained(\"tbs17/MathBERT\", from_pt=True)\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "        \n",
    "        self.train_ds = None\n",
    "        self.validation_data = None\n",
    "        self.epochs = None\n",
    "        \n",
    "        self.history = None\n",
    "\n",
    "    def tokenize_text(self):\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "        Buat func untuk single text tokenize\n",
    "        \"\"\"\n",
    "        ...\n",
    "        \n",
    "    def tokenize_train_ds(self):\n",
    "        ...\n",
    "\n",
    "    def compile_model(self, train_ds, val_ds, epochs, learning_rate, optimizer_type='adamw', loss=tf.keras.losses.mean_squared_error, metrics=['mae']):\n",
    "        self.train_ds = train_ds\n",
    "        self.validation_data = val_ds\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        steps_per_epoch = tf.data.experimental.cardinality(self.train_ds).numpy()\n",
    "        num_train_steps = steps_per_epoch * self.epochs\n",
    "        optimizer = optimization.create_optimizer(\n",
    "            init_lr=learning_rate,\n",
    "            num_train_steps=num_train_steps,\n",
    "            num_warmup_steps=int(0.1*num_train_steps),\n",
    "            optimizer_type=optimizer_type\n",
    "        )\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    def build_model(self):\n",
    "        input_ids_layer = tf.keras.Input(shape=(512,), dtype=tf.int32, name='input_ids_layer')\n",
    "        attention_mask_layer = tf.keras.Input(shape=(512,), dtype=tf.int32, name='attention_mask_layer')\n",
    "        token_type_ids_layer = tf.keras.Input(shape=(512,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "        inputs = [input_ids_layer, attention_mask_layer, token_type_ids_layer]\n",
    "        \n",
    "        self.encoder.trainable = False\n",
    "        encoded_text = self.encoder(\n",
    "            input_ids=input_ids_layer, \n",
    "            attention_mask=attention_mask_layer,\n",
    "            token_type_ids=token_type_ids_layer)\n",
    "        last_hidden_state = encoded_text['last_hidden_state'][:, 0, :]\n",
    "        \n",
    "        net = tf.keras.layers.Dropout(0.1)(last_hidden_state)\n",
    "        net = tf.keras.layers.Dense(512, activation='relu')(net)\n",
    "        net = tf.keras.layers.Dropout(0.1)(net)\n",
    "        net = tf.keras.layers.Dense(256, activation='relu')(net)\n",
    "        net = tf.keras.layers.Dropout(0.1)(net)\n",
    "        net = tf.keras.layers.Dense(128, activation='relu')(net)\n",
    "        net = tf.keras.layers.Dropout(0.1)(net)\n",
    "        net = tf.keras.layers.Dense(1, activation='linear')(net)\n",
    "    \n",
    "        return tf.keras.Model(inputs=inputs, outputs=net)\n",
    "    \n",
    "    def train(self):\n",
    "        print(\"Start training..\")\n",
    "        start = perf_counter()\n",
    "        history = self.model.fit(\n",
    "            self.train_ds, \n",
    "            validation_data=self.validation_data, \n",
    "            epochs=self.epochs\n",
    "        )\n",
    "        end = perf_counter()\n",
    "        print(f\"\\nTotal training time: {end-start:.2f}s\")\n",
    "\n",
    "        self.history = history\n",
    "        return history\n",
    "    \n",
    "    def evaluate_test(self, test_ds):\n",
    "        loss, mae = self.model.evaluate(test_ds)\n",
    "        return {'loss': loss, 'mae': mae}\n",
    "        \n",
    "    def plot_training_history_over_time(self, figsize=(14, 12)):\n",
    "        print(self.history.keys())\n",
    "        print(\"Training history over time\")\n",
    "        \n",
    "        mae = self.history['mae']\n",
    "        val_acc = self.history['val_mae']\n",
    "        loss = self.history['loss']\n",
    "        val_mae = self.history['val_mae']\n",
    "        epochs = range(1, len(mae) + 1)\n",
    "        \n",
    "        figure, ax = plt.subplots(2, 1, figsize=figsize, layout=\"constrained\")\n",
    "        # loss\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "        plt.plot(epochs, val_mae, 'b', label='Validation loss')\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # mae\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(epochs, mae, 'r', label='Training mae')\n",
    "        plt.plot(epochs, val_acc, 'b', label='Validation mae')\n",
    "        plt.title('Training and validation mae')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.legend()\n",
    "        \n",
    "    \n",
    "tes_model = MathBertRegressorModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
      "array([[  101,  2019,  1050,  2335,  1050,  8185,  3005, 10445,  2272,\n",
      "         2013,  1996,  2275,  1055,  1027,  1063,  1015,  1010,  1016,\n",
      "         1010, 25510, 12868,  1010,  1016,  2078,  1011,  1015,  1065,\n",
      "         2003,  2170,  1037,  3165,  8185,  2065,  1010,  2005,  2169,\n",
      "         1045,  1027,  1015,  1010,  1016,  1010, 25510, 12868,  1010,\n",
      "         1050,  1010,  1996,  1045,  1011, 16215,  5216,  1998,  1996,\n",
      "         1045,  1011, 16215,  5930,  2362,  5383,  2035,  3787,  1997,\n",
      "         1055,  1012,  2265,  2008,  1024,  1006,  1037,  1007,  2045,\n",
      "         2003,  2053,  3165,  8185,  2005,  1050,  1027,  2722,  1025,\n",
      "         1006,  1038,  1007,  3165, 21520,  4839,  2005, 25773,  2116,\n",
      "         5300,  1997,  1050,  1012,   102,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0]])>, 'token_type_ids': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
      "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0]])>}\n",
      "odict_keys(['last_hidden_state', 'pooler_output'])\n",
      "tf.Tensor(\n",
      "[[[-1.8647121  -1.9752758   1.2795306  ...  0.41569513  0.2378122\n",
      "    1.3350254 ]\n",
      "  [ 0.38929352 -0.8751742   0.74196565 ...  0.51497775  0.59667474\n",
      "   -0.7997414 ]\n",
      "  [-0.35375068  0.11389102  0.15809153 ...  0.8368493   1.223067\n",
      "   -0.9928568 ]\n",
      "  ...\n",
      "  [-1.1485913  -0.08123475  0.72744554 ... -1.0094066   0.4700071\n",
      "    0.01841445]\n",
      "  [-0.8421191   0.42261755  1.9361407  ... -0.4174215   0.3731976\n",
      "    0.43502393]\n",
      "  [-1.0777634  -1.6411539   1.8360983  ... -1.447171    1.577932\n",
      "    0.0733182 ]]], shape=(1, 512, 768), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.11337584]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def regression_model(text):\n",
    "    encoded_text = tokenizer(text, padding='max_length', max_length=512, truncation=True ,return_tensors='tf')\n",
    "    print(encoded_text)\n",
    "    outputs = model(encoded_text)\n",
    "    print(outputs.keys())\n",
    "    print(outputs['last_hidden_state'])\n",
    "\n",
    "    last_hidden_state = outputs['last_hidden_state'][:, 0, :]  # Get CLS token representation\n",
    "    net = tf.keras.layers.Dropout(0.1)(last_hidden_state)\n",
    "    net = tf.keras.layers.Dense(512, activation='relu')(net)\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(256, activation='relu')(net)\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(128, activation='relu')(net)\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation='linear')(net)\n",
    "\n",
    "    return net\n",
    "\n",
    "regression_model(kalimat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfhub_handle_preprocess = \"https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-cased-preprocess/versions/3\"\n",
    "tfhub_handle_encoder    = \"https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-cased-l-12-h-768-a-12/versions/4\"\n",
    "\n",
    "tokenizer_TFHUB = hub.load(tfhub_handle_preprocess)\n",
    "model_TFHUB = hub.load(tfhub_handle_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
       " array([[[ 0.3765292 ,  0.0570243 , -0.06860661, ...,  0.03646583,\n",
       "           0.03290379, -0.15170978],\n",
       "         [-0.6161929 , -0.12232574,  0.26684394, ..., -0.25346434,\n",
       "           0.35446113,  0.20755616],\n",
       "         [-0.8727162 ,  0.6887318 ,  0.2333233 , ...,  0.11457242,\n",
       "           1.0981388 ,  0.30038944],\n",
       "         ...,\n",
       "         [-0.4910985 ,  0.03118425,  0.2841043 , ..., -0.13263746,\n",
       "          -0.03974351,  0.1775467 ],\n",
       "         [-0.6594098 , -0.17344199,  0.14063303, ..., -0.1556083 ,\n",
       "           0.09107862,  0.41679722],\n",
       "         [-0.45056465,  0.22565478,  0.21684526, ..., -0.46307218,\n",
       "           0.12560755,  0.3367008 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
       " array([[[ 6.0117072e-01,  4.3541983e-02, -7.3847249e-02, ...,\n",
       "          -1.9527021e-01, -1.3143281e-03, -4.4605013e-02],\n",
       "         [ 1.4845307e-01, -2.0472026e-01,  2.7761897e-01, ...,\n",
       "          -6.4247358e-01,  6.2928402e-01,  2.0168100e-01],\n",
       "         [-3.8816035e-01,  2.2168748e-02,  1.4957234e-01, ...,\n",
       "           5.4846543e-01,  1.2630876e+00,  8.6462474e-01],\n",
       "         ...,\n",
       "         [ 1.3004419e-01,  2.5146723e-02,  1.5899575e-01, ...,\n",
       "          -3.2041791e-01,  2.4891962e-01, -1.0600408e-01],\n",
       "         [-1.0427038e-01,  9.5985830e-04,  1.4351410e-01, ...,\n",
       "          -3.1062412e-01,  1.9214432e-01, -1.1990209e-01],\n",
       "         [ 1.4903925e-01,  1.4150824e-01,  1.4711843e-01, ...,\n",
       "          -5.4313475e-01,  2.6129618e-01, -8.8129699e-02]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
       " array([[[ 0.73372203,  0.18691498, -0.34645632, ..., -0.1759085 ,\n",
       "          -0.0572914 , -0.07436418],\n",
       "         [ 0.29348138,  0.22948109,  0.38099125, ..., -0.87492067,\n",
       "           0.5331389 ,  0.16463077],\n",
       "         [ 0.13684902,  0.47854543,  0.28830674, ...,  0.5542247 ,\n",
       "           0.994245  ,  0.67379963],\n",
       "         ...,\n",
       "         [ 0.19243394,  0.09898021, -0.09803756, ..., -0.23199646,\n",
       "           0.5199216 , -0.35973698],\n",
       "         [-0.07154028,  0.0623596 ,  0.14379543, ..., -0.23947622,\n",
       "           0.14292859, -0.29744494],\n",
       "         [ 0.1225979 ,  0.21153164,  0.07990498, ..., -0.48119998,\n",
       "           0.27490532, -0.2704412 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
       " array([[[ 0.50884324,  0.28491434, -0.39744222, ...,  0.01212564,\n",
       "          -0.02460098, -0.11997587],\n",
       "         [ 0.3126076 , -0.06216961,  0.36652485, ..., -0.2959793 ,\n",
       "           0.8509807 , -0.03478417],\n",
       "         [ 0.16285886,  0.6824113 ,  0.11120689, ...,  0.90446925,\n",
       "           0.8403726 ,  0.86506975],\n",
       "         ...,\n",
       "         [ 0.09462427,  0.36913878,  0.06725626, ..., -0.16771638,\n",
       "           0.5668555 , -0.5569057 ],\n",
       "         [-0.07074393,  0.14608672,  0.37777779, ..., -0.18225762,\n",
       "           0.28964737, -0.40540916],\n",
       "         [ 0.07682618,  0.22871491,  0.3130944 , ..., -0.33607835,\n",
       "           0.40741423, -0.19027942]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
       " array([[[ 0.7076343 ,  0.2700751 , -0.71510905, ...,  0.16976996,\n",
       "          -0.0228866 , -0.00502998],\n",
       "         [ 0.0295887 ,  0.2713766 ,  0.8877188 , ..., -0.41175085,\n",
       "           1.2385542 ,  0.32393333],\n",
       "         [ 0.6321878 ,  0.74056506, -0.10069102, ...,  0.5872337 ,\n",
       "           1.3683896 ,  1.0357289 ],\n",
       "         ...,\n",
       "         [ 0.43683243,  0.4774983 ,  0.15343875, ...,  0.13751654,\n",
       "           0.74162227, -0.48407036],\n",
       "         [ 0.41450283,  0.2831375 ,  0.36789215, ...,  0.01460243,\n",
       "           0.648083  , -0.4418322 ],\n",
       "         [ 0.47078824,  0.3968996 ,  0.32126066, ..., -0.07715909,\n",
       "           0.73975724, -0.25891772]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
       " array([[[ 0.6060477 ,  0.4679265 , -0.7936161 , ..., -0.15302505,\n",
       "           0.09834006,  0.20523061],\n",
       "         [-0.41643503,  0.4163305 ,  0.6089176 , ..., -0.06105687,\n",
       "           0.97869617,  0.13348001],\n",
       "         [ 0.62715286,  0.7030128 , -0.15396419, ...,  0.8963719 ,\n",
       "           0.6691629 ,  0.9715453 ],\n",
       "         ...,\n",
       "         [ 0.16456752,  0.62047964,  0.30772528, ...,  0.23502642,\n",
       "           0.39555165, -0.69238794],\n",
       "         [ 0.41244146,  0.20710334,  0.4917288 , ...,  0.18156663,\n",
       "           0.67477477, -0.33394063],\n",
       "         [ 0.5612334 ,  0.31197602,  0.38377407, ...,  0.1201672 ,\n",
       "           0.7628033 , -0.19881526]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
       " array([[[ 0.77969956,  0.35621727, -0.8051825 , ..., -0.39047885,\n",
       "           0.07344758,  0.16384399],\n",
       "         [-0.49751258,  0.61972505,  0.4593332 , ..., -0.32154134,\n",
       "           0.86349064, -0.18697304],\n",
       "         [ 0.14601287,  0.59802806, -0.14316276, ...,  0.98164046,\n",
       "           0.82397604,  0.93588346],\n",
       "         ...,\n",
       "         [-0.00513637,  0.7024809 ,  0.24231127, ...,  0.1435426 ,\n",
       "           0.06920198, -0.58769625],\n",
       "         [ 0.40006396,  0.27775982,  0.17928818, ...,  0.17492144,\n",
       "           0.72598916, -0.39282572],\n",
       "         [ 0.587328  ,  0.31702292,  0.04871627, ...,  0.0726209 ,\n",
       "           0.8501153 , -0.18691513]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
       " array([[[ 0.547217  ,  0.28292242, -0.61880195, ..., -0.2694588 ,\n",
       "          -0.1047217 ,  0.11368942],\n",
       "         [-0.88432115,  0.6871267 ,  0.9842586 , ...,  0.08814965,\n",
       "           0.97662127, -0.80339074],\n",
       "         [ 0.3077701 ,  0.61950064,  0.09426211, ...,  1.2399302 ,\n",
       "           0.7866838 ,  0.7898263 ],\n",
       "         ...,\n",
       "         [ 0.15326464,  1.079101  , -0.02254857, ...,  0.5588329 ,\n",
       "           0.40597612, -0.79663986],\n",
       "         [ 0.6175622 ,  0.80700415,  0.15254901, ...,  0.14960894,\n",
       "           0.92887664, -0.38584936],\n",
       "         [ 0.79643685,  0.7150733 ,  0.07409817, ...,  0.07949351,\n",
       "           0.92366034, -0.1812347 ]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
       " array([[[ 0.8846333 ,  0.00822563, -0.41988865, ..., -0.4114674 ,\n",
       "           0.16149572,  0.55028105],\n",
       "         [-1.1313275 ,  0.2433164 ,  1.1065398 , ...,  0.01123969,\n",
       "           0.2934427 , -1.2935259 ],\n",
       "         [ 0.24905214,  0.49723005,  0.37903762, ...,  1.1217821 ,\n",
       "           0.79262495,  1.0512867 ],\n",
       "         ...,\n",
       "         [ 0.5849649 ,  0.9095083 , -0.6003717 , ...,  0.75328606,\n",
       "          -0.08624244, -0.8615667 ],\n",
       "         [ 0.7275635 ,  0.52868277,  0.09618474, ...,  0.30298212,\n",
       "           0.5962138 , -0.6526814 ],\n",
       "         [ 0.82034546,  0.42824468,  0.14436346, ...,  0.22698405,\n",
       "           0.6144902 , -0.43945327]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
       " array([[[ 1.0324373 ,  0.07337765, -0.50439227, ..., -0.48572484,\n",
       "           0.24420178,  0.58926404],\n",
       "         [-0.6643173 , -0.41935498,  1.1860882 , ..., -0.47298712,\n",
       "          -0.2727283 , -0.6757303 ],\n",
       "         [ 0.27430838,  0.92298895,  0.05226858, ...,  1.128521  ,\n",
       "           0.852634  ,  0.84870124],\n",
       "         ...,\n",
       "         [ 0.57556266,  0.48753035, -0.52915645, ...,  1.2961657 ,\n",
       "          -0.12327997, -0.664919  ],\n",
       "         [ 0.74954474,  0.17043614,  0.38530004, ...,  0.76984024,\n",
       "           0.36547896, -0.64838177],\n",
       "         [ 0.8548831 ,  0.17808804,  0.49497163, ...,  0.5724101 ,\n",
       "           0.3071435 , -0.39492553]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
       " array([[[ 1.4540629 , -0.13654497,  0.10296646, ..., -0.32564908,\n",
       "           0.22904715,  0.57843316],\n",
       "         [-0.64318013, -0.85879505,  1.5691509 , ..., -0.57133466,\n",
       "          -0.19977307, -0.5300168 ],\n",
       "         [ 0.55794376,  1.1305094 ,  0.03790968, ...,  0.7637826 ,\n",
       "           0.5543427 ,  1.0916058 ],\n",
       "         ...,\n",
       "         [ 0.82972753,  0.43021262, -0.18805934, ...,  1.5961189 ,\n",
       "          -0.4525682 , -0.8066297 ],\n",
       "         [ 0.79184914, -0.03348944,  0.25290126, ...,  1.1479062 ,\n",
       "           0.07586223, -0.49858245],\n",
       "         [ 0.8439187 , -0.02730516,  0.42922878, ...,  0.8451839 ,\n",
       "           0.21165702, -0.17781147]]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
       " array([[[ 0.8778735 ,  0.02790615,  0.2910243 , ..., -0.10739107,\n",
       "           0.04316454,  0.22288828],\n",
       "         [ 0.3165699 , -0.46106875,  0.71183807, ..., -0.28792626,\n",
       "          -0.04725548,  0.09184892],\n",
       "         [ 0.40817356,  0.5260925 ,  0.20781934, ...,  0.18058844,\n",
       "           0.05632091,  1.1079692 ],\n",
       "         ...,\n",
       "         [ 0.3936262 ,  0.13355641,  0.03347414, ...,  0.64386475,\n",
       "          -0.10042323, -0.26250404],\n",
       "         [ 0.34473252, -0.09429996,  0.16362582, ...,  0.46367878,\n",
       "           0.20962182, -0.07190164],\n",
       "         [ 0.36193585, -0.07649568,  0.20299432, ...,  0.39783695,\n",
       "           0.27095965,  0.06479928]]], dtype=float32)>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['sequence_output', 'encoder_outputs', 'default', 'pooled_output']\n",
    "\n",
    "outputtt = model_TFHUB(tokenizer_TFHUB([kalimat]))\n",
    "outputtt['encoder_outputs']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x2180a8203d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "    # Step 1: Define input layer\n",
    "    input_ids_KERAS_INPUT = tf.keras.Input(shape=(), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask_KERAS_INPUT = tf.keras.Input(shape=(), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    # BERT encoding\n",
    "    outputs = model(input_ids_KERAS_INPUT, attention_mask=attention_mask_KERAS_INPUT)\n",
    "    pooled_output = outputs.last_hidden_state[:, 0, :]  # Use CLS token\n",
    "    # # Step 2: Tokenize batches of text inputs\n",
    "    # bert_preprocess = hub.load(\"https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-cased-preprocess/versions/3\")\n",
    "    # tokenize = hub.KerasLayer(bert_preprocess.tokenize)\n",
    "    # tokenized_input = tokenize(text_input)\n",
    "    \n",
    "    # # Step 3: Pack input sequences for the Transformer encoder\n",
    "    # bert_pack_inputs = hub.KerasLayer(\n",
    "    #     bert_preprocess.bert_pack_inputs,\n",
    "    #     arguments=dict(seq_length=512))\n",
    "    # encoder_inputs = bert_pack_inputs([tokenized_input])\n",
    "    \n",
    "    # Load BERT encoder\n",
    "    encoder = hub.KerasLayer(\"https://www.kaggle.com/models/tensorflow/bert/frameworks/TensorFlow2/variations/en-cased-l-12-h-768-a-12/versions/4\", trainable=True, name='BERT_encoder')\n",
    "    # Pass encoder inputs through BERT encoder\n",
    "    outputs = encoder(encoder_inputs)\n",
    "\n",
    "    # Define classifier layers\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(512, activation='relu')(net)  # Additional dense layer\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(256, activation='relu')(net)  # Additional dense layer\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(128, activation='relu')(net)  # Additional dense layer\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(1, activation='linear', name='regressor')(net)\n",
    "    \n",
    "    return tf.keras.Model(text_input, net)\n",
    "\n",
    "\n",
    "build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output keys                     -> ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "input_word_ids _ input_ids      -> [  101  5646  2035  4972  1042  1024  8785 10322  1054  2000  8785 10322]\n",
      "input_mask _ attention_mask     -> [1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "input_type_ids _ token_type_ids -> [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "shape: (1, 512)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Determine all functions f : mathbb R to mathbb R satisfying the following two conditions: (a) f(x + y) + f(x - y) = 2f(x)f(y) for all x, y in mathbb R, and (b) lim_{x to infty} f(x) = 0.\"\n",
    "encoded_inputs = tokenizer(example_text, return_tensors=\"tf\", padding=\"max_length\", max_length=512, truncation=True)\n",
    "\n",
    "print(f\"output keys                     -> {list(encoded_inputs.keys())}\")\n",
    "print(f\"input_word_ids _ input_ids      -> {encoded_inputs['input_ids'][0, :12]}\")\n",
    "print(f\"input_mask _ attention_mask     -> {encoded_inputs['attention_mask'][0, :12]}\")\n",
    "print(f\"input_type_ids _ token_type_ids -> {encoded_inputs['token_type_ids'][0, :12]}\")\n",
    "print(f\"shape: {encoded_inputs['attention_mask'].shape}\\n\")\n",
    "\n",
    "# output = model(encoded_inputs)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT               --> 'hElLO an bisa the aku'\n",
      "tokenize                 --> ['hello', 'an', 'bis', '##a', 'the', 'ak', '##u']\n",
      "decode                   --> [CLS] hello an bisa the aku [SEP] [PAD] [PAD] [PAD]\n",
      "encode                   --> [101, 7592, 2019, 20377, 2050, 1996, 17712, 2226, 102]\n",
      "convert_ids_to_tokens    --> ['[CLS]', 'hello', 'an', 'bis', '##a', 'the', 'ak', '##u', '[SEP]']\n",
      "convert_tokens_to_ids    --> [101, 7592, 2019, 20377, 2050, 1996, 17712, 2226, 102]\n",
      "convert_tokens_to_string --> hello an bisa the aku\n",
      "\n",
      "cls_token : [CLS]  - cls_token_id: 101\n",
      "mask_token: [MASK] - mask_token_id: 103\n",
      "pad_token : [PAD]  - pad_token_id: 0 - pad_token_type_id: 0\n",
      "unk_token : [UNK]  - unk_token_id: 100\n",
      "sep_token : [SEP]  - sep_token_id: 102\n",
      "\n",
      "all_special_ids             --> [100, 102, 0, 101, 103]\n",
      "all_special_tokens          --> ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "all_special_tokens_extended --> ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "\n",
      "name_or_path              --> tbs17/MathBERT\n",
      "vocab_size                --> 30522\n",
      "model_max_length          --> 1000000000000000019884624838656\n",
      "model_input_names         --> ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "prepare_for_model         --> {'input_ids': [101, 101, 7592, 2019, 20377, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "SPECIAL_TOKENS_ATTRIBUTES --> ['bos_token', 'eos_token', 'unk_token', 'sep_token', 'pad_token', 'cls_token', 'mask_token', 'additional_special_tokens']\n"
     ]
    }
   ],
   "source": [
    "example_text = 'hElLO an bisa the aku'\n",
    "encoded_inputs = tokenizer(example_text, return_tensors=\"tf\", padding=\"max_length\", max_length=12, truncation=True)\n",
    "\n",
    "print(f\"INPUT TEXT               --> '{example_text}'\")\n",
    "print(f\"tokenize                 --> {tokenizer.tokenize(example_text)}\")\n",
    "print(f\"decode                   --> {tokenizer.decode(encoded_inputs['input_ids'][0])}\")\n",
    "print(f\"encode                   --> {tokenizer.encode(example_text)}\")\n",
    "print(f\"convert_ids_to_tokens    --> {tokenizer.convert_ids_to_tokens([101, 7592, 2019, 20377, 2050, 1996, 17712, 2226, 102])}\")\n",
    "print(f\"convert_tokens_to_ids    --> {tokenizer.convert_tokens_to_ids(['[CLS]', 'hello', 'an', 'bis', '##a', 'the', 'ak', '##u', '[SEP]'])}\")\n",
    "print(f\"convert_tokens_to_string --> {tokenizer.convert_tokens_to_string(['hello', 'an', 'bis', '##a', 'the', 'ak', '##u'])}\")\n",
    "\n",
    "print()\n",
    "print(f\"cls_token : {tokenizer.cls_token}  - cls_token_id: {tokenizer.cls_token_id}\")\n",
    "print(f\"mask_token: {tokenizer.mask_token} - mask_token_id: {tokenizer.mask_token_id}\")\n",
    "print(f\"pad_token : {tokenizer.pad_token}  - pad_token_id: {tokenizer.pad_token_id} - pad_token_type_id: {tokenizer.pad_token_type_id}\")\n",
    "print(f\"unk_token : {tokenizer.unk_token}  - unk_token_id: {tokenizer.unk_token_id}\")\n",
    "print(f\"sep_token : {tokenizer.sep_token}  - sep_token_id: {tokenizer.sep_token_id}\")\n",
    "print()\n",
    "print(f\"all_special_ids             --> {tokenizer.all_special_ids}\")\n",
    "print(f\"all_special_tokens          --> {tokenizer.all_special_tokens}\")\n",
    "print(f\"all_special_tokens_extended --> {tokenizer.all_special_tokens_extended}\")\n",
    "print()\n",
    "print(f\"name_or_path              --> {tokenizer.name_or_path}\")\n",
    "print(f\"vocab_size                --> {tokenizer.vocab_size}\")\n",
    "print(f\"model_max_length          --> {tokenizer.model_max_length}\")\n",
    "print(f\"model_input_names         --> {tokenizer.model_input_names}\")\n",
    "print(f\"prepare_for_model         --> {tokenizer.prepare_for_model([101, 7592, 2019, 20377])}\")\n",
    "\n",
    "print(f\"SPECIAL_TOKENS_ATTRIBUTES --> {tokenizer.SPECIAL_TOKENS_ATTRIBUTES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare & Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE  # Output: -1 --> <class 'int'>\n",
    "batch_size = 16\n",
    "seed = 42\n",
    "\n",
    "\n",
    "from utils.load_dataset_for_regression_model import load_and_prepare_dataset\n",
    "\n",
    "train_ds, val_ds, test_ds = load_and_prepare_dataset(\"../data/regression/imo/\", seed=seed, batch_size=batch_size, AUTOTUNE=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathBertRegressorModel:\n",
    "    def __init__(self, seq_length=512):\n",
    "        self.tokenizer_MathBERT = BertTokenizer.from_pretrained('tbs17/MathBERT', output_hidden_states=True)\n",
    "        # self.model_MathBERT = TFBertModel.from_pretrained(\"tbs17/MathBERT\", from_pt=True)\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "        self.model = self.__build_model()\n",
    "        self.history = None\n",
    "        \n",
    "        self.train_ds = None\n",
    "        self.validation_data = None\n",
    "        self.epochs = None\n",
    "        self.learning_rate = None\n",
    "        self.optimizer_type = None\n",
    "        self.loss = None\n",
    "        self.metrics = None\n",
    "        self.optimizer = None\n",
    "        self.__is_compiled = False\n",
    "        \n",
    "        self.__is_trained = False\n",
    "\n",
    "    def __build_model(self):\n",
    "        # Step 1: Define text input layer\n",
    "        text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "        \n",
    "        # Step 2: Tokenize and prepare input sequences\n",
    "        def tokenize_text(text: str):\n",
    "            inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.seq_length, return_tensors=\"tf\")\n",
    "            return {\n",
    "                'input_ids': inputs['input_ids'],\n",
    "                'attention_mask': inputs['attention_mask']\n",
    "            }\n",
    "            \n",
    "        tokenized_input = tf.keras.layers.Lambda(tokenize_text, output_shape=(self.seq_length,), dtype='int32')(text_input)\n",
    "\n",
    "        # Step 3: Load BERT model\n",
    "        bert_model = TFBertModel.from_pretrained('tbs17/MathBERT', from_pt=True)\n",
    "\n",
    "        # Step 4: Pass inputs through BERT model\n",
    "        bert_output = bert_model(tokenized_input)\n",
    "\n",
    "        # Additional dense layers for regression\n",
    "        pooled_output = bert_output.pooler_output\n",
    "        net = tf.keras.layers.Dense(512, activation='relu')(pooled_output)  # Additional dense layer\n",
    "        net = tf.keras.layers.Dropout(0.1)(net)\n",
    "        net = tf.keras.layers.Dense(256, activation='relu')(net)  # Additional dense layer\n",
    "        net = tf.keras.layers.Dropout(0.1)(net)\n",
    "        net = tf.keras.layers.Dense(128, activation='relu')(net)  # Additional dense layer\n",
    "        net = tf.keras.layers.Dropout(0.1)(net)\n",
    "        net = tf.keras.layers.Dense(1, activation='linear', name='regression_output')(net)\n",
    "\n",
    "        return tf.keras.Model(text_input, net)\n",
    "    \n",
    "    def compile_model(\n",
    "        self, \n",
    "        train_ds, \n",
    "        validation_data,\n",
    "        epochs, \n",
    "        learning_rate, \n",
    "        optimizer_type='adamw', \n",
    "        loss=tf.keras.losses.mean_squared_error,\n",
    "        metrics=['mae']):\n",
    "        \n",
    "        self.train_ds = train_ds\n",
    "        self.validation_data = validation_data\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_type = optimizer_type\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        \n",
    "        steps_per_epoch = tf.data.experimental.cardinality(self.train_ds).numpy()\n",
    "        num_train_steps = steps_per_epoch * self.epochs\n",
    "        optimizer = optimization.create_optimizer(\n",
    "            init_lr=self.learning_rate,\n",
    "            num_train_steps=num_train_steps,\n",
    "            num_warmup_steps=int(0.1*num_train_steps),\n",
    "            optimizer_type=self.optimizer_type\n",
    "        )\n",
    "        self.optimizer= optimizer\n",
    "        \n",
    "        # Comple model\n",
    "        self.model.compile(optimizer=self.optimizer, loss=self.loss, metrics=self.metrics)\n",
    "        self.__is_compiled = True\n",
    "    \n",
    "    def train(self):\n",
    "        if self.__is_compiled is False: raise Warning(\"Model is not compiled yet\")\n",
    "        \n",
    "        start = perf_counter()\n",
    "        history = self.model.fit(\n",
    "            self.train_ds, \n",
    "            validation_data=self.validation_data, \n",
    "            epochs=self.epochs\n",
    "            )\n",
    "        end = perf_counter()\n",
    "        print(f\"\\nTotal training time: {end-start:.2f}s\")\n",
    "        \n",
    "        self.history = history.history\n",
    "        self.__is_trained = True\n",
    "        return history\n",
    "    \n",
    "    def evaluate_test(self, test_ds):\n",
    "        if self.__is_trained is False: raise Warning(\"Model is not trained yet\")\n",
    "\n",
    "        loss, mae = self.model.evaluate(test_ds)\n",
    "\n",
    "        return {'loss': loss, 'mae': mae}\n",
    "    \n",
    "    def predict(self, texts):\n",
    "        # Tokenize input texts\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        \n",
    "        for text in texts:\n",
    "            encoded_dict = self.BERTtokenizer.encode_plus(\n",
    "                text, \n",
    "                add_special_tokens=True, \n",
    "                max_length=self.seq_length, \n",
    "                padding='max_length', \n",
    "                return_attention_mask=True, \n",
    "                return_tensors='tf', \n",
    "                truncation=True\n",
    "            )\n",
    "            input_ids.append(encoded_dict['input_ids'])\n",
    "            attention_masks.append(encoded_dict['attention_mask'])\n",
    "        \n",
    "        input_ids = tf.concat(input_ids, axis=0)\n",
    "        attention_masks = tf.concat(attention_masks, axis=0)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = self.model.predict([input_ids, attention_masks])\n",
    "        return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "learning_rate = 1e-6\n",
    "optimizer_type = 'adamw'\n",
    "loss = tf.keras.losses.mean_squared_error\n",
    "metrics = ['mae']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751490f685504088a66969cccba9a518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IBDA\\.conda\\envs\\victor_aops_mathbert\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\IBDA\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8addad21261141d485f4bdf1a0f463f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df22b617b4f74a6985f18d3ec7832652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/569 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6984db2f67d545edae9a3dee9ee3d139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/441M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'bert.embeddings.position_ids', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "math_bert = MathBertRegressorModel()\n",
    "math_bert.compile_model(\n",
    "    train_ds=train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs, \n",
    "    learning_rate=learning_rate, \n",
    "    optimizer_type=optimizer_type, \n",
    "    loss=loss, \n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\IBDA\\.conda\\envs\\victor_aops_mathbert\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\IBDA\\.conda\\envs\\victor_aops_mathbert\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\IBDA\\.conda\\envs\\victor_aops_mathbert\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\IBDA\\.conda\\envs\\victor_aops_mathbert\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\IBDA\\.conda\\envs\\victor_aops_mathbert\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\IBDA\\.conda\\envs\\victor_aops_mathbert\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=string>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmath_bert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36mMathBertRegressorModel.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_compiled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mWarning\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel is not compiled yet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m start \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[1;32m---> 83\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m end \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal training time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\IBDA\\.conda\\envs\\victor_aops_mathbert\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file308mrn9d.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\IBDA\\.conda\\envs\\victor_aops_mathbert\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\IBDA\\.conda\\envs\\victor_aops_mathbert\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\IBDA\\.conda\\envs\\victor_aops_mathbert\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\IBDA\\.conda\\envs\\victor_aops_mathbert\\lib\\site-packages\\keras\\engine\\training.py\", line 993, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\IBDA\\.conda\\envs\\victor_aops_mathbert\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\IBDA\\.conda\\envs\\victor_aops_mathbert\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 216, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Layer \"model\" expects 2 input(s), but it received 1 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None,) dtype=string>]\n"
     ]
    }
   ],
   "source": [
    "math_bert.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_bert.plot_training_history_over_time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    def plot_training_history_over_time(self, figsize=(14, 12)):\n",
    "        if self.history is None: raise Warning(\"Nothing to plot because model is not trained yet\")\n",
    "        \n",
    "        print(self.history.keys())\n",
    "        print(\"Training history over time\")\n",
    "        \n",
    "        mae = self.history['mae']\n",
    "        val_acc = self.history['val_mae']\n",
    "        loss = self.history['loss']\n",
    "        val_mae = self.history['val_mae']\n",
    "        epochs = range(1, len(mae) + 1)\n",
    "        \n",
    "        figure, ax = plt.subplots(2, 1, figsize=figsize, layout=\"constrained\")\n",
    "\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(epochs, loss, 'r', label='Training loss')  # r is for \"solid red line\"\n",
    "        plt.plot(epochs, val_mae, 'b', label='Validation loss')  # b is for \"solid blue line\"\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(epochs, mae, 'r', label='Training mae')\n",
    "        plt.plot(epochs, val_acc, 'b', label='Validation mae')\n",
    "        plt.title('Training and validation mae')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aops_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
