{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import perf_counter\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "import matplotlib.pyplot as plt\n",
    "from official.nlp import optimization  # to create AdamW optimizer\n",
    "\n",
    "from transformers import BertTokenizer, TFBertModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('tbs17/MathBERT', output_hidden_states=True)\n",
    "encoder = TFBertModel.from_pretrained(\"tbs17/MathBERT\", from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output keys                     -> ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "input_word_ids _ input_ids      -> [  101  5646  2035  4972  1042  1024  8785 10322  1054  2000  8785 10322]\n",
      "input_mask _ attention_mask     -> [1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "input_type_ids _ token_type_ids -> [0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "shape: (1, 512)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "example_text = \"Determine all functions f : mathbb R to mathbb R satisfying the following two conditions: (a) f(x + y) + f(x - y) = 2f(x)f(y) for all x, y in mathbb R, and (b) lim_{x to infty} f(x) = 0.\"\n",
    "encoded_inputs = tokenizer(example_text, return_tensors=\"tf\", padding=\"max_length\", max_length=512, truncation=True)\n",
    "\n",
    "print(f\"output keys                     -> {list(encoded_inputs.keys())}\")\n",
    "print(f\"input_word_ids _ input_ids      -> {encoded_inputs['input_ids'][0, :12]}\")\n",
    "print(f\"input_mask _ attention_mask     -> {encoded_inputs['attention_mask'][0, :12]}\")\n",
    "print(f\"input_type_ids _ token_type_ids -> {encoded_inputs['token_type_ids'][0, :12]}\")\n",
    "print(f\"shape: {encoded_inputs['attention_mask'].shape}\\n\")\n",
    "\n",
    "# output = model(encoded_inputs)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT TEXT               --> 'hElLO an bisa the aku'\n",
      "tokenize                 --> ['hello', 'an', 'bis', '##a', 'the', 'ak', '##u']\n",
      "decode                   --> [CLS] hello an bisa the aku [SEP] [PAD] [PAD] [PAD]\n",
      "encode                   --> [101, 7592, 2019, 20377, 2050, 1996, 17712, 2226, 102]\n",
      "convert_ids_to_tokens    --> ['[CLS]', 'hello', 'an', 'bis', '##a', 'the', 'ak', '##u', '[SEP]']\n",
      "convert_tokens_to_ids    --> [101, 7592, 2019, 20377, 2050, 1996, 17712, 2226, 102]\n",
      "convert_tokens_to_string --> hello an bisa the aku\n",
      "\n",
      "cls_token : [CLS]  - cls_token_id: 101\n",
      "mask_token: [MASK] - mask_token_id: 103\n",
      "pad_token : [PAD]  - pad_token_id: 0 - pad_token_type_id: 0\n",
      "unk_token : [UNK]  - unk_token_id: 100\n",
      "sep_token : [SEP]  - sep_token_id: 102\n",
      "\n",
      "all_special_ids             --> [100, 102, 0, 101, 103]\n",
      "all_special_tokens          --> ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "all_special_tokens_extended --> ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "\n",
      "name_or_path              --> tbs17/MathBERT\n",
      "vocab_size                --> 30522\n",
      "model_max_length          --> 1000000000000000019884624838656\n",
      "model_input_names         --> ['input_ids', 'token_type_ids', 'attention_mask']\n",
      "prepare_for_model         --> {'input_ids': [101, 101, 7592, 2019, 20377, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
      "SPECIAL_TOKENS_ATTRIBUTES --> ['bos_token', 'eos_token', 'unk_token', 'sep_token', 'pad_token', 'cls_token', 'mask_token', 'additional_special_tokens']\n"
     ]
    }
   ],
   "source": [
    "example_text = 'hElLO an bisa the aku'\n",
    "encoded_inputs = tokenizer(example_text, return_tensors=\"tf\", padding=\"max_length\", max_length=12, truncation=True)\n",
    "\n",
    "print(f\"INPUT TEXT               --> '{example_text}'\")\n",
    "print(f\"tokenize                 --> {tokenizer.tokenize(example_text)}\")\n",
    "print(f\"decode                   --> {tokenizer.decode(encoded_inputs['input_ids'][0])}\")\n",
    "print(f\"encode                   --> {tokenizer.encode(example_text)}\")\n",
    "print(f\"convert_ids_to_tokens    --> {tokenizer.convert_ids_to_tokens([101, 7592, 2019, 20377, 2050, 1996, 17712, 2226, 102])}\")\n",
    "print(f\"convert_tokens_to_ids    --> {tokenizer.convert_tokens_to_ids(['[CLS]', 'hello', 'an', 'bis', '##a', 'the', 'ak', '##u', '[SEP]'])}\")\n",
    "print(f\"convert_tokens_to_string --> {tokenizer.convert_tokens_to_string(['hello', 'an', 'bis', '##a', 'the', 'ak', '##u'])}\")\n",
    "\n",
    "print()\n",
    "print(f\"cls_token : {tokenizer.cls_token}  - cls_token_id: {tokenizer.cls_token_id}\")\n",
    "print(f\"mask_token: {tokenizer.mask_token} - mask_token_id: {tokenizer.mask_token_id}\")\n",
    "print(f\"pad_token : {tokenizer.pad_token}  - pad_token_id: {tokenizer.pad_token_id} - pad_token_type_id: {tokenizer.pad_token_type_id}\")\n",
    "print(f\"unk_token : {tokenizer.unk_token}  - unk_token_id: {tokenizer.unk_token_id}\")\n",
    "print(f\"sep_token : {tokenizer.sep_token}  - sep_token_id: {tokenizer.sep_token_id}\")\n",
    "print()\n",
    "print(f\"all_special_ids             --> {tokenizer.all_special_ids}\")\n",
    "print(f\"all_special_tokens          --> {tokenizer.all_special_tokens}\")\n",
    "print(f\"all_special_tokens_extended --> {tokenizer.all_special_tokens_extended}\")\n",
    "print()\n",
    "print(f\"name_or_path              --> {tokenizer.name_or_path}\")\n",
    "print(f\"vocab_size                --> {tokenizer.vocab_size}\")\n",
    "print(f\"model_max_length          --> {tokenizer.model_max_length}\")\n",
    "print(f\"model_input_names         --> {tokenizer.model_input_names}\")\n",
    "print(f\"prepare_for_model         --> {tokenizer.prepare_for_model([101, 7592, 2019, 20377])}\")\n",
    "\n",
    "print(f\"SPECIAL_TOKENS_ATTRIBUTES --> {tokenizer.SPECIAL_TOKENS_ATTRIBUTES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare & Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataMaster:\n",
    "    def __init__(self, path: str, max_length=512):\n",
    "        self.path = path\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('tbs17/MathBERT', output_hidden_states=True)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def tokenize_text(self, kalimat: str):\n",
    "        return self.tokenizer(kalimat, padding='max_length', max_length=self.max_length, truncation=True, return_tensors='tf')\n",
    "\n",
    "    def load_tokenized_train_val_test_ds(self, batch_size, seed, AUTOTUNE=tf.data.AUTOTUNE):\n",
    "        df_train = pd.read_csv(f\"{self.path}train.csv\")\n",
    "        df_test = pd.read_csv(f\"{self.path}test.csv\")\n",
    "        df_val = pd.read_csv(f\"{self.path}val.csv\")\n",
    "\n",
    "        train_post_canonical = list(df_train['post_canonical'].values)\n",
    "        train_score = df_train['score'].values\n",
    "\n",
    "        test_post_canonical = list(df_test['post_canonical'].values)\n",
    "        test_score = df_test['score'].values\n",
    "\n",
    "        val_post_canonical = list(df_val['post_canonical'].values)\n",
    "        val_score = df_val['score'].values\n",
    "        \n",
    "        train_ds = tf.data.Dataset.from_tensor_slices((self.tokenize_text(train_post_canonical), train_score))\n",
    "        val_ds = tf.data.Dataset.from_tensor_slices((self.tokenize_text(val_post_canonical), val_score))\n",
    "        test_ds = tf.data.Dataset.from_tensor_slices((self.tokenize_text(test_post_canonical), test_score))\n",
    "        \n",
    "        train_ds = train_ds.shuffle(buffer_size=len(df_train), seed=seed).batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n",
    "        val_ds = val_ds.batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n",
    "        test_ds = test_ds.batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n",
    "        \n",
    "        return train_ds, val_ds, test_ds\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "batch_size = 16\n",
    "seed = 42\n",
    "\n",
    "datamaster = DataMaster(\"../data/regression/imo/\")\n",
    "train_ds, val_ds, test_ds = datamaster.load_tokenized_train_val_test_ds(batch_size=batch_size, seed=seed, AUTOTUNE=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MathBertRegressorModel:\n",
    "    def __init__(self, max_length=512) -> None:\n",
    "        self.max_length = max_length\n",
    "        self.encoder = TFBertModel.from_pretrained(\"tbs17/MathBERT\", from_pt=True)\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "        \n",
    "        self.train_ds = None\n",
    "        self.validation_data = None\n",
    "        self.epochs = None\n",
    "        \n",
    "        self.history = None\n",
    "\n",
    "    def compile_model(self, train_ds, validation_data, epochs, learning_rate, optimizer_type='adamw', loss=tf.keras.losses.mean_squared_error, metrics=['mae']):\n",
    "        self.train_ds = train_ds\n",
    "        self.validation_data = validation_data\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        steps_per_epoch = tf.data.experimental.cardinality(self.train_ds).numpy()\n",
    "        num_train_steps = steps_per_epoch * self.epochs\n",
    "        optimizer = optimization.create_optimizer(\n",
    "            init_lr=learning_rate,\n",
    "            num_train_steps=num_train_steps,\n",
    "            num_warmup_steps=int(0.1*num_train_steps),\n",
    "            optimizer_type=optimizer_type\n",
    "        )\n",
    "        self.model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    def build_model(self):\n",
    "        input_ids_layer = tf.keras.Input(shape=(self.max_length,), dtype=tf.int32, name='input_ids_layer')\n",
    "        attention_mask_layer = tf.keras.Input(shape=(self.max_length,), dtype=tf.int32, name='attention_mask_layer')\n",
    "        token_type_ids_layer = tf.keras.Input(shape=(self.max_length,), dtype=tf.int32, name='token_type_ids_layer')\n",
    "        inputs = [input_ids_layer, attention_mask_layer, token_type_ids_layer]\n",
    "        \n",
    "        self.encoder.trainable = False\n",
    "        encoded_text = self.encoder(\n",
    "            input_ids=input_ids_layer, \n",
    "            attention_mask=attention_mask_layer,\n",
    "            token_type_ids=token_type_ids_layer)\n",
    "        last_hidden_state = encoded_text['last_hidden_state'][:, 0, :]  # Output -> dict.keys(['last_hidden_state', 'pooler_output'])\n",
    "        \n",
    "        net = tf.keras.layers.Dropout(0.1)(last_hidden_state)\n",
    "        net = tf.keras.layers.Dense(512, activation='relu')(net)\n",
    "        net = tf.keras.layers.Dropout(0.1)(net)\n",
    "        net = tf.keras.layers.Dense(256, activation='relu')(net)\n",
    "        net = tf.keras.layers.Dropout(0.1)(net)\n",
    "        net = tf.keras.layers.Dense(128, activation='relu')(net)\n",
    "        net = tf.keras.layers.Dropout(0.1)(net)\n",
    "        net = tf.keras.layers.Dense(1, activation='linear')(net)\n",
    "    \n",
    "        return tf.keras.Model(inputs=inputs, outputs=net)\n",
    "    \n",
    "    def train(self):\n",
    "        print(\"Start training..\")\n",
    "        start = perf_counter()\n",
    "        history = self.model.fit(\n",
    "            self.train_ds, \n",
    "            validation_data=self.validation_data, \n",
    "            epochs=self.epochs\n",
    "        )\n",
    "        end = perf_counter()\n",
    "        print(f\"\\nTotal training time: {end-start:.2f}s\")\n",
    "\n",
    "        self.history = history.history\n",
    "        return history\n",
    "    \n",
    "    def evaluate_test(self, test_ds):\n",
    "        loss, mae = self.model.evaluate(test_ds)\n",
    "        return {'loss': loss, 'mae': mae}\n",
    "        \n",
    "    def plot_training_history_over_time(self, figsize=(14, 12)):\n",
    "        print(self.history.keys())\n",
    "        print(\"Training history over time\")\n",
    "        \n",
    "        mae = self.history['mae']\n",
    "        val_acc = self.history['val_mae']\n",
    "        loss = self.history['loss']\n",
    "        val_mae = self.history['val_mae']\n",
    "        epochs = range(1, len(mae) + 1)\n",
    "        \n",
    "        figure, ax = plt.subplots(2, 1, figsize=figsize, layout=\"constrained\")\n",
    "        # loss\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "        plt.plot(epochs, val_mae, 'b', label='Validation loss')\n",
    "        plt.title('Training and validation loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        # mae\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(epochs, mae, 'r', label='Training mae')\n",
    "        plt.plot(epochs, val_acc, 'b', label='Validation mae')\n",
    "        plt.title('Training and validation mae')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('MAE')\n",
    "        plt.legend()\n",
    "\n",
    "epochs = 40\n",
    "learning_rate = 1e-6\n",
    "optimizer_type = 'adamw'\n",
    "loss = tf.keras.losses.mean_squared_error\n",
    "metrics = ['mae']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mathbert_regressor_model = MathBertRegressorModel()\n",
    "mathbert_regressor_model.compile_model(\n",
    "    train_ds, \n",
    "    val_ds, \n",
    "    epochs=2, \n",
    "    learning_rate=1e-4,\n",
    "    learning_rate=learning_rate, \n",
    "    optimizer_type=optimizer_type, \n",
    "    loss=loss, \n",
    "    metrics=metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mathbert_regressor_model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mathbert_regressor_model.plot_training_history_over_time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aops_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
