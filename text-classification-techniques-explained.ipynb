{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2f7433f",
   "metadata": {
    "papermill": {
     "duration": 0.0306,
     "end_time": "2024-01-18T17:08:45.866360",
     "exception": false,
     "start_time": "2024-01-18T17:08:45.835760",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"color: #7b6b59; font-size: 30px; text-align: center;\">Diverse Approaches to Document Classification</div>\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "**Ways to Fine Tune the Model**\n",
    "\n",
    "1. Feature extraction ‚Äì We can use a pre-trained model as a feature extraction mechanism. What we can do is that we can remove the output layer( the one which gives the probabilities for being in each of the 1000 classes) and then use the entire network as a fixed feature extractor for the new data set.\n",
    "\n",
    "1. Use the Architecture of the pre-trained model ‚Äì What we can do is that we use architecture of the model while we initialize all the weights randomly and train the model according to our dataset again.\n",
    "\n",
    "1. Train some layers while freeze others ‚Äì Another way to use a pre-trained model is to train is partially. What we can do is we keep the weights of initial layers of the model frozen while we retrain only the higher layers. We can try and test as to how many layers to be frozen and how many to be trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38f36db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7650b8b",
   "metadata": {
    "papermill": {
     "duration": 0.030406,
     "end_time": "2024-01-18T17:08:45.927287",
     "exception": false,
     "start_time": "2024-01-18T17:08:45.896881",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Import Python Libraries</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "690592a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:08:45.986911Z",
     "iopub.status.busy": "2024-01-18T17:08:45.986507Z",
     "iopub.status.idle": "2024-01-18T17:09:07.767406Z",
     "shell.execute_reply": "2024-01-18T17:09:07.766474Z"
    },
    "papermill": {
     "duration": 21.813508,
     "end_time": "2024-01-18T17:09:07.770123",
     "exception": false,
     "start_time": "2024-01-18T17:08:45.956615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "#from torch.utils.data import DataLoader, Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a29f0bd",
   "metadata": {
    "papermill": {
     "duration": 0.028364,
     "end_time": "2024-01-18T17:09:07.828198",
     "exception": false,
     "start_time": "2024-01-18T17:09:07.799834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Load the Dataset</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03e61733",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:09:07.890057Z",
     "iopub.status.busy": "2024-01-18T17:09:07.888750Z",
     "iopub.status.idle": "2024-01-18T17:09:08.002764Z",
     "shell.execute_reply": "2024-01-18T17:09:08.001656Z"
    },
    "papermill": {
     "duration": 0.14829,
     "end_time": "2024-01-18T17:09:08.005430",
     "exception": false,
     "start_time": "2024-01-18T17:09:07.857140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_prompts = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\")\n",
    "train_essays = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\")\n",
    "test_essays = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a37154f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:09:08.130432Z",
     "iopub.status.busy": "2024-01-18T17:09:08.130015Z",
     "iopub.status.idle": "2024-01-18T17:09:08.149536Z",
     "shell.execute_reply": "2024-01-18T17:09:08.148498Z"
    },
    "papermill": {
     "duration": 0.056643,
     "end_time": "2024-01-18T17:09:08.152034",
     "exception": false,
     "start_time": "2024-01-18T17:09:08.095391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>instructions</th>\n",
       "      <th>source_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Car-free cities</td>\n",
       "      <td>Write an explanatory essay to inform fellow ci...</td>\n",
       "      <td># In German Suburb, Life Goes On Without Cars ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Does the electoral college work?</td>\n",
       "      <td>Write a letter to your state senator in which ...</td>\n",
       "      <td># What Is the Electoral College? by the Office...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prompt_id                       prompt_name  \\\n",
       "0          0                   Car-free cities   \n",
       "1          1  Does the electoral college work?   \n",
       "\n",
       "                                        instructions  \\\n",
       "0  Write an explanatory essay to inform fellow ci...   \n",
       "1  Write a letter to your state senator in which ...   \n",
       "\n",
       "                                         source_text  \n",
       "0  # In German Suburb, Life Goes On Without Cars ...  \n",
       "1  # What Is the Electoral College? by the Office...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prompts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c2581b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:09:08.214997Z",
     "iopub.status.busy": "2024-01-18T17:09:08.214590Z",
     "iopub.status.idle": "2024-01-18T17:09:08.226220Z",
     "shell.execute_reply": "2024-01-18T17:09:08.225161Z"
    },
    "papermill": {
     "duration": 0.046638,
     "end_time": "2024-01-18T17:09:08.229421",
     "exception": false,
     "start_time": "2024-01-18T17:09:08.182783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0059830c</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>005db917</td>\n",
       "      <td>0</td>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008f63e3</td>\n",
       "      <td>0</td>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00940276</td>\n",
       "      <td>0</td>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00c39458</td>\n",
       "      <td>0</td>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  prompt_id                                               text  \\\n",
       "0  0059830c          0  Cars. Cars have been around since they became ...   \n",
       "1  005db917          0  Transportation is a large necessity in most co...   \n",
       "2  008f63e3          0  \"America's love affair with it's vehicles seem...   \n",
       "3  00940276          0  How often do you ride in a car? Do you drive a...   \n",
       "4  00c39458          0  Cars are a wonderful thing. They are perhaps o...   \n",
       "\n",
       "   generated  \n",
       "0          0  \n",
       "1          0  \n",
       "2          0  \n",
       "3          0  \n",
       "4          0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_essays.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29aeadeb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:09:08.300881Z",
     "iopub.status.busy": "2024-01-18T17:09:08.298744Z",
     "iopub.status.idle": "2024-01-18T17:09:08.308691Z",
     "shell.execute_reply": "2024-01-18T17:09:08.307135Z"
    },
    "papermill": {
     "duration": 0.050713,
     "end_time": "2024-01-18T17:09:08.311782",
     "exception": false,
     "start_time": "2024-01-18T17:09:08.261069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1378, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_essays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47d4e065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:09:08.388017Z",
     "iopub.status.busy": "2024-01-18T17:09:08.387574Z",
     "iopub.status.idle": "2024-01-18T17:09:08.394498Z",
     "shell.execute_reply": "2024-01-18T17:09:08.393532Z"
    },
    "papermill": {
     "duration": 0.042222,
     "end_time": "2024-01-18T17:09:08.396819",
     "exception": false,
     "start_time": "2024-01-18T17:09:08.354597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_essays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81be1589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:09:08.461293Z",
     "iopub.status.busy": "2024-01-18T17:09:08.460857Z",
     "iopub.status.idle": "2024-01-18T17:09:10.770683Z",
     "shell.execute_reply": "2024-01-18T17:09:10.769718Z"
    },
    "papermill": {
     "duration": 2.345065,
     "end_time": "2024-01-18T17:09:10.773290",
     "exception": false,
     "start_time": "2024-01-18T17:09:08.428225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\")\n",
    "train_df = df[df.prompt_name != \"Car-free cities\"].reset_index(drop=True)\n",
    "valid_df = df[df.prompt_name == \"Car-free cities\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c478b6bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:09:10.843566Z",
     "iopub.status.busy": "2024-01-18T17:09:10.843136Z",
     "iopub.status.idle": "2024-01-18T17:09:10.850043Z",
     "shell.execute_reply": "2024-01-18T17:09:10.849077Z"
    },
    "papermill": {
     "duration": 0.040182,
     "end_time": "2024-01-18T17:09:10.852335",
     "exception": false,
     "start_time": "2024-01-18T17:09:10.812153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40151, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "940c0c6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:09:10.917128Z",
     "iopub.status.busy": "2024-01-18T17:09:10.916487Z",
     "iopub.status.idle": "2024-01-18T17:09:10.928403Z",
     "shell.execute_reply": "2024-01-18T17:09:10.927311Z"
    },
    "papermill": {
     "duration": 0.046694,
     "end_time": "2024-01-18T17:09:10.930695",
     "exception": false,
     "start_time": "2024-01-18T17:09:10.884001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>source</th>\n",
       "      <th>RDizzl3_seven</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phones\\n\\nModern humans today are always on th...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This essay will explain if drivers should or s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Driving while the use of cellular devices\\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phones &amp; Driving\\n\\nDrivers should not be able...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cell Phone Operation While Driving\\n\\nThe abil...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Phones\\n\\nModern humans today are always on th...      0   \n",
       "1  This essay will explain if drivers should or s...      0   \n",
       "2  Driving while the use of cellular devices\\n\\nT...      0   \n",
       "3  Phones & Driving\\n\\nDrivers should not be able...      0   \n",
       "4  Cell Phone Operation While Driving\\n\\nThe abil...      0   \n",
       "\n",
       "          prompt_name           source  RDizzl3_seven  \n",
       "0  Phones and driving  persuade_corpus          False  \n",
       "1  Phones and driving  persuade_corpus          False  \n",
       "2  Phones and driving  persuade_corpus          False  \n",
       "3  Phones and driving  persuade_corpus          False  \n",
       "4  Phones and driving  persuade_corpus          False  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d561330",
   "metadata": {
    "papermill": {
     "duration": 0.029978,
     "end_time": "2024-01-18T17:09:10.990943",
     "exception": false,
     "start_time": "2024-01-18T17:09:10.960965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Exploratory Data Analysis</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d467861",
   "metadata": {
    "papermill": {
     "duration": 0.030589,
     "end_time": "2024-01-18T17:09:11.052102",
     "exception": false,
     "start_time": "2024-01-18T17:09:11.021513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color: #7b6b59;\">Labels Distribution in Essay Data</span>\n",
    "\n",
    "- `generated`: Whether the essay was written by a student (0) or generated by an LLM (1). This field is the target and is not present in test_essays.csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "550ce442",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:09:11.115750Z",
     "iopub.status.busy": "2024-01-18T17:09:11.114804Z",
     "iopub.status.idle": "2024-01-18T17:09:11.129460Z",
     "shell.execute_reply": "2024-01-18T17:09:11.128356Z"
    },
    "papermill": {
     "duration": 0.048977,
     "end_time": "2024-01-18T17:09:11.131798",
     "exception": false,
     "start_time": "2024-01-18T17:09:11.082821",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generated\n",
       "0    1375\n",
       "1       3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_essays['generated'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbd76599",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:09:11.194787Z",
     "iopub.status.busy": "2024-01-18T17:09:11.193945Z",
     "iopub.status.idle": "2024-01-18T17:09:11.203702Z",
     "shell.execute_reply": "2024-01-18T17:09:11.202529Z"
    },
    "papermill": {
     "duration": 0.043965,
     "end_time": "2024-01-18T17:09:11.206018",
     "exception": false,
     "start_time": "2024-01-18T17:09:11.162053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generated\n",
       "0    0.997823\n",
       "1    0.002177\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_essays['generated'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dda153c",
   "metadata": {
    "papermill": {
     "duration": 0.032044,
     "end_time": "2024-01-18T17:09:11.268656",
     "exception": false,
     "start_time": "2024-01-18T17:09:11.236612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:24px;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Standard Approaches: Vectorization and Classic Machine Learning (ML) Model</div>\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "A simple approach for text classification is to convert text passages in vectors and then use standard ML algorithms such as logistic regression or tree-based models. The key question then becomes: How do you transform a text passage in a vector?\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Option 1: TF-IDF, Sparse Vectorization and Classic Machine Learning (ML) Model</span>\n",
    "\n",
    "TF-IDF (or **term frequency ‚Äî inverse document frequency**) is one way to achieve this vectorization. It returns a vector with one dimension for each word in a given vocabulary. Each component of this vector reflects the frequency of the corresponding word in the input text compared to the entire collection of texts.\n",
    "\n",
    "**TF-IDF has several drawbacks. It does not consider the order of the words in the text and it ignores the semantic similarity between words.** It also does not distinguish between the various meanings of a polysemous word (e.g., ‚Äúsound‚Äù as in ‚Äúa loud sound,‚Äù ‚Äúthey sound correct,‚Äù or ‚Äúa sound proposal‚Äù).\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Option 2: Embeddings obtained from a pre-trained deep learning model, Dense Vectorization and Classic ML Model</span>\n",
    "\n",
    "A more effective approach, in particular if the training dataset is relatively small, is to use the vector representations (or **sentence embeddings**) obtained from a pre-trained deep learning model such as BERT.\n",
    "\n",
    "<img width=\"921\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/901505fd-908a-4810-8fb5-66022a0cbe76\">\n",
    "\n",
    "***Sparse vectorization with TF-IDF (left), dense vectorization with sentence embeddings (right)***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7389413",
   "metadata": {
    "papermill": {
     "duration": 0.029595,
     "end_time": "2024-01-18T17:09:11.327867",
     "exception": false,
     "start_time": "2024-01-18T17:09:11.298272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Approach 1: Sparse Vectorization and Classic Machine Learning (ML) Model</div>\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Overview of vectorization options</span>\n",
    "\n",
    "**Vectors & Word Embeddings: TF-IDF vs Word2Vec vs Bag-of-words vs BERT:**\n",
    "\n",
    "As discussed above, TF-IDF can be used to vectorize text into a format more agreeable for ML & NLP techniques. However while it is a popular NLP algorithm it is not the only one out there.\n",
    "\n",
    "1. **Bag of Words:** Bag of Words (BoW) simply counts the frequency of words in a document. Thus the vector for a document has the frequency of each word in the corpus for that document.  The key difference between bag of words and TF-IDF is that the former does not incorporate any sort of inverse document frequency (IDF)  and is only a frequency count (TF).\n",
    "\n",
    "1. **Word2Vec:**  Word2Vec is an algorithm that uses shallow 2-layer, not deep, neural networks to ingest a corpus and produce sets of vectors. Some key differences between TF-IDF and word2vec is that TF-IDF is a statistical measure that we can apply to terms in a document and then use that to form a vector whereas word2vec will produce a vector for a term and then more work may need to be done to convert that set of vectors into a singular vector or other format. Additionally TF-IDF does not take into consideration the context of the words in the corpus whereas word2vec does.\n",
    "\n",
    "1. **BERT - Bidirectional Encoder Representations from Transformers:** BERT is an ML/NLP technique developed by Google that uses a transformer based ML model to  convert phrases, words, etc into vectors. Key differences between TF-IDF and BERT are as follows: TF-IDF does not take into account the semantic meaning or context of the words whereas BERT does. Also BERT uses deep neural networks as part of its architecture, meaning that it can be much more computationally expensive than TF-IDF which has no such requirements. \n",
    "\n",
    "**Feature Engineering with Bag-of-Words or TF-IDF:**\n",
    "\n",
    "Instead of using deep learning methods, you might utilize statistical methods for text representation like Bag-of-Words or TF-IDF, combined with machine learning algorithms.\n",
    "\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">TF-IDF</span>\n",
    "\n",
    "Most machine learning algorithms are fulfilled with mathematical things such as statistics, algebra, calculus and etc. They expect the data to be numerical such as a 2-dimensional array with rows as instances and columns as features. The problem with natural language is that the data is in the form of raw text, so that the text needs to be transformed into a vector. **The process of transforming text into a vector is commonly referred to as text vectorization.** It‚Äôs a fundamental process in natural language processing because none of the machine learning algorithms understand a text, not even computers. Text vectorization algorithm namely TF-IDF vectorizer, which is a very popular approach for traditional machine learning algorithms can help in transforming text into vectors. In order to process natural language, the text must be represented as a numerical feature. **The process of transforming text into a numerical feature is called text vectorization.** TF-IDF is one of the most popular text vectorizers, the calculation is very simple and easy to understand. It gives the rare term high weight and gives the common term low weight. TF-IDF stands for term frequency-inverse document frequency and it is a measure, used in the fields of information retrieval (IR) and machine learning, that can quantify the importance or relevance of string representations (words, phrases, lemmas, etc)  in a document amongst a collection of documents (also known as a corpus).\n",
    "\n",
    "**Term frequency-inverse document frequency** is a text vectorizer that transforms the text into a usable vector. It combines 2 concepts, Term Frequency (TF) and Document Frequency (DF). TF-IDF can be broken down into two parts **TF (term frequency)** and **IDF (inverse document frequency)**.\n",
    "\n",
    "\n",
    "- **The term frequency** is the number of occurrences of a specific term in a document. Term frequency indicates how important a specific term in a document. Term frequency represents every text from the data as a matrix whose rows are the number of documents and columns are the number of distinct terms throughout all documents. Term frequency works by looking at the frequency of a particular term you are concerned with relative to the document. There are multiple measures, or ways, of defining frequency:\n",
    "\n",
    "    - Number of times the word appears in a document (raw count).\n",
    "    - Term frequency adjusted for the length of the document (raw count of occurences divided by number of words in the document).\n",
    "    - Logarithmically scaled frequency (e.g. log(1 + raw count)).\n",
    "    - Boolean frequency (e.g. 1 if the term occurs, or 0 if the term does not occur, in the document).\n",
    "\n",
    "- **Document frequency** is the number of documents containing a specific term. Document frequency indicates how common the term is.\n",
    "\n",
    "- **Inverse document frequency (IDF)** is the weight of a term, it aims to reduce the weight of a term if the term‚Äôs occurrences are scattered throughout all the documents. IDF can be calculated as follow:\n",
    "\n",
    "    <img width=\"781\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/e321a50a-138a-438b-9ee4-9320d21a8aed\">\n",
    "    \n",
    "    Where idf·µ¢ is the IDF score for term i, df·µ¢ is the number of documents containing term i, and n is the total number of documents. The higher the DF of a term, the lower the IDF for the term. When the number of DF is equal to n which means that the term appears in all documents, the IDF will be zero, since log(1) is zero, when in doubt just put this term in the stopword list because it doesn't provide much information. **What is IDF (inverse document frequency)?** Inverse document frequency looks at how common (or uncommon) a word is amongst the corpus. IDF is calculated as follows where t is the term (word) we are looking to measure the commonness of and N is the number of documents (d) in the corpus (D).. The denominator is simply the number of documents in which the term, t, appears in. The reason we need IDF is to help correct for words like ‚Äúof‚Äù, ‚Äúas‚Äù, ‚Äúthe‚Äù, etc. since they appear frequently in an English corpus. Thus by taking inverse document frequency, we can minimize the weighting of frequent terms while making infrequent terms have a higher impact. Finally IDFs can also be pulled from either a background corpus, which corrects for sampling bias, or the dataset being used in the experiment at hand.\n",
    "\n",
    "    <img width=\"706\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/6012bdc9-5847-4234-9e8c-987adfd2828e\">\n",
    "    \n",
    "    Note: It can be possible for a term to not appear in the corpus at all, which can result in a divide-by-zero error. One way to handle this is to take the existing count and add 1. Thus making the denominator (1 + count). An example of how the  popular library scikit-learn handles this can be seen below.\n",
    "    \n",
    "    <img width=\"739\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/68d7d8f0-84a7-4b3c-a811-4695eae291d9\">\n",
    "\n",
    "- The **TF-IDF score** as the name suggests is just a multiplication of the term frequency matrix with its IDF, it can be calculated as follow:\n",
    "    \n",
    "    <img width=\"692\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/3d3217ff-fccb-4012-a962-b70c8d40c379\">\n",
    "    \n",
    "    Where w·µ¢‚±º is TF-IDF score for term i in document j, tf·µ¢‚±º is term frequency for term i in document j, and idf·µ¢ is IDF score for term i. To summarize the key intuition motivating TF-IDF is the importance of a term is inversely related to its frequency across documents.TF gives us information on how often a term appears in a document and IDF gives us information about the relative rarity of a term in the collection of documents. By multiplying these values together we can get our final TF-IDF value.\n",
    "    \n",
    "    <img width=\"710\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/6ca35452-1cca-4f6b-b695-253cd13fe27a\">\n",
    "\n",
    "\n",
    "**The higher the TF-IDF score the more important or relevant the term is; as a term gets less relevant, its TF-IDF score will approach 0.**\n",
    "\n",
    "\n",
    "**Example:** Suppose we have 3 texts and we need to vectorize these texts using TF-IDF.\n",
    "\n",
    "<img width=\"614\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/a4e32193-47f9-4291-bfd4-5a1f6046f481\">\n",
    "\n",
    "1. **Step 1:** Create a term frequency matrix where rows are documents and columns are distinct terms throughout all documents. Count word occurrences in every text.\n",
    "    \n",
    "    <img width=\"830\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/77e48f3a-13cf-4c51-bd45-32010ff239d7\">\n",
    "\n",
    "1. **Step 2:** Compute inverse document frequency (IDF) using the previously explained formula. The term i and processing has 0 IDF score, as previously mentioned we can drop these terms, but for the sake of simplicity, we keep these terms here.\n",
    "    \n",
    "    <img width=\"839\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/0bc6eec5-5317-4694-9ebb-c84f9c1b9d88\">\n",
    "    \n",
    "1. **Step 3:** Multiply TF matrix with IDF respectively. That's it üòÉ! the text is now ready to feed into a machine learning algorithm.\n",
    "\n",
    "     <img width=\"833\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/fd090353-4915-4f01-a88c-ee3e381e6382\">\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Pros of using TF-IDF</span>\n",
    "\n",
    "The biggest advantages of TF-IDF come from how simple and easy to use it is. It is simple to calculate, it is computationally cheap, and it is a simple starting point for similarity calculations (via TF-IDF vectorization + cosine similarity).\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Limitations, Cons of using TF-IDF</span>\n",
    "\n",
    "1. It is only useful as a lexical level feature.\n",
    "\n",
    "1. Synonymities are neglected.\n",
    "\n",
    "1. It doesn't capture semantic. Something to be aware of is that TF-IDF cannot help carry semantic meaning. It considers the importance of the words due to how it weighs them, but it cannot necessarily derive the contexts of the words and understand importance that way.\n",
    "\n",
    "1. The highest TF-IDF score may not make sense with the topic of the document, since IDF gives high weight if the DF of a term is low.\n",
    "\n",
    "1. It neglects the sequence of the terms. Also as mentioned above, like BoW, TF-IDF ignores word order and thus compound nouns like ‚ÄúQueen of England‚Äù will not be considered as a ‚Äúsingle unit‚Äù. This also extends to situations like negation with ‚Äúnot pay the bill‚Äù vs ‚Äúpay the bill‚Äù, where the order makes a big difference. In both cases using NER tools and underscores, ‚Äúqueen_of_england‚Äù or ‚Äúnot_pay‚Äù are ways to handle treating the phrase as a single unit. No concept of word order: TF-IDF treats all words as equally important, regardless of their order or position in the document. This can be problematic for certain applications, such as sentiment analysis, where word order can be crucial for determining the sentiment of a document.\n",
    "\n",
    "1. Another disadvantage is that it can suffer from memory-inefficiency since TF-IDF can suffer from the curse of dimensionality. Recall that the length of TF-IDF vectors is equal to the size of the vocabulary. In some classification contexts this may not be an issue but in other contexts like clustering this can be unwieldy as the number of documents increases. Thus looking into some of the above named alternatives (BERT, Word2Vec) may be necessary. **Vocabulary size:** The vocabulary size can become very large when working with large datasets, which can lead to high-dimensional feature spaces and difficulty in interpreting the results.\n",
    "\n",
    "1. Assumes independence: TF-IDF assumes that the terms in a document are independent of each other. However, this is often not the case in natural language, where words are often related to each other in complex ways.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Where to use TF-IDF</span>\n",
    "\n",
    "As we can see, TF-IDF can be a very handy metric for determining how important a term is in a document. But how is TF-IDF used? There are three main applications for TF-IDF. These are in machine learning, information retrieval, and text summarization/keyword extraction.\n",
    "\n",
    "\n",
    "1. **Using TF-IDF in machine learning & natural language processing:** Machine learning algorithms often use numerical data, so when dealing with textual data or any natural language processing (NLP) task, a sub-field of ML/AI dealing with text, that data first needs to be converted to a vector of numerical data by a process known as vectorization. TF-IDF vectorization involves calculating the TF-IDF score for every word in your corpus relative to that document and then putting that information into a vector (see images above). Thus each document in your corpus would have its own vector, and the vector would have a TF-IDF score for every single word in the entire collection of documents. ***Once you have these vectors you can apply them to various use cases such as seeing if two documents are similar by comparing their TF-IDF vector using cosine similarity.***\n",
    "\n",
    "1. **Using TF-IDF in information retrieval:** TF-IDF also has use cases in the field of information retrieval, with one common example being search engines. Since TF-IDF can tell you about the relevant importance of a term based upon a document, a search engine can use TF-IDF to help rank search results based on relevance, with results which are more relevant to the user having higher TF-IDF scores.\n",
    "\n",
    "1. **Using TF-IDF in text summarization & keyword extraction:** Since TF-IDF weights words based on relevance, one can use this technique to determine that the words with the highest relevance are the most important. This can be used to help summarize articles more efficiently or to simply determine keywords (or even tags) for a document. Measures relevance: TF-IDF measures the importance of a term in a document, based on the frequency of the term in the document and the inverse document frequency (IDF) of the term across the entire corpus. This helps to identify which terms are most relevant to a particular document.\n",
    "\n",
    "1. **Interpretable:** The scores generated by TF-IDF are easy to interpret and understand, as they represent the importance of a term in a document relative to its importance across the entire corpus.\n",
    "\n",
    "1. Works well with different languages: TF-IDF can be used with different languages and character encodings, making it a versatile technique for processing multilingual text data.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Conclusion</span>\n",
    "\n",
    "TF-IDF (Term Frequency - Inverse Document Frequency) is a handy algorithm that uses the frequency of words to determine how relevant those words are to a given document. It‚Äôs a relatively simple but intuitive approach to weighting words, allowing it to act as a great jumping off point for a variety of tasks. This includes building search engines, summarizing documents, or other tasks in the information retrieval and machine learning domains.\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">How to implement TF-IDF with scikit-learn</span>\n",
    "\n",
    "1. Thanks to the `TfidfVectorizer` class, implementing TF-IDF with `scikit-learn` is a fairly straightforward process. The first step is importing `TfidfVectorizer` and creating a list of documents to analyze and convert into TF-IDF features.\n",
    "\n",
    "1. Next, create an instance of the `TfidfVectorizer` class with the desired customization options, such as tokenization patterns, stopword removal or IDF smoothing parameters.\n",
    "\n",
    "1. Then, to fit and transform the corpus, call the `fit_transform()` method on the vectorizer instance and pass in the corpus. This computes term frequencies and inverse document frequencies while transforming the text data into a matrix of TF-IDF features.\n",
    "\n",
    "1. Finally, call `get_feature_names()` to inspect feature names and their corresponding TF-IDF values, then convert the variable to an array using toarray():\n",
    "\n",
    "By following these steps, you can implement TF-IDF with scikit-learn and transform your raw text data into valuable numerical representations for further analysis or feeding into machine learning models.\n",
    "\n",
    "When using the `TfidfVectorizer` from `scikit-learn`, **you do not necessarily need to tokenize the text yourself before passing it to the vectorizer**. TfidfVectorizer has built-in capabilities to tokenize and preprocess the text. Here's how it works by default:\n",
    "\n",
    "1. **Default Tokenization in TfidfVectorizer:**\n",
    "\n",
    "    - **Tokenization:** By default, TfidfVectorizer tokenizes the text by extracting word tokens and ignores punctuation and whitespace. This is typically done using a regular expression that defines what constitutes a token (word). The default pattern is `r\"(?u)\\b\\w\\w+\\b\"`, which captures sequences of alphanumeric characters (words) that are at least two characters long. This pattern is specified in the token_pattern argument. So while spaces between words usually signify where one word ends and another begins (and thus often correspond to word boundaries), the regex isn't splitting text directly on spaces. Instead, it's looking for those alphanumeric sequences that are bounded by non-word characters or the edges of the string, which more robustly constitutes what we think of as whole, standalone words. This method is more reliable because:\n",
    "        - **It ignores punctuation:** For example, in \"end-of-sentence.\", the period is not part of the last word, and the pattern correctly excludes it from the token \"sentence\".\n",
    "        - **It handles complex word separations:** Not all words are neatly separated by spaces, especially in languages with different scripts or in cases with punctuation like hyphens, apostrophes, etc. The pattern correctly identifies words in many of these cases.\n",
    "        In summary, while spaces are a significant part of how the pattern determines where words begin and end, the actual process involves identifying sequences of word characters that are delineated by word boundaries, which provides a more nuanced and effective approach to word tokenization in varied text environments.\n",
    "        \n",
    "    - **Preprocessing:** It converts all characters to lowercase (unless you set lowercase=False) and performs normalization, such as accent stripping, if specified.\n",
    "\n",
    "1. **Customization Options:**\n",
    "    1. **Custom Tokenizer:** You can provide a custom tokenizer function to the `tokenizer` parameter. This function takes a string as input and returns a list of tokens. If you have specific tokenization needs (e.g., handling special cases, working with a non-standard text format), you might implement and use your custom tokenizer.\n",
    "\n",
    "    1. **Custom Preprocessor:** Similarly, you can provide a custom preprocessing function to the `preprocessor` parameter. This function also takes a string as input and returns the processed string. It's applied to the text before tokenization.\n",
    "\n",
    "\n",
    "***Should You Tokenize Beforehand?***\n",
    "\n",
    "- **Usually Unnecessary:** For standard text processing needs, the default behavior of TfidfVectorizer is often sufficient. It is designed to handle typical cases of text vectorization, including tokenization and case normalization.\n",
    "\n",
    "- **Custom Needs:** If your text data requires specialized handling, such as dealing with a particular language's nuances, handling mixed text types, or integrating with an existing text processing pipeline, you might perform tokenization (and other text preprocessing) before vectorization. In such cases, you could use the tokenizer and preprocessor parameters to integrate your custom functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8cc55e",
   "metadata": {
    "papermill": {
     "duration": 0.030166,
     "end_time": "2024-01-18T17:09:11.387976",
     "exception": false,
     "start_time": "2024-01-18T17:09:11.357810",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. **`ngram_range=(1, 3)`:** This parameter defines the range of n-gram sizes to include in the token counts. (1, 3) means that it will consider unigrams (single words), bigrams (two consecutive words), and trigrams (three consecutive words) as individual features for vectorization. Essentially, it's looking at the individual words, pairs of consecutive words, and triplets of consecutive words when creating the vectors.\n",
    "\n",
    "1. **`sublinear_tf=True`:** This parameter applies sublinear tf scaling, i.e., it replaces term frequency (tf) with 1 + log(tf). The idea is to reduce the sensitivity of the vectorizer to terms that occur very frequently and therefore might skew the results disproportionately. It's a way to temper the effect of terms that appear very often and might dominate the feature set. By transforming the frequency to the logarithmic scale, increases in term frequency have a gradually smaller effect on the computation of TF-IDF.\n",
    "\n",
    "1. **`lowercase=False`:** This indicates that the text will not be automatically converted to lowercase before tokenizing. By default, TfidfVectorizer converts all characters to lowercase to ensure that the same words in different cases are counted as the same token.\n",
    "\n",
    "1. **`analyzer='word'`:** This parameter sets the unit of features to words. Other options might include 'char' or 'char_wb' for character n-grams. 'word' means it will consider tokens of words as the feature base.\n",
    "\n",
    "1. **`tokenizer=dummy`:** This specifies a custom tokenizer function. Typically, TfidfVectorizer tokenizes the string by extracting words of at least two letters. By setting tokenizer to 'dummy', you are replacing the default tokenizer with your own custom function named dummy. This function will be used to split the text into tokens.\n",
    "\n",
    "1. **`token_pattern=None`:** Normally, this parameter defines the regex pattern that the tokenizer uses to find tokens in the text string. By setting it to None, and providing a custom tokenizer, you're effectively ignoring the default regex pattern and relying entirely on the custom tokenizer you've provided.\n",
    "\n",
    "1. **`preprocessor=dummy`:** Similar to the tokenizer, this specifies a custom pre-processing function. The default preprocessor in TfidfVectorizer takes care of removing accents and performing other cleaning steps. By setting it to 'dummy', you are specifying that your own custom function named dummy should be used for preprocessing the text.\n",
    "\n",
    "1. **`strip_accents='unicode'`:** This is used to remove accents during the preprocessing step. 'unicode' is a method that works on any characters that have a direct Unicode equivalent. It's an effective way to standardize text by removing accents and diacritical marks that might lead to variations in how words are processed.\n",
    "\n",
    "\n",
    "\n",
    "1. **Fitting the Vectorizer:** Initially, when we fit TfidfVectorizer to our documents (e.g., using vectorizer.fit(texts)), it learns the vocabulary of the corpus, meaning it identifies all unique terms used across all documents, considering the constraints and specifications we've given it (like token patterns, n-grams, etc.).\n",
    "\n",
    "1. **Building the Vocabulary Dictionary:** After fitting, the vectorizer has a complete list of terms used in the documents. It then creates a mapping of these terms to specific indices. This mapping is stored in `vectorizer.vocabulary_`.\n",
    "    \n",
    "    - Keys: Each unique term or token found in the corpus.\n",
    "    - Values: A unique integer index corresponding to each term. This index is used when creating the sparse matrix representation of the documents where each term's TF-IDF score will be placed.\n",
    "    \n",
    "    ```python \n",
    "{\n",
    "    'galaxy': 123,\n",
    "    'black hole': 15,\n",
    "    'star cluster': 678,\n",
    "    'nebula': 321,\n",
    "    ...\n",
    "}\n",
    "```\n",
    "    In this hypothetical vocabulary:\n",
    "\n",
    "    - The term 'galaxy' is found at column index 123 in the TF-IDF matrix.\n",
    "    - The term 'black hole' is found at column index 15, and so on.\n",
    "\n",
    "1. When you **transform** your documents into their TF-IDF representation using the fitted vectorizer (via vectorizer.transform(texts)), each document is represented as a sparse vector with the length of the total vocabulary, where most values are zero except for the indices corresponding to the terms present in the document.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70a6385d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:09:11.452110Z",
     "iopub.status.busy": "2024-01-18T17:09:11.451733Z",
     "iopub.status.idle": "2024-01-18T17:10:52.457530Z",
     "shell.execute_reply": "2024-01-18T17:10:52.456405Z"
    },
    "papermill": {
     "duration": 101.041513,
     "end_time": "2024-01-18T17:10:52.460319",
     "exception": false,
     "start_time": "2024-01-18T17:09:11.418806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3),sublinear_tf=True)\n",
    "X = vectorizer.fit_transform(train_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5b2243d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:10:52.522283Z",
     "iopub.status.busy": "2024-01-18T17:10:52.521914Z",
     "iopub.status.idle": "2024-01-18T17:11:02.248347Z",
     "shell.execute_reply": "2024-01-18T17:11:02.247174Z"
    },
    "papermill": {
     "duration": 9.759433,
     "end_time": "2024-01-18T17:11:02.250821",
     "exception": false,
     "start_time": "2024-01-18T17:10:52.491388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00' '00 00' '00 00 and' ... 'ÂÆåÂÖ®Á¶ÅÊ≠¢‰ΩøÁî®ÊâãÊú∫Â∫îËØ•ÊòØÂêàÊ≥ïÂíåÈÅìË∑ØÂÆâÂÖ®ÁöÑÂîØ‰∏ÄÈÄâÊã©'\n",
      " 'ÂÆåÂÖ®Á¶ÅÊ≠¢‰ΩøÁî®ÊâãÊú∫Â∫îËØ•ÊòØÂêàÊ≥ïÂíåÈÅìË∑ØÂÆâÂÖ®ÁöÑÂîØ‰∏ÄÈÄâÊã© ‰øùÊä§ÊâÄÊúâÈÅìË∑Ø‰ΩøÁî®ËÄÖÁöÑÂÆâÂÖ®'\n",
      " 'ÂÆåÂÖ®Á¶ÅÊ≠¢‰ΩøÁî®ÊâãÊú∫Â∫îËØ•ÊòØÂêàÊ≥ïÂíåÈÅìË∑ØÂÆâÂÖ®ÁöÑÂîØ‰∏ÄÈÄâÊã© ‰øùÊä§ÊâÄÊúâÈÅìË∑Ø‰ΩøÁî®ËÄÖÁöÑÂÆâÂÖ® Âè∏Êú∫ÂøÖÈ°ªÂú®È©æÈ©∂Êó∂Â∞ÜÂÖ®ÈÉ®Ê≥®ÊÑèÂäõÈÉΩÈõÜ‰∏≠Âú®ÈÅìË∑Ø‰∏ä']\n"
     ]
    }
   ],
   "source": [
    "# Inspect feature names and TF-IDF values \n",
    "print(vectorizer.get_feature_names_out()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0eb218d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:11:02.311956Z",
     "iopub.status.busy": "2024-01-18T17:11:02.311112Z",
     "iopub.status.idle": "2024-01-18T17:11:12.910399Z",
     "shell.execute_reply": "2024-01-18T17:11:12.909285Z"
    },
    "papermill": {
     "duration": 10.63247,
     "end_time": "2024-01-18T17:11:12.913005",
     "exception": false,
     "start_time": "2024-01-18T17:11:02.280535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>be in contact</th>\n",
       "      <td>0.067319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way how</th>\n",
       "      <td>0.059980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>always on their</th>\n",
       "      <td>0.059508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>are always on</th>\n",
       "      <td>0.055284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in contact</th>\n",
       "      <td>0.051028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>further without</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>further with you</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>further with this</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>further with their</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ÂÆåÂÖ®Á¶ÅÊ≠¢‰ΩøÁî®ÊâãÊú∫Â∫îËØ•ÊòØÂêàÊ≥ïÂíåÈÅìË∑ØÂÆâÂÖ®ÁöÑÂîØ‰∏ÄÈÄâÊã© ‰øùÊä§ÊâÄÊúâÈÅìË∑Ø‰ΩøÁî®ËÄÖÁöÑÂÆâÂÖ® Âè∏Êú∫ÂøÖÈ°ªÂú®È©æÈ©∂Êó∂Â∞ÜÂÖ®ÈÉ®Ê≥®ÊÑèÂäõÈÉΩÈõÜ‰∏≠Âú®ÈÅìË∑Ø‰∏ä</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6207945 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       tfidf\n",
       "be in contact                                       0.067319\n",
       "way how                                             0.059980\n",
       "always on their                                     0.059508\n",
       "are always on                                       0.055284\n",
       "in contact                                          0.051028\n",
       "...                                                      ...\n",
       "further without                                     0.000000\n",
       "further with you                                    0.000000\n",
       "further with this                                   0.000000\n",
       "further with their                                  0.000000\n",
       "ÂÆåÂÖ®Á¶ÅÊ≠¢‰ΩøÁî®ÊâãÊú∫Â∫îËØ•ÊòØÂêàÊ≥ïÂíåÈÅìË∑ØÂÆâÂÖ®ÁöÑÂîØ‰∏ÄÈÄâÊã© ‰øùÊä§ÊâÄÊúâÈÅìË∑Ø‰ΩøÁî®ËÄÖÁöÑÂÆâÂÖ® Âè∏Êú∫ÂøÖÈ°ªÂú®È©æÈ©∂Êó∂Â∞ÜÂÖ®...  0.000000\n",
       "\n",
       "[6207945 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer=X[0] \n",
    "\n",
    "# place tf-idf values in a pandas data frame \n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=vectorizer.get_feature_names_out(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2cc759",
   "metadata": {
    "papermill": {
     "duration": 0.030228,
     "end_time": "2024-01-18T17:11:12.975120",
     "exception": false,
     "start_time": "2024-01-18T17:11:12.944892",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The `vocab = vectorizer.vocabulary_` line returns a Python dictionary from the fitted `TfidfVectorizer` object. The dictionary's keys are the terms (or tokens) found in the document corpus, and the values are the column indices of these terms in the resulting TF-IDF matrix.\n",
    "The term 'galaxy' is found at column index 123 in the TF-IDF matrix.\n",
    "The term 'black hole' is found at column index 15, and so on.  This vocabulary is crucial because it maintains a consistent mapping of terms to indices, ensuring that when you transform new documents into vectors, the terms align correctly with the learned model's features. It's essential for both understanding the feature space of your model and for preparing new text inputs for predictions or further analysis with the trained vectorizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27a908da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:11:13.038515Z",
     "iopub.status.busy": "2024-01-18T17:11:13.037558Z",
     "iopub.status.idle": "2024-01-18T17:11:13.042666Z",
     "shell.execute_reply": "2024-01-18T17:11:13.041423Z"
    },
    "papermill": {
     "duration": 0.039779,
     "end_time": "2024-01-18T17:11:13.045057",
     "exception": false,
     "start_time": "2024-01-18T17:11:13.005278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Getting vocab\n",
    "vocab = vectorizer.vocabulary_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a7bff0",
   "metadata": {
    "papermill": {
     "duration": 0.03155,
     "end_time": "2024-01-18T17:11:13.108149",
     "exception": false,
     "start_time": "2024-01-18T17:11:13.076599",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:24px;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Approach 2: Extracting embeddings from pre-trained models and Classic Machine Learning (ML) Model - Transfer Learning without Fine-Tuning</div>\n",
    "\n",
    "\n",
    "\n",
    "Instead of fine-tuning a pre-trained model, you could use it as a feature extractor. For instance, you can pass your documents through a pre-trained model (like BERT) to get embeddings and then train a simpler machine learning model (like Logistic Regression) on those features.\n",
    "\n",
    "**Extracting embeddings from pre-trained BERT| Huggingface Transformers**\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Overview</span>\n",
    "\n",
    "\n",
    "The need for standardization in training models and using the language model, Hugging Face, was found.NLP is democratized by Hugging Face, where the constructed API allows easy access to pre-trained models, datasets, and tokens. This Hugging Face's transformers library generates embeddings, and we use the pre-trained BERT model to extract the embeddings.\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">How to use embeddings for feature extraction?</span>\n",
    "\n",
    "Now, let‚Äôs talk about how you can use BERT with your text: The BERT Model learns complex understandings of the English language, which can help you extract different aspects of text for various tasks. If you have a set of sentences with labels, you can train a regular classifier using the information produced by the BERT Model as input (the text). To obtain the features of a particular text using this model in TensorFlow see the code below.\n",
    "\n",
    "**How to use embeddings to extract information from text column?**\n",
    "\n",
    "We are going to take advantage of the incredible hugging face ü§ó framework to extract information from this feature.\n",
    "\n",
    "1. **Step 1:** First, we need to import the model and the tokenizer: There are different models that we can try, and you check them here: https://huggingface.co/models?pipeline_tag=feature-extraction It is important to use the model‚Äôs tokenizer so that it receives the data in a proper format and they are also useful since they already clean up the data for you. Each tokenizer will have different ways of dealing with the data, therefore it is important to read about them.\n",
    "\n",
    "1. **Step 2:** Second, we extract the hidden state associated to the token CLS which represents an entire sequence of text and rather than dealing with a 768 array for each token in a string, we just need to deal with one (the 768 dimension varies from model to model).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "564aef70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:11:13.174477Z",
     "iopub.status.busy": "2024-01-18T17:11:13.173509Z",
     "iopub.status.idle": "2024-01-18T17:11:16.037617Z",
     "shell.execute_reply": "2024-01-18T17:11:16.036431Z"
    },
    "papermill": {
     "duration": 2.900415,
     "end_time": "2024-01-18T17:11:16.040293",
     "exception": false,
     "start_time": "2024-01-18T17:11:13.139878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0442dff3206b46c7aedaa88235a12ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e89d5b0b07e46d6b2bd9b2e279ed62b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49040db2f8e3450f8f6d14cec927756b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226a42c65dee4de1a7b32cddfe11cf09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0345a09810344a097bc95499ccb3cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: We need to import the model and the tokenizer\n",
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "#model = TFBertModel.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "\n",
    "\n",
    "#custom_text = \"You are welcome to utilize any text of your choice.\"\n",
    "#encoded_input = tokenizer(custom_text, return_tensors='tf')\n",
    "#output_embeddings = model(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb4a804a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:11:16.107957Z",
     "iopub.status.busy": "2024-01-18T17:11:16.106968Z",
     "iopub.status.idle": "2024-01-18T17:11:16.111951Z",
     "shell.execute_reply": "2024-01-18T17:11:16.110846Z"
    },
    "papermill": {
     "duration": 0.040898,
     "end_time": "2024-01-18T17:11:16.114267",
     "exception": false,
     "start_time": "2024-01-18T17:11:16.073369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: we extract the hidden state associated to the token CLS\n",
    "#train_df[\"embeddings\"] = train_df[\"text\"].apply(lambda x: model(**tokenizer(x, return_tensors=\"pt\", truncation=True)).last_hidden_state[:,0,:].detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ff451",
   "metadata": {
    "papermill": {
     "duration": 0.032667,
     "end_time": "2024-01-18T17:11:16.179618",
     "exception": false,
     "start_time": "2024-01-18T17:11:16.146951",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This piece of code is a straightforward example of how to use the BERT tokenizer and model from the Hugging Face `transformers `library for encoding text into embeddings. Here's a breakdown of what each part does:\n",
    "\n",
    "1. **Importing the Necessary Classes:**\n",
    "    - **BertTokenizer:** A tokenizer class for BERT. It handles the conversion from text to tokens that BERT understands.\n",
    "    - **TFBertModel:** The BERT model class compatible with TensorFlow.\n",
    "\n",
    "1. **Using the BERT Tokenizer and Model:**\n",
    "    \n",
    "    1. **Load Pre-trained Models:**\n",
    "        - `tokenizer = BertTokenizer.from_pretrained('bert-base-cased')`: Loads the BERT tokenizer for the 'bert-base-cased' version. This tokenizer is responsible for breaking the text down into tokens that BERT can understand.\n",
    "        - `model = TFBertModel.from_pretrained(\"bert-base-cased\")`: Loads the pre-trained BERT model. This model will generate embeddings for the input text.\n",
    "\n",
    "1. **Prepare Custom Text:**\n",
    "\n",
    "    - `custom_text = \"You are welcome to utilize any text of your choice.\"`: A sample text that you want to convert into embeddings.\n",
    "\n",
    "1. **Tokenize the Text:**\n",
    "    - `encoded_input = tokenizer(custom_text, return_tensors='tf')`: The tokenizer converts the text into a format suitable for the BERT model. The `return_tensors='tf'` argument tells the tokenizer to return TensorFlow tensors.\n",
    "\n",
    "1. **Generate Embeddings:**\n",
    "\n",
    "    - `output_embeddings = model(encoded_input)`: Passes the tokenized input to the BERT model. The model returns the embeddings, which are a rich, contextual representation of each token in the input text.\n",
    "\n",
    "1. **Understanding the Output:** The `output_embeddings` returned by the model is typically a complex structure containing several types of embeddings:\n",
    "\n",
    "    - **Last Hidden State:** The output corresponding to the last layer of the BERT model, which gives you the embeddings for each token in the input sequence.\n",
    "    - **Pooler Output:** A pooled output of the last hidden state, which represents the entire input sequence, often used in classification tasks.\n",
    "\n",
    "To print the dimensions of the output_embeddings, you would typically focus on these two parts. Here is how you can do it:\n",
    "\n",
    "In these lines of code:\n",
    "\n",
    "- `output_embeddings.last_hidden_state.shape` will give you the dimensions of the last hidden state, which is usually of the form `[batch_size, sequence_length, hidden_size]`.\n",
    "- `output_embeddings.pooler_output.shape` will give you the dimensions of the pooled output, typically `[batch_size, hidden_size]`.\n",
    "Understanding these dimensions:\n",
    "\n",
    "- **batch_size:** The number of sequences processed at a time (for your case, it will be 1 as you're processing a single sentence).\n",
    "- **sequence_length:** The length of the tokenized input (number of tokens).\n",
    "- **hidden_size:** The size of the hidden layers in the BERT model. For 'bert-base-cased', it is usually 768.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd197693",
   "metadata": {
    "papermill": {
     "duration": 0.031892,
     "end_time": "2024-01-18T17:11:16.244410",
     "exception": false,
     "start_time": "2024-01-18T17:11:16.212518",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:24px;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Important Deep Learning Concepts</div>\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Cross-Validation</span>\n",
    "\n",
    "In machine learning (ML), generalization usually refers to the ability of an algorithm to be effective across various inputs. It means that the ML model does not encounter performance degradation on the new inputs from the same distribution of the training data.\n",
    "\n",
    "For human beings generalization is the most natural thing possible. We can classify on the fly. For example, we would definitely recognize a dog even if we didn‚Äôt see this breed before. Nevertheless, it might be quite a challenge for an ML model. That‚Äôs why checking the algorithm‚Äôs ability to generalize is an important task that requires a lot of attention when building the model.\n",
    "\n",
    "To do that, we use Cross-Validation (CV). There is always a need to validate the stability of your machine learning model. I mean you just can‚Äôt fit the model to your training data and hope it would accurately work for the real data it has never seen before. You need some kind of assurance that your model has got most of the patterns from the data correct, and its not picking up too much on the noise, or in other words its low on bias and variance.\n",
    "\n",
    "Cross Validation is a very useful technique:\n",
    "\n",
    "- for **assessing the effectiveness of your model**, particularly in cases where you need to mitigate overfitting. \n",
    "- It is also of use in **determining the hyper parameters of your model, in the sense that which parameters will result in lowest test error.** \n",
    "\n",
    "This is all the basic you need to get started with cross validation. You can get started with all kinds of validation techniques using `Scikit-Learn`, that gets you up and running with just a few lines of code in python.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">What is cross-validation?</span>\n",
    "\n",
    "This process of deciding whether the numerical results quantifying hypothesized relationships between variables, are acceptable as descriptions of the data, is known as validation. Generally, an error estimation for the model is made after training, better known as evaluation of residuals. In this process, a numerical estimate of the difference in predicted and original responses is done, also called the training error. However, this only gives us an idea about how well our model does on data used to train it. **Now its possible that the model is underfitting or overfitting the data. So, the problem with this evaluation technique is that it does not give an indication of how well the learner will generalize to an independent/ unseen data set. Getting this idea about our model is known as Cross Validation.**\n",
    "\n",
    "- **Cross-validation is a technique for evaluating a machine learning model and testing its performance.** CV is commonly used in applied ML tasks. It helps to compare and select an appropriate model for the specific predictive modeling problem.\n",
    "\n",
    "CV is easy to understand, easy to implement, and it tends to have a lower bias than other methods used to count the model‚Äôs efficiency scores. All this makes cross-validation a powerful tool for selecting the best model for the specific task.\n",
    "\n",
    "There are a lot of different techniques that may be used to cross-validate a model. Still, all of them have a similar algorithm:\n",
    "\n",
    "1. **Divide** the dataset into two parts: one for training, other for testing\n",
    "1. **Train** the model on the training set\n",
    "1. **Validate** the model on the test set\n",
    "1. **Repeat** 1-3 steps a couple of times. This number depends on the CV method that you are using\n",
    "\n",
    "As you may know, there are plenty of CV techniques. Some of them are commonly used, others work only in theory. \n",
    "\n",
    "1. **Hold-out**\n",
    "1. **K-folds**\n",
    "1. **Stratified K-folds**\n",
    "1. Repeated K-folds\n",
    "1. Nested K-folds\n",
    "1. Time series CV\n",
    "\n",
    "Above listed validation techniques are also referred to as **Non-exhaustive cross validation methods**. These do not compute all ways of splitting the original sample, i.e. you just have to decide how many subsets need to be made. Also, these are approximations of method listed below, also called **Exhaustive Methods,** that computes all possible ways the data can be split into training and test sets.\n",
    "\n",
    "1. **Leave-one-out**\n",
    "1. **Leave-p-out**\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Cross-Validation Techniques</span>\n",
    "\n",
    "1. **Hold-out cross-validation:** Hold-out cross-validation is the simplest and most common technique. You might not know that it is a hold-out method but you certainly use it every day. *We usually use the hold-out method on large datasets as it requires training the model only once.* It is really easy to implement hold-out. The error estimation then tells how our model is doing on unseen data or the validation set. For example, you may do it using `sklearn.model_selection.train_test_split`. The algorithm of hold-out technique:\n",
    "\n",
    "    - Divide the dataset into two parts: the **training set** and the **test set**. Usually, 80% of the dataset goes to the training set and 20% to the test set but you may choose any splitting that suits you better\n",
    "    - Train the model on the training set\n",
    "    - Validate on the test set\n",
    "    - Save the result of the validation\n",
    "    \n",
    "    1. Disadvantages:\n",
    "        - For example, a dataset that is not completely even distribution-wise. If so we may end up in a rough spot after the split. For example, the training set will not represent the test set. Both training and test sets may differ a lot, one of them might be easier or harder.  This is a simple kind of cross validation technique, also known as the holdout method. Although this method doesn‚Äôt take any overhead to compute and is better than traditional validation, it still suffers from issues of high variance. This is because it is not certain which data points will end up in the validation set and the result might be entirely different for different sets.\n",
    "        - Moreover, the fact that we test our model only once might be a bottleneck for this method. Due to the reasons mentioned before, the result obtained by the hold-out technique may be considered inaccurate. \n",
    "\n",
    "1. **k-Fold cross-validation:** k-Fold cross-validation is a technique that minimizes the disadvantages of the hold-out method. k-Fold introduces a new way of splitting the dataset which helps to overcome the ‚Äútest only once bottleneck‚Äù. As there is never enough data to train your model, removing a part of it for validation poses a problem of underfitting. **By reducing the training data, we risk losing important patterns/ trends in data set, which in turn increases error induced by bias.** So, what we require is a method that provides ample data for training the model and also leaves ample data for validation. K Fold cross validation does exactly that. In K Fold cross validation, the data is divided into k subsets. Now the holdout method is repeated k times, such that each time, one of the k subsets is used as the test set/ validation set and the other k-1 subsets are put together to form a training set. The error estimation is averaged over all k trials to get total effectiveness of our model. As can be seen, every data point gets to be in a validation set exactly once, and gets to be in a training set k-1 times. This significantly reduces bias as we are using most of the data for fitting, and also significantly reduces variance as most of the data is also being used in validation set. Interchanging the training and test sets also adds to the effectiveness of this method. **As a general rule and empirical evidence, K = 5 or 10 is generally preferred, but nothing‚Äôs fixed and it can take any value.**\n",
    "\n",
    "    The algorithm of the k-Fold technique:\n",
    "    \n",
    "    <img width=\"500\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/376f6016-01a3-499d-816f-a677f544bf2e\">\n",
    "    \n",
    "    - Pick a number of folds ‚Äì k. Usually, k is 5 or 10 but you can choose any number which is less than the dataset‚Äôs length.\n",
    "    - Split the dataset into k equal (if possible) parts (they are called folds)\n",
    "    - Choose k ‚Äì 1 folds as the training set. The remaining fold will be the test set\n",
    "    - Train the model on the training set. On each iteration of cross-validation, you must train a new model independently of the model trained on the previous iteration\n",
    "    - Validate on the test set\n",
    "    - Save the result of the validation\n",
    "    - Repeat steps 3 ‚Äì 6 k times. Each time use the remaining  fold as the test set. \n",
    "    - In the end, you should have validated the model on every fold that you have. To get the final score average the results that you got on step 6.\n",
    "    \n",
    "    To perform k-Fold cross-validation you can use `sklearn.model_selection.KFold`. In general, it is always better to use k-Fold technique instead of hold-out. In a head to head, comparison k-Fold gives a more stable and trustworthy result since training and testing is performed on several different parts of the dataset. We can make the overall score even more robust if we increase the number of folds to test the model on many different sub-datasets.\n",
    "    \n",
    "    1. Disadvantages:\n",
    "        - Still, k-Fold method has a disadvantage. Increasing k results in training more models and the training process might be really expensive and time-consuming.\n",
    "\n",
    "1. **Stratified k-Fold cross-validation:** Sometimes we may face a large imbalance of the target value in the dataset. For example, in a dataset concerning wristwatch prices, there might be a larger number of wristwatch having a high price. In the case of classification, in cats and dogs dataset there might be a large shift towards the dog class. **Stratified k-Fold is a variation of the standard k-Fold CV technique which is designed to be effective in such cases of target imbalance.** In some cases, there may be a large imbalance in the response variables. For example, in dataset concerning price of houses, there might be large number of houses having high price. Or in case of classification, there might be several times more negative samples than positive samples. For such problems, a slight variation in the K Fold cross validation technique is made, such that each fold contains approximately the same percentage of samples of each target class as the complete set, or in case of prediction problems, the mean response value is approximately equal in all the folds. This variation is also known as Stratified K Fold.\n",
    "\n",
    "    It works as follows. Stratified k-Fold splits the dataset on k folds such that each fold contains approximately the same percentage of samples of each target class as the complete set. In the case of regression, Stratified k-Fold makes sure that the mean target value is approximately equal in all the folds. The algorithm of Stratified k-Fold technique:\n",
    "\n",
    "    - Pick a number of folds ‚Äì k\n",
    "    - Split the dataset into k folds. Each fold must contain approximately the same percentage of samples of each target class as the complete set \n",
    "    - Choose k ‚Äì 1 folds which will be the training set. The remaining fold will be the test set\n",
    "    - Train the model on the training set. On each iteration a new model must be trained\n",
    "    - Validate on the test set\n",
    "    - Save the result of the validation\n",
    "    - Repeat steps 3 ‚Äì 6 k times. Each time use the remaining  fold as the test set. In the end, you should have validated the model on every fold that you have.\n",
    "    - To get the final score average the results that you got on step 6.\n",
    "\n",
    "    As you may have noticed, the algorithm for Stratified k-Fold technique is similar to the standard k-Folds. You don‚Äôt need to code something additionally as the method will do everything necessary for you. Stratified k-Fold also has a built-in method in sklearn ‚Äì `sklearn.model_selection.StratifiedKFold`. All mentioned above about k-Fold CV is true for Stratified k-Fold technique. When choosing between different CV methods, make sure you are using the proper one. For example, you might think that your model performs badly simply because you are using k-Fold CV to validate the model which was trained on the dataset with a class imbalance. To avoid that you should always do a proper exploratory data analysis on your data.\n",
    "\n",
    "1. **Repeated k-Fold cross-validation:** Repeated k-Fold cross-validation or Repeated random sub-sampling CV is probably the most robust of all CV techniques in this paper. It is a variation of k-Fold but in the case of Repeated k-Folds k is not the number of folds. It is the number of times we will train the model. The general idea is that on every iteration we will randomly select samples all over the dataset as our test set. For example, if we decide that 20% of the dataset will be our test set, 20% of samples will be randomly selected and the rest 80% will become the training set. The algorithm of Repeated k-Fold technique:\n",
    "\n",
    "    - Pick k ‚Äì number of times the model will be trained\n",
    "    - Pick a number of samples which will be the test set\n",
    "    - Split the dataset\n",
    "    - Train on the training set. On each iteration of cross-validation, a new model must be trained\n",
    "    - Validate on the test set\n",
    "    - Save the result of the validation\n",
    "    - Repeat steps 3-6 k times\n",
    "    - To get the final score average the results that you got on step 6.\n",
    "    \n",
    "    Repeated k-Fold has clear advantages over standard k-Fold CV. Firstly, the proportion of train/test split is not dependent on the number of iterations. Secondly, we can even set unique proportions for every iteration. Thirdly, random selection of samples from the dataset makes Repeated k-Fold even more robust to selection bias. Still, there are some disadvantages. k-Fold CV guarantees that the model will be tested on all samples, whereas Repeated k-Fold is based on randomization which means that some samples may never be selected to be in the test set at all. At the same time, some samples might be selected multiple times. Thus making it a bad choice for imbalanced datasets. Sklearn will help you to implement a Repeated k-Fold CV. Just use `sklearn.model_selection.RepeatedKFold`. In sklearn implementation of this technique you must set the number of folds that you want to have (n_splits) and the number of times the split will be performed (n_repeats). It guarantees that you will have different folds on each iteration.\n",
    "\n",
    "1. **Leave-one-out cross-validation:** Leave-one-out —Åross-validation (LOOCV) is an extreme case of k-Fold CV. Imagine if k is equal to n where n is the number of samples in the dataset. Such k-Fold case is equivalent to Leave-one-out technique. The algorithm of LOOCV technique:\n",
    "\n",
    "    - Choose one sample from the dataset which will be the test set\n",
    "    - The remaining n ‚Äì 1 samples will be the training set\n",
    "    - Train the model on the training set. On each iteration, a new model must be trained\n",
    "    - Validate on the test set\n",
    "    - Save the result of the validation\n",
    "    - Repeat steps 1 ‚Äì 5 n times as for n samples we have n different training and test sets\n",
    "    - To get the final score average the results that you got on step 5.\n",
    "    \n",
    "    For LOOCV sklearn also has a built-in method. It can be found in the model_selection library ‚Äì sklearn.model_selection.LeaveOneOut.\n",
    "    \n",
    "    1. Advantages:\n",
    "        - The greatest advantage of Leave-one-out cross-validation is that it doesn‚Äôt waste much data. We use only one sample from the whole dataset as a test set, whereas the rest is the training set. \n",
    "        \n",
    "    1. Disadvantages:\n",
    "        - But when compared with k-Fold CV, LOOCV requires building n models instead of k models, when we know that n which stands for the number of samples in the dataset is much higher than k. It means LOOCV is more computationally expensive than k-Fold, it may take plenty of time to cross-validate the model using LOOCV. **Thus, the Data Science community has a general rule based on empirical evidence and different researches, which suggests that 5- or 10-fold cross-validation should be preferred over LOOCV.**\n",
    "\n",
    "1. **Leave-p-out cross-validation:** Leave-p-out cross-validation (LpOC) is similar to Leave-one-out CV as it creates all the possible training and test sets by using p samples as the test set. All mentioned about LOOCV is true and for LpOC. This approach leaves p data points out of training data, i.e. if there are n data points in the original sample then, n-p samples are used to train the model and p points are used as the validation set. This is repeated for all combinations in which original sample can be separated this way, and then the error is averaged for all trials, to give overall effectiveness. **This method is exhaustive in the sense that it needs to train and validate the model for all possible combinations, and for moderately large p, it can become computationally infeasible.** Still, it is worth mentioning that unlike LOOCV and k-Fold test sets will overlap for LpOC if p is higher than 1. The algorithm of LpOC technique:\n",
    "\n",
    "    - Choose p samples from the dataset which will be the test set\n",
    "    - The remaining n ‚Äì p samples will be the training set\n",
    "    - Train the model on the training set. On each iteration, a new model must be trained\n",
    "    - Validate on the test set\n",
    "    - Save the result of the validation\n",
    "    - Repeat steps 2 ‚Äì 5 Cpn times \n",
    "    - To get the final score average the results that you got on step 5\n",
    "    \n",
    "    You can perform Leave-p-out CV using sklearn ‚Äì `sklearn.model_selection.LeavePOut`. LpOC has all the disadvantages of the LOOCV, but, nevertheless, it‚Äôs as robust as LOOCV. A particular case of this method is when p = 1. This is known as Leave one out cross validation. **This method is generally preferred over the previous one because it does not suffer from the intensive computation, as number of possible combinations is equal to number of data points in original sample or n.**\n",
    "\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Weight Decay</span>\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">What is weight decay?</span>\n",
    "\n",
    "- Weight decay is a **regularization technique** by adding a small penalty, usually the L2 norm of the weights (all the weights of the model), to the loss function.  **`loss = loss + weight decay parameter * L2 norm of the weights`**\n",
    "\n",
    "- Weight decay **is a form of regularization that penalizes large weights in the network.** It does this by adding a term to the loss function that is proportional to the sum of the squared weights. This term reduces the magnitude of the weights and prevents them from growing too large. Weight decay can also be seen as a way of introducing prior knowledge that the weights should be small and smooth.\n",
    "\n",
    "Some people prefer to only apply weight decay to the weights and not the bias. **PyTorch applies weight decay to both weights and bias.**\n",
    "\n",
    "The weight decay is a regularization parameter that prevents the model weights from ‚Äòexploding‚Äô. Zeroing the weight decay for these parameters is usually done by default in various projects and frameworks, but it‚Äôs still worth checking since it is still not the default behavior for Pytorch.\n",
    "\n",
    "Weight decay essentially pulls the weights towards 0. **While this is beneficial for convolutional and linear layer weights, Batchnorm layer parameters are meant to scale (the gamma parameter) and shift (the beta parameter) the normalized input of the layer. As such, forcing these values to a lower value would affect the distribution and result in inferior results.**\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">How does weight decay work?</span>\n",
    "\n",
    "\n",
    "Weight decay works by updating the weights in the opposite direction of their current value, scaled by a factor called the weight decay rate. This factor determines how much the weights are shrunk at each step of the optimization. A higher weight decay rate means more regularization and less overfitting, but also less flexibility and more underfitting. A lower weight decay rate means less regularization and more overfitting, but also more flexibility and less underfitting. The optimal weight decay rate depends on the data, the model, and the learning rate.\n",
    "\n",
    " \n",
    "### <span style=\"color: #7b6b59;\">Why do we use weight decay?</span>\n",
    "\n",
    "1. **To prevent overfitting.**\n",
    "\n",
    "1. **To keep the weights small and avoid exploding gradient.** Because the L2 norm of the weights are added to the loss, each iteration of your network will try to optimize/minimize the model weights in addition to the loss. This will help keep the weights as small as possible, preventing the weights to grow out of control, and thus avoid exploding gradient.\n",
    "\n",
    "**What are the benefits of weight decay?**\n",
    "\n",
    "Weight decay offers a variety of advantages for neural network training and performance. It can reduce the variance of the model, which leads to better generalization ability. Additionally, weight decay prevents the weights from becoming too large, which could cause numerical instability or gradient explosion. Furthermore, it simplifies the model and makes it more interpretable. Finally, it can also improve the convergence speed and stability of the optimization algorithm.\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">What are the drawbacks of weight decay??</span>\n",
    "\n",
    "\n",
    "Weight decay has some drawbacks that should be taken into account. For instance, it adds an extra hyperparameter to tune, making the model selection process more complex and costly. Additionally, it can reduce the capacity and expressiveness of a model, especially for deep and complex networks. Furthermore, weight decay can interfere with the learning of sparse or important features since it treats all weights equally. Finally, it can cause underfitting if the weight decay rate is too high or if the data is noisy or insufficient.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">How do we use weight decay?</span>\n",
    "\n",
    "\n",
    "To use weight decay, we can simply define the weight decay parameter in the `torch.optim.SGD` optimizer or the `torch.optim.Adam optimizer`. \n",
    "\n",
    "<img width=\"941\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/94444aa1-11d7-40e9-a39f-a86e7f666d09\">\n",
    "\n",
    "Note that Adam uses a different equation for the loss. But the key concept is the same. Also, as I mentioned above that **PyTorch applies weight decay to both weights and bias.** If you would like to only use weights, you can use `model.named_parameters()` function. **`model.named_parameters()` also allows you to do more complex weight decay operations like using weight decay in different layers.**\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">How do you choose the weight decay rate?</span>\n",
    "\n",
    "Choosing the weight decay rate is a trade-off between regularization and flexibility. There is no universal formula or rule for setting the weight decay rate, as it depends on many factors, such as the data, the model, the learning rate, and the optimization algorithm. However, some general guidelines are to start with a small weight decay rate and increase it gradually until an improvement in validation or test performance is seen. Validation sets or cross-validation can be used to evaluate the effect of different rates on model performance and select the one that minimizes validation error. Additionally, grid searches or random searches can be employed to explore a range of rates and find the optimal one for a given problem. ***Finally, learning rate schedulers or adaptive optimizers can adjust the weight decay rate dynamically based on learning progress.***\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">How do you compare weight decay with other regularization methods?</span>\n",
    "\n",
    "Weight decay is one of the most common and effective regularization methods for neural networks, but it is not the only one. Other regularization methods that can be used alone or in combination with weight decay include noise injection, dropout, batch normalization, early stopping, and data augmentation. Each of these methods has its own advantages and disadvantages, making the best choice dependent on the specific problem and data. As a general rule, use weight decay as a baseline regularization method as it is simple, effective, and widely applicable. For noisy, sparse, or imbalanced data or for very deep or complex networks, noise injection or dropout is recommended. Batch normalization or early stopping should be used when a network suffers from slow or unstable convergence or when the learning rate is too high or too low. Data augmentation should be used when data is limited, simple, or homogeneous or when the network is very flexible or expressive.\n",
    "\n",
    "\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">2. Optimizers</span>\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "\n",
    "Many people may be using optimizers while training the neural network without knowing that the method is known as optimization. \n",
    "\n",
    "- Optimizers are algorithms or methods used to change the attributes of your neural network such as weights and learning rate in order to reduce the losses. How you should change your weights or learning rates of your neural network to reduce the losses is defined by the optimizers you use. Optimization algorithms or strategies are responsible for reducing the losses and to provide the most accurate results possible.\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Optimization Algorithms</span>\n",
    "\n",
    "We‚Äôll learn about different types of optimizers and their advantages:\n",
    "\n",
    "\n",
    "1. **Gradient Descent:** Gradient Descent is the most basic but most used optimization algorithm. It‚Äôs used heavily in linear regression and classification algorithms. Backpropagation in neural networks also uses a gradient descent algorithm. Gradient descent is a first-order optimization algorithm which is dependent on the first order derivative of a loss function. It calculates that which way the weights should be altered so that the function can reach a minima. Through backpropagation, the loss is transferred from one layer to another and the model‚Äôs parameters also known as weights are modified depending on the losses so that the loss can be minimized. algorithm: Œ∏=Œ∏‚àíŒ±‚ãÖ‚àáJ(Œ∏). Gradient descent is an optimization algorithm based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum. Gradient Descent iteratively reduces a loss function by moving in the direction opposite to that of steepest ascent. It is dependent on the derivatives of the loss function for finding minima. uses the data of the entire training set to calculate the gradient of the cost function to the parameters which requires large amount of memory and slows down the process. How big/small the steps are gradient descent takes into the direction of the local minimum are determined **by the learning rate**, which figures out how fast or slow we will move towards the optimal weights.\n",
    "\n",
    "    <img width=\"879\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/1ce01c91-6d49-45ab-aed5-05679edcbc01\">\n",
    "    \n",
    "    <img width=\"819\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/36cdb708-717a-47fe-83a4-dd63fe8c7af5\">\n",
    "\n",
    "    1. **Advantages:**\n",
    "    \n",
    "        - Easy computation.\n",
    "        - Easy to implement.\n",
    "        - Easy to understand.\n",
    "    1. **Disadvantages:**\n",
    "\n",
    "        - May trap at local minima.\n",
    "        - Weights are changed after calculating gradient on the whole dataset. So, if the dataset is too large than this may take years to converge to the minima. Because this method calculates the gradient for the entire data set in one update, the calculation is very slow.\n",
    "        - Requires large memory to calculate gradient on the whole dataset. It requires large memory and it is computationally expensive.\n",
    "\n",
    "1. **Stochastic Gradient Descent:** It‚Äôs a variant of Gradient Descent. It tries to update the model‚Äôs parameters more frequently. In this, the model parameters are altered after computation of loss on each training example. So, if the dataset contains 1000 rows SGD will update the model parameters 1000 times in one cycle of dataset instead of one time as in Gradient Descent. `Œ∏=Œ∏‚àíŒ±‚ãÖ‚àáJ(Œ∏;x(i);y(i)) , where {x(i) ,y(i)}` are the training examples. As the model parameters are frequently updated parameters have high variance and fluctuations in loss functions at different intensities. It is a variant of Gradient Descent. It update the model parameters one by one. If the model has 10K dataset SGD will update the model parameters 10k times.\n",
    "\n",
    "    <img width=\"812\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/4bbb77e3-5c50-4bbd-a467-b1e240970db2\">\n",
    "\n",
    "    1. **Advantages:**\n",
    "\n",
    "        - Frequent updates of model parameters hence, converges in less time.\n",
    "        - Requires less memory as no need to store values of loss functions.\n",
    "        - May get new minima‚Äôs.\n",
    "        - Allows the use of large data sets as it has to update only one example at a time.\n",
    "   \n",
    "    1. **Disadvantages:**\n",
    "\n",
    "        - High variance in model parameters.\n",
    "        - May shoot even after achieving global minima.\n",
    "        - To get the same convergence as gradient descent needs to slowly reduce the value of learning rate.\n",
    "        - The frequent can also result in noisy gradients which may cause the error to increase instead of decreasing it.\n",
    "        - Frequent updates are computationally expensive.\n",
    "\n",
    "1. **Mini-Batch Gradient Descent:** It‚Äôs best among all the variations of gradient descent algorithms. It is an improvement on both SGD and standard gradient descent. It updates the model parameters after every batch. So, the dataset is divided into various batches and after every batch, the parameters are updated. `Œ∏=Œ∏‚àíŒ±‚ãÖ‚àáJ(Œ∏; B(i)), where {B(i)}` are the batches of training examples. It is a combination of the concepts of SGD and batch gradient descent. It simply splits the training dataset into small batches and performs an update for each of those batches. This creates a balance between the robustness of stochastic gradient descent and the efficiency of batch gradient descent. it can reduce the variance when the parameters are updated, and the convergence is more stable. It splits the data set in batches in between 50 to 256 examples, chosen at random.\n",
    "\n",
    "    1. **Advantages:**\n",
    "\n",
    "        - Frequently updates the model parameters and also has less variance.\n",
    "        - Requires medium amount of memory.\n",
    "        - It leads to more stable convergence.\n",
    "        - more efficient gradient calculations.\n",
    "        \n",
    "    1. **All types of Gradient Descent have some challenges:**\n",
    "\n",
    "        - Choosing an optimum value of the learning rate. If the learning rate is too small than gradient descent may take ages to converge. If the learning rate is too small, the convergence rate will be slow. If it is too large, the loss function will oscillate or even deviate at the minimum value.\n",
    "\n",
    "        - Have a constant learning rate for all the parameters. There may be some parameters which we may not want to change at the same rate.\n",
    "        - May get trapped at local minima. Mini-batch gradient descent does not guarantee good convergence,\n",
    "\n",
    "1. **Momentum:** Momentum was invented for reducing high variance in SGD and softens the convergence. It accelerates the convergence towards the relevant direction and reduces the fluctuation to the irrelevant direction. One more hyperparameter is used in this method known as momentum symbolized by ‚ÄòŒ≥‚Äô. `V(t)=Œ≥V(t‚àí1)+Œ±.‚àáJ(Œ∏)`. Now, the weights are updated by `Œ∏=Œ∏‚àíV(t)`. The momentum term `Œ≥` is usually set to 0.9 or a similar value.\n",
    "\n",
    "    1. **Advantages:**\n",
    "\n",
    "        - Reduces the oscillations and high variance of the parameters.\n",
    "        - Converges faster than gradient descent.\n",
    "\n",
    "    1. **Disadvantages:**\n",
    "\n",
    "        - One more hyper-parameter is added which needs to be selected manually and accurately.\n",
    "\n",
    "1. **Nesterov Accelerated Gradient:** Momentum may be a good method but if the momentum is too high the algorithm may miss the local minima and may continue to rise up. So, to resolve this issue the NAG algorithm was developed. It is a look ahead method. We know we‚Äôll be using `Œ≥V(t‚àí1)` for modifying the weights so, `Œ∏‚àíŒ≥V(t‚àí1)` approximately tells us the future location. Now, we‚Äôll calculate the cost based on this future parameter rather than the current one. `V(t)=Œ≥V(t‚àí1)+Œ±. ‚àáJ( Œ∏‚àíŒ≥V(t‚àí1) )` and then update the parameters using `Œ∏=Œ∏‚àíV(t)`. \n",
    "     \n",
    "     <img width=\"434\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/c9351e81-bce5-4717-8654-27b300497341\">\n",
    " \n",
    "     1. **Advantages:**\n",
    "        \n",
    "        - Does not miss the local minima.\n",
    "        - Slows if minima‚Äôs are occurring.\n",
    "    \n",
    "     1. **Disadvantages:**\n",
    "\n",
    "        - Still, the hyperparameter needs to be selected manually.\n",
    "   \n",
    "1. **Adagrad**: One of the disadvantages of all the optimizers explained is that the learning rate is constant for all parameters and for each cycle. This optimizer changes the learning rate. It changes the learning rate `‚ÄòŒ∑‚Äô` for each parameter and at every time step `‚Äòt‚Äô`. It‚Äôs a type second order optimization algorithm. It works on the derivative of an error function.\n",
    "    \n",
    "    <img width=\"564\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/6910d495-c191-4857-9360-e1402b7c6023\">\n",
    "    \n",
    "    `Œ∑` is a learning rate which is modified for given parameter `Œ∏(i)` at a given time based on previous gradients calculated for given parameter `Œ∏(i)`. We store the sum of the squares of the gradients w.r.t. `Œ∏(i)` up to time step `t`, while `œµ` is a smoothing term that avoids division by zero (usually on the order of 1e‚àí8). Interestingly, without the square root operation, the algorithm performs much worse. **It makes big updates for less frequent parameters and a small step for frequent parameters.**\n",
    "\n",
    "    1. **Advantages:**\n",
    "\n",
    "        - Learning rate changes for each training parameter.\n",
    "        - Don‚Äôt need to manually tune the learning rate.\n",
    "        - Able to train on sparse data.\n",
    "\n",
    "    1. **Disadvantages:**\n",
    "\n",
    "        - Computationally expensive as a need to calculate the second order derivative.\n",
    "\n",
    "1. **AdaDelta:** It is an extension of AdaGrad which tends to remove the decaying learning Rate problem of it. Instead of accumulating all previously squared gradients, Adadelta limits the window of accumulated past gradients to some fixed size w. In this exponentially moving average is used rather than the sum of all the gradients.\n",
    "\n",
    "    1. **Advantages:**\n",
    "\n",
    "        - Now the learning rate does not decay and the training does not stop.\n",
    "        \n",
    "    1. **Disadvantages:**\n",
    "        - Computationally expensive.\n",
    "\n",
    "1. **Adam:**  Adam (Adaptive Moment Estimation) works with momentums of first and second order. The intuition behind the Adam is that we don‚Äôt want to roll so fast just because we can jump over the minimum, we want to decrease the velocity a little bit for a careful search. In addition to storing an exponentially decaying average of past squared gradients like AdaDelta, Adam also keeps an exponentially decaying average of past gradients.\n",
    "\n",
    "    1. **Advantages:**\n",
    "\n",
    "        - The method is too fast and converges rapidly.\n",
    "        - Rectifies vanishing learning rate, high variance.\n",
    "\n",
    "    1. **Disadvantages:**\n",
    "\n",
    "        - Computationally costly.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Conclusions</span>\n",
    "\n",
    "- **Adam is the best optimizers. If one wants to train the neural network in less time and more efficiently than Adam is the optimizer.**\n",
    "\n",
    "- **For sparse data use the optimizers with dynamic learning rate.**\n",
    "\n",
    "- **If, want to use gradient descent algorithm than min-batch gradient descent is the best option.The learning rate is always decreasing results in slow training.**\n",
    "\n",
    "**How to choose optimizers?**\n",
    "\n",
    "1. If the data is sparse, use the self-applicable methods, namely Adagrad, Adadelta, RMSprop, Adam.\n",
    "\n",
    "1. RMSprop, Adadelta, Adam have similar effects in many cases.\n",
    "\n",
    "1. Adam just added bias-correction and momentum on the basis of RMSprop,\n",
    "\n",
    "1. As the gradient becomes sparse, Adam will perform better than RMSprop.\n",
    "\n",
    "\n",
    "<img width=\"911\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/110e15de-29a7-4a7b-8850-211ba8d8c0ce\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b8e17c",
   "metadata": {
    "papermill": {
     "duration": 0.032351,
     "end_time": "2024-01-18T17:11:16.309829",
     "exception": false,
     "start_time": "2024-01-18T17:11:16.277478",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:24px;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Advanced Approaches: Fine-tune a pre-trained model</div>\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "**What does fine-tuning a pre-trained model mean?** \n",
    "\n",
    "The fine-tuning technique is used to optimize a model‚Äôs performance on a new or different task. It is used to tailor a model to meet a specific need or domain, say cancer detection, in the field of healthcare. Pre-trained models are fine-tuned by training them on large amounts of labeled data for a certain task, such as Natural Language Processing (NLP) or image classification. Once trained, the model can be applied to similar new tasks or datasets with limited labeled data by fine-tuning the pre-trained model.\n",
    "\n",
    "The fine-tuning process is commonly used in transfer learning, where a pre-trained model is used as a starting point to train a new model for a contrasting but related task. A pre-trained model can significantly diminish the labeled data required to train a new model, making it an effective tool for tasks where labeled data is scarce or expensive.\n",
    "\n",
    "**How does fine-tuning pre-trained models work?**\n",
    "\n",
    "Fine-tuning a pre-trained model works by updating the parameters utilizing the available labeled data instead of starting the training process from the ground up. The following are the generic steps involved in fine-tuning:\n",
    "\n",
    "1. **Loading the pre-trained model:** The initial phase in the process is to select and load the right model, which has already been trained on a large amount of data, for a related task.\n",
    "\n",
    "1. **Modifying the model for the new task - Adjust the Architecture:** Once a pre-trained model is loaded, its top layers must be replaced or retrained to customize it for the new task. Adapting the pre-trained model to new data is necessary because the top layers are often task specific. After selecting the pre-trained model, you need to make modifications to the model‚Äôs architecture to fit the requirements of your specific task. This typically involves modifying the top layers of the model. For example, you may need to change the number of output neurons in the final layer to match the number of classes in your classification task.\n",
    "\n",
    "1. **Freezing particular layers:** The earlier layers facilitating low-level feature extraction are usually frozen in a pre-trained model. Since these layers have already learned general features that are useful for various tasks, freezing them may allow the model to preserve these features, avoiding overfitting the limited labeled data available in the new task. Depending on the complexity of your task and the size of your dataset, you can choose to freeze some layers in the pre-trained model. Freezing a layer means preventing it from updating its weights during the fine-tuning process. This can be beneficial if the lower layers of the pre-trained model have already learned general features that are useful for your task. On the other hand, unfreezing allows the corresponding layers to adapt to the new data during fine-tuning.\n",
    "\n",
    "1. **Training the new layers:** With the labeled data available for the new task, the newly created layers are then trained, all the while keeping the weights of the earlier layers constant. As a result, the model‚Äôs parameters can be adapted to the new task, and its feature representations can be refined. Once you have adjusted the architecture and decided which layers to freeze or unfreeze, it‚Äôs time to train the modified model on your task-specific dataset. During training, it‚Äôs advisable to use a smaller learning rate than what was used in the initial pre-training phase. This helps prevent drastic changes to the already learned representations while allowing the model to adapt to the new data.\n",
    "\n",
    "1. **Fine-tuning the model:** Once the new layers are trained, you can fine-tune the entire model on the new task using the available limited data. Every task and dataset is unique, and it may require further experimentation with hyperparameters, loss functions, and other training strategies. Fine-tuning is not a one-size-fits-all approach, and you may need to iterate and fine-tune your fine-tuning strategy to achieve optimal results.\n",
    "\n",
    "**Understanding fine-tuning with an example**\n",
    "\n",
    "Suppose you have a pre-trained model trained on a wide range of medical data or images that can detect abnormalities like tumors and want to adapt the model for a specific use case, say identifying a rare type of cancer, but you have a limited set of labeled data available. In such a case, you must fine-tune the model by adding new layers on top of the pre-trained model and training the newly added layers with the available data. Typically, the earlier layers of a pre-trained model, which extract low-level features, are frozen to prevent overfitting.\n",
    "\n",
    "**Best practices to follow when fine-tuning a pre-trained model**\n",
    "\n",
    "While fine-tuning a pre-trained model, several best practices can help ensure successful outcomes. Here are some key practices to follow:\n",
    "\n",
    "1. **Understand the pre-trained model:** Gain a comprehensive understanding of the pre-trained model architecture, its strengths, limitations, and the task it was initially trained on. This knowledge can enhance the fine-tuning process and help make appropriate modifications.\n",
    "\n",
    "1. **Select a relevant pre-trained model:** Choose a pre-trained model that aligns closely with the target task or domain. A model trained on similar data or a related task will provide a better starting point for fine-tuning.\n",
    "\n",
    "1. **Freeze early layers:** Typically, the lower layers of a pre-trained model capture generic features and patterns. Freeze these early layers during fine-tuning to preserve the learned representations. This practice helps prevent catastrophic forgetting and lets the model focus on task-specific fine-tuning.\n",
    "\n",
    "1. **Adjust learning rate**: Experiment with different learning rates during fine-tuning. It is typical to use a smaller learning rate compared to the initial pre-training phase. A lower learning rate allows the model to adapt more gradually and prevent drastic changes that could lead to overfitting.\n",
    "\n",
    "1. **Utilize transfer learning techniques:** Transfer learning methods can enhance fine-tuning performance. Techniques like feature extraction, where pre-trained layers are used as fixed feature extractors, or gradual unfreezing, where layers are unfrozen gradually during training, can help preserve and transfer valuable knowledge.\n",
    "\n",
    "1. **Regularize the model:** Apply regularization techniques, **such as dropout or weight decay,** during fine-tuning to prevent overfitting. Regularization helps the model generalize better and reduces the risk of memorizing specific training examples.\n",
    "\n",
    "1. **Monitor and evaluate performance:** Continuously monitor and evaluate the performance of the fine-tuned model on validation or holdout datasets. Use appropriate evaluation metrics to assess the model‚Äôs progress and make informed decisions on further fine-tuning adjustments.\n",
    "\n",
    "1. **Data augmentation:** Augment the training data by applying transformations, perturbations, or adding noise. Data augmentation can increase the diversity and generalizability of the training data, leading to better fine-tuning results.\n",
    "\n",
    "1. **Consider domain adaptation:** If the target task or domain significantly differs from the pre-training data, consider domain adaptation techniques. These methods aim to bridge the gap between the pre-training data and the target data, improving the model‚Äôs performance on the specific task.\n",
    "\n",
    "1. **Regularly backup and save checkpoints:** Save model checkpoints at regular intervals during fine-tuning to ensure progress is saved and prevent data loss. This practice allows for easy recovery and enables the exploration of different fine-tuning strategies.\n",
    "\n",
    "There are two ways to do it: Since we are looking to fine-tune the model for a downstream task like classification, we can directly use:\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">1. A simple way</span>\n",
    "\n",
    "**Fine-tuning pretrained NLP models with Huggingface‚Äôs Trainer:** *A simple way to fine-tune pretrained NLP models without native Pytorch or Tensorflow*\n",
    "\n",
    "While working on a data science competition, I was fine-tuning a pre-trained model and realised how tedious it was to fine-tune a model using native PyTorch or Tensorflow. I experimented with Huggingface‚Äôs **Trainer API** and was surprised by how easy it was.\n",
    "\n",
    "- **Train Our Classification Model:** Now that our input data is properly formatted, it‚Äôs time to fine tune the pre-trained model, for instance a BERT model.\n",
    "    - For this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n",
    "\n",
    "    - **Classification Head:** Finally, the output from the pooler is passed through the classification head, which simply involves projecting the pooled embedding into a space with dimensionality equal to the number of different classes. It is called a head because this component of the model can be swapped out to suit a particular task. This is in contrast to the backbone of BERT ‚Äî responsible for creating the contextualized representations of the tokens in the sequence ‚Äî that remains the same regardless of the task.\n",
    "    - Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task. [Here](https://huggingface.co/transformers/v2.2.0/model_doc/bert.html) is the current list of classes provided for fine-tuning.\n",
    "    \n",
    "    - The `BertForSequenceClassification` class is the outermost class that we call to instantiate our BERT model. It houses both the base architecture (self.bert) and the classification head (self.classifier). The outputs are the logits for which there is one value for each class. Taking the maximum value of these logits will give us the predicted class. However, if it is desired to interpret the logits as probabilities the softmax function will need to be applied. `BertForSequenceClassification` performs fine-tuning of logistic regression layer on the output dimension of 768.\n",
    "   \n",
    "    - We‚Äôll be using `BertForSequenceClassification`. This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n",
    "    \n",
    "    - So, in summary, we fine-tune the entire pre-trained BERT model, including the last layers specifically designed for our classification task. It adjusts the model to our document classification problem using the data we provide, but it doesn't train the model entirely from scratch. The distinction is that \"from scratch\" would mean initializing all the model's weights randomly and learning them solely from our data, which usually requires a much larger dataset and more computational resources. Here, we're leveraging the general understanding already embedded in the BERT model from its pre-training, which provides a significant head start for most NLP tasks.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">2. Adding Custom Layers on Top of a Hugging Face Model</span>\n",
    "\n",
    "Alternatively, we can define a custom module, that created a bert model based on the pre-trained weights and adds layers on top of it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a29e34",
   "metadata": {
    "papermill": {
     "duration": 0.031667,
     "end_time": "2024-01-18T17:11:16.374792",
     "exception": false,
     "start_time": "2024-01-18T17:11:16.343125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Approach 3: Fine-tune a pre-trained model with ü§ó Transformers</div>\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">1. Introduction</span>\n",
    "\n",
    "There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. ü§ó Transformers provides access to thousands of pretrained models for a wide range of tasks. **When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning,** an incredibly powerful training technique. In this section, we will fine-tune a pretrained model with a deep learning framework of our choice:\n",
    "\n",
    "- Fine-tune a pretrained model with ü§ó Transformers PyTorch Trainer.\n",
    "- Fine-tune a pretrained model in TensorFlow with Keras.\n",
    "- Fine-tune a pretrained model in native PyTorch.\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">2. Create a dataset or Prepare the dataset</span>\n",
    "\n",
    "- **From in-memory data:** Eventually, it‚Äôs also possible to instantiate a datasets.Dataset directly from in-memory data, currently one or:\n",
    "    - a python dict, or\n",
    "    - a pandas dataframe.\n",
    "\n",
    "A `datasets.Dataset` instance is more precisely a table with rows and columns in which the columns are typed. Querying an example (a single row) will thus return a python dictionary with keys corresponding to columns names, and values corresponding to the example‚Äôs value for each column.\n",
    "\n",
    "You can get the number of rows and columns of the dataset with various standard attributes. \n",
    "\n",
    "Sometimes, you may need to create a dataset if you‚Äôre working with your own data. Creating a dataset with **ü§ó Datasets confers all the advantages of the library to your dataset: fast loading and processing, stream enormous datasets, memory-mapping, and more.** You can easily and rapidly create a dataset with ü§ó Datasets low-code approaches, reducing the time it takes to start training a model. In many cases, it is as easy as dragging and dropping your data files into a dataset repository on the Hub.\n",
    "\n",
    "Creating a `Dataset` object from our dataset when fine-tuning a pre-trained model with Hugging Face Transformers is important for several reasons:\n",
    "\n",
    "1. **Efficiency:** The `Dataset` object is optimized for performance. It enables efficient data loading, preprocessing, and iteration, which is crucial when dealing with large datasets common in NLP tasks.\n",
    "\n",
    "1. **Easy Integration:** Hugging Face Transformers and Datasets libraries are designed to work together seamlessly. By using a `Dataset` object, we can directly apply transformations, tokenization, and batching, which are necessary for preparing our data for the model.\n",
    "\n",
    "1. **Consistency and Reproducibility:** Creating a `Dataset` object ensures that data processing steps are consistent. This is important for reproducibility of results, a key aspect of any scientific experiment. You can share your dataset with others, and they'll be able to achieve the same results using the same preprocessing steps.\n",
    "\n",
    "1. **Advanced Features:** The Dataset object comes with many advanced features like easy slicing, indexing, and even complex transformations. It supports operations like `map`, `filter`, and `shuffle`, which are essential for training neural networks.\n",
    "\n",
    "1. **Scalability:** Datasets in Hugging Face are designed to be scalable. They can handle datasets much larger than your system's RAM and facilitate distributed training by efficiently managing memory and processing.\n",
    "\n",
    "1. **Community Standards:** Using widely adopted standards like the Dataset object from Hugging Face ensures that your work is accessible and understandable by a broader community. It also makes it easier for you to use datasets and models shared by others.\n",
    "\n",
    "In essence, **using a `Dataset` object simplifies the data preprocessing pipeline, ensures efficient and reproducible training, and aligns your work with community practices.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "157e0f30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:11:16.440453Z",
     "iopub.status.busy": "2024-01-18T17:11:16.439416Z",
     "iopub.status.idle": "2024-01-18T17:11:16.766326Z",
     "shell.execute_reply": "2024-01-18T17:11:16.765233Z"
    },
    "papermill": {
     "duration": 0.362506,
     "end_time": "2024-01-18T17:11:16.768886",
     "exception": false,
     "start_time": "2024-01-18T17:11:16.406380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the train dataset is: (40151, 5)\n",
      "---------------------------------------------------\n",
      "The number of columns in the train dataset is: 5\n",
      "The column names are: ['text', 'label', 'prompt_name', 'source', 'RDizzl3_seven']\n",
      "The columns' detailed types are: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'prompt_name': Value(dtype='string', id=None), 'source': Value(dtype='string', id=None), 'RDizzl3_seven': Value(dtype='bool', id=None)}\n",
      "---------------------------------------------------\n",
      "The number of rows in the train dataset is: 40151\n",
      "Or the length of the train dataset is: 40151\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "valid_dataset = Dataset.from_pandas(valid_df)\n",
    "\n",
    "print(f\"The shape of the train dataset is: {train_dataset.shape}\")\n",
    "print(\"---------------------------------------------------\")\n",
    "\n",
    "print(f\"The number of columns in the train dataset is: {train_dataset.num_columns}\")\n",
    "print(f\"The column names are: {train_dataset.column_names}\")\n",
    "print(f\"The columns' detailed types are: {train_dataset.features}\")\n",
    "\n",
    "print(\"---------------------------------------------------\")\n",
    "print(f\"The number of rows in the train dataset is: {train_dataset.num_rows}\")\n",
    "print(f\"Or the length of the train dataset is: {len(train_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "254520e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:11:16.834292Z",
     "iopub.status.busy": "2024-01-18T17:11:16.833442Z",
     "iopub.status.idle": "2024-01-18T17:11:16.838305Z",
     "shell.execute_reply": "2024-01-18T17:11:16.837265Z"
    },
    "papermill": {
     "duration": 0.039277,
     "end_time": "2024-01-18T17:11:16.840296",
     "exception": false,
     "start_time": "2024-01-18T17:11:16.801019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# While you can access a single row with the train_dataset[i] pattern, \n",
    "# you can also access several rows using slice notation or with a list of indices (or a numpy/torch/tf array of indices):\n",
    "#print(train_dataset[1])\n",
    "#print(\"--------------------------------\\n\")\n",
    "#print(train_dataset[:2])\n",
    "#print(\"--------------------------------\\n\")\n",
    "#print(train_dataset[\"text\"][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed9eeb0",
   "metadata": {
    "papermill": {
     "duration": 0.031466,
     "end_time": "2024-01-18T17:11:16.903492",
     "exception": false,
     "start_time": "2024-01-18T17:11:16.872026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color: #7b6b59;\">3. Initialise pre-trained model and tokenizer</span>\n",
    "\n",
    "Before we can fine-tune a pretrained model, we have to prepare it for training. As you now know, we need a tokenizer to process the text and include a **padding** and **truncation** strategy to handle any variable sequence lengths. To process our dataset in one step, use ü§ó Datasets `map` method to apply a preprocessing function over the entire dataset:\n",
    "\n",
    "To feed our text to deberta, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary.\n",
    "\n",
    "The tokenization must be performed by the tokenizer included with deberta‚Äìthe below cell will download this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69a14f21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:11:16.968222Z",
     "iopub.status.busy": "2024-01-18T17:11:16.967814Z",
     "iopub.status.idle": "2024-01-18T17:11:19.152185Z",
     "shell.execute_reply": "2024-01-18T17:11:19.150922Z"
    },
    "papermill": {
     "duration": 2.220406,
     "end_time": "2024-01-18T17:11:19.154888",
     "exception": false,
     "start_time": "2024-01-18T17:11:16.934482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dca366dc81841768bc28f7368a1c535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a78fae2863452aa2322cbe53c97efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/578 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105cbbf38c3646f49277de47a4070c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-xsmall\", use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc51a42",
   "metadata": {
    "papermill": {
     "duration": 0.031894,
     "end_time": "2024-01-18T17:11:19.220135",
     "exception": false,
     "start_time": "2024-01-18T17:11:19.188241",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Since we are using a pretrained model, we need to ensure that the input data is in the same form as what the pretrained model was trained on. Thus, we would need to instantiate the tokenizer using the name of the model.\n",
    "\n",
    "Now that the model and tokenizer have been initialised, we can proceed to preprocess the data.\n",
    "\n",
    "**Preprocess text using pretrained tokenizer**\n",
    "\n",
    "Let us preprocess the text using the tokenizer intialised earlier.\n",
    "\n",
    "The input text that we are using for the tokenizer is a list of strings.\n",
    "\n",
    "We have set `padding=True`, `truncation=True`, `max_length=128` so that we can get same length inputs for the model- the long texts will be truncated to 128 tokens while the short texts will have extra tokens added to make it 128 tokens.\n",
    "\n",
    "128 tokens is used because this is the maximum token length that the pre-trained model can take.\n",
    "\n",
    "After tokenizing your text, you will get a python dictionary with 3 keys:\n",
    "\n",
    "- Input_ids\n",
    "- token_type_ids\n",
    "- attention_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c99f30e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:11:19.285733Z",
     "iopub.status.busy": "2024-01-18T17:11:19.285314Z",
     "iopub.status.idle": "2024-01-18T17:11:19.290522Z",
     "shell.execute_reply": "2024-01-18T17:11:19.289530Z"
    },
    "papermill": {
     "duration": 0.040833,
     "end_time": "2024-01-18T17:11:19.292780",
     "exception": false,
     "start_time": "2024-01-18T17:11:19.251947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_function(samples):\n",
    "    return tokenizer(samples[\"text\"], max_length=128, padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ed32a477",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:11:19.358168Z",
     "iopub.status.busy": "2024-01-18T17:11:19.357780Z",
     "iopub.status.idle": "2024-01-18T17:12:02.174016Z",
     "shell.execute_reply": "2024-01-18T17:12:02.173041Z"
    },
    "papermill": {
     "duration": 42.852074,
     "end_time": "2024-01-18T17:12:02.176232",
     "exception": false,
     "start_time": "2024-01-18T17:11:19.324158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2195eda59b8a4586aac98376bd71dfb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dde08a95900484196d493facf8ce29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_valid_dataset = valid_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d1b7e",
   "metadata": {
    "papermill": {
     "duration": 0.032908,
     "end_time": "2024-01-18T17:12:02.242956",
     "exception": false,
     "start_time": "2024-01-18T17:12:02.210048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color: #7b6b59;\">4. Train with PyTorch Trainer</span>\n",
    "\n",
    "ü§ó Transformers provides a Trainer class optimized for training ü§ó Transformers models, making it easier to start training without manually writing your own training loop. The Trainer API supports a wide range of training options and features such as logging, gradient accumulation, and mixed precision.\n",
    "\n",
    "1. **Start by loading your model and specify the number of expected labels.**: You will see a warning about some of the pretrained weights not being used and some weights being randomly initialized. Don‚Äôt worry, this is completely normal! The pretrained head of the BERT model is discarded, and replaced with a randomly initialized classification head. You will fine-tune this new model head on your sequence classification task, transferring the knowledge of the pretrained model to it.\n",
    "1. **Training hyperparameters:** Next, create a `TrainingArguments` class which contains all the hyperparameters you can tune as well as flags for activating different training options. For this tutorial you can start with the default training hyperparameters, but feel free to experiment with these to find your optimal settings.\n",
    "1. **Evaluate:** `Trainer` does not automatically evaluate model performance during training. You‚Äôll need to pass Trainer a function to compute and report metrics. \n",
    "1. **Trainer:** Create a `Trainer` object with your model, training arguments, training and test datasets, and evaluation function. Then fine-tune your model by calling `train()`.\n",
    "\n",
    "For this task, we first want to modify the pre-trained Deberta model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n",
    "\n",
    "Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained Deberta model, each has different top layers and output types designed to accomodate their specific NLP task. We‚Äôll be using `AutoModelForSequenceClassification`. This is the normal Deberta model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task. Have also a look on [BertForSequenceClassification source code](https://huggingface.co/transformers/v3.0.2/_modules/transformers/modeling_bert.html#BertForSequenceClassification).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ff46955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:02.309991Z",
     "iopub.status.busy": "2024-01-18T17:12:02.309577Z",
     "iopub.status.idle": "2024-01-18T17:12:04.248324Z",
     "shell.execute_reply": "2024-01-18T17:12:04.247226Z"
    },
    "papermill": {
     "duration": 1.974778,
     "end_time": "2024-01-18T17:12:04.251074",
     "exception": false,
     "start_time": "2024-01-18T17:12:02.276296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638f70fe8e2c4f0281f4ba1e7c1cf9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-xsmall and are newly initialized: ['classifier.weight', 'pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DebertaV2ForSequenceClassification(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 384, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (key_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (value_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "              (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 384)\n",
       "      (LayerNorm): LayerNorm((384,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (pooler): ContextPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (dropout): StableDropout()\n",
       "  )\n",
       "  (classifier): Linear(in_features=384, out_features=2, bias=True)\n",
       "  (dropout): StableDropout()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"microsoft/deberta-v3-xsmall\", num_labels=2)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be954306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:04.319686Z",
     "iopub.status.busy": "2024-01-18T17:12:04.318895Z",
     "iopub.status.idle": "2024-01-18T17:12:04.326633Z",
     "shell.execute_reply": "2024-01-18T17:12:04.325615Z"
    },
    "papermill": {
     "duration": 0.044414,
     "end_time": "2024-01-18T17:12:04.328907",
     "exception": false,
     "start_time": "2024-01-18T17:12:04.284493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2509"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_name = \"roc_auc\"\n",
    "train_batch_size = 4\n",
    "eval_batch_size = 32\n",
    "grad_acc = 4\n",
    "num_steps = len(train_df) // (train_batch_size * grad_acc)\n",
    "num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d4abc",
   "metadata": {
    "papermill": {
     "duration": 0.033538,
     "end_time": "2024-01-18T17:12:04.395438",
     "exception": false,
     "start_time": "2024-01-18T17:12:04.361900",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Defining TrainingArguments and Trainer**\n",
    "\n",
    "Here is where the magic of the Trainer function is. We can define the training parameters in the TrainingArguments and Trainer class as well as train the model with a single command.\n",
    "\n",
    "We need to first define a function to calculate the metrics of the validation set. Since this is a binary classification problem, we can use accuracy, precision, recall and f1 score.\n",
    "\n",
    "Next, we specify some training parameters, set the pretrained model, train data and evaluation data in the TrainingArgs and Trainer class.\n",
    "\n",
    "After we have defined the parameters , simply run `trainer.train()` to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ade06ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:04.465625Z",
     "iopub.status.busy": "2024-01-18T17:12:04.465216Z",
     "iopub.status.idle": "2024-01-18T17:12:04.474072Z",
     "shell.execute_reply": "2024-01-18T17:12:04.472957Z"
    },
    "papermill": {
     "duration": 0.046868,
     "end_time": "2024-01-18T17:12:04.476391",
     "exception": false,
     "start_time": "2024-01-18T17:12:04.429523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"deberta-v3-xsmall_finetuned\",\n",
    "    evaluation_strategy=\"steps\", # If you‚Äôd like to monitor your evaluation metrics during fine-tuning, specify the evaluation_strategy parameter in your training arguments to report the evaluation metric at the end of each epoch\n",
    "    save_strategy = \"steps\",\n",
    "    eval_steps = num_steps // 3,\n",
    "    save_steps = num_steps // 3,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    gradient_accumulation_steps=grad_acc,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=metric_name,\n",
    "    report_to='none', # change to wandb after enabling internet access\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fca5b566",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:04.549501Z",
     "iopub.status.busy": "2024-01-18T17:12:04.548502Z",
     "iopub.status.idle": "2024-01-18T17:12:04.555226Z",
     "shell.execute_reply": "2024-01-18T17:12:04.553957Z"
    },
    "papermill": {
     "duration": 0.045489,
     "end_time": "2024-01-18T17:12:04.557529",
     "exception": false,
     "start_time": "2024-01-18T17:12:04.512040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    probs = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
    "    auc = roc_auc_score(labels, probs[:,1], multi_class='ovr')\n",
    "    return {\"roc_auc\": auc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a71d979",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:04.629481Z",
     "iopub.status.busy": "2024-01-18T17:12:04.629103Z",
     "iopub.status.idle": "2024-01-18T17:12:04.643992Z",
     "shell.execute_reply": "2024-01-18T17:12:04.642852Z"
    },
    "papermill": {
     "duration": 0.053836,
     "end_time": "2024-01-18T17:12:04.646522",
     "exception": false,
     "start_time": "2024-01-18T17:12:04.592686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_valid_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb40ce21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:04.717799Z",
     "iopub.status.busy": "2024-01-18T17:12:04.717372Z",
     "iopub.status.idle": "2024-01-18T17:12:04.722056Z",
     "shell.execute_reply": "2024-01-18T17:12:04.721022Z"
    },
    "papermill": {
     "duration": 0.043124,
     "end_time": "2024-01-18T17:12:04.724343",
     "exception": false,
     "start_time": "2024-01-18T17:12:04.681219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad0ec11",
   "metadata": {
    "papermill": {
     "duration": 0.0344,
     "end_time": "2024-01-18T17:12:04.793659",
     "exception": false,
     "start_time": "2024-01-18T17:12:04.759259",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color: #7b6b59;\">5. Making prediction</span>\n",
    "\n",
    "After the model is trained, we repeat the same steps for the test data:\n",
    "\n",
    "1. Tokenize test data with pretrained tokenizer\n",
    "1. Create torch dataset\n",
    "1. Load trained model\n",
    "1. Define Trainer\n",
    "\n",
    "To load the trained model from the previous steps, set the model_path to the path containing the trained model weights.\n",
    "\n",
    "To make prediction, only a single command is needed as well `test_trainer.predict(test_dataset)` .\n",
    "\n",
    "After making a prediction, you will only get the raw prediction. Additional preprocessing steps will be needed to get it to a usable format.\n",
    "\n",
    "Since the task is just a simple sequence classification task, we can just obtain the argmax across axis 1. Note that other NLP tasks may require different ways to preprocess the raw predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae9ce880",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:04.863113Z",
     "iopub.status.busy": "2024-01-18T17:12:04.862393Z",
     "iopub.status.idle": "2024-01-18T17:12:04.866929Z",
     "shell.execute_reply": "2024-01-18T17:12:04.865862Z"
    },
    "papermill": {
     "duration": 0.04169,
     "end_time": "2024-01-18T17:12:04.869154",
     "exception": false,
     "start_time": "2024-01-18T17:12:04.827464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
    "# test_ds = Dataset.from_pandas(test)\n",
    "# test_ds_enc = test_ds.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b0dfb8da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:04.940203Z",
     "iopub.status.busy": "2024-01-18T17:12:04.939135Z",
     "iopub.status.idle": "2024-01-18T17:12:04.943868Z",
     "shell.execute_reply": "2024-01-18T17:12:04.942951Z"
    },
    "papermill": {
     "duration": 0.042921,
     "end_time": "2024-01-18T17:12:04.945912",
     "exception": false,
     "start_time": "2024-01-18T17:12:04.902991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#test_preds = trainer.predict(test_ds_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "487603b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:05.014079Z",
     "iopub.status.busy": "2024-01-18T17:12:05.013686Z",
     "iopub.status.idle": "2024-01-18T17:12:05.018332Z",
     "shell.execute_reply": "2024-01-18T17:12:05.017313Z"
    },
    "papermill": {
     "duration": 0.041425,
     "end_time": "2024-01-18T17:12:05.020604",
     "exception": false,
     "start_time": "2024-01-18T17:12:04.979179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# logits = test_preds.predictions\n",
    "# probs = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
    "# sub = pd.DataFrame()\n",
    "# sub['id'] = test['id']\n",
    "# sub['generated'] = probs[:,1]\n",
    "# sub.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48706685",
   "metadata": {
    "papermill": {
     "duration": 0.033872,
     "end_time": "2024-01-18T17:12:05.088718",
     "exception": false,
     "start_time": "2024-01-18T17:12:05.054846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Approach 4: Adding Custom Layers on Top of a Hugging Face Model</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc256c3",
   "metadata": {
    "papermill": {
     "duration": 0.034792,
     "end_time": "2024-01-18T17:12:05.157618",
     "exception": false,
     "start_time": "2024-01-18T17:12:05.122826",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color: #7b6b59;\">Step 1: Tokenization with Hugging Face ü§ó Transformers</span>\n",
    "\n",
    "\n",
    "A tokenizer is in charge of preparing the inputs for a model. The Hugging Face library contains tokenizers for all the models. \n",
    "\n",
    "Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors. ü§ó Transformers provides a set of preprocessing classes to help prepare your data for the model. Text, use a **Tokenizer** to convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors. The main tool for preprocessing textual data is a [tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer). \n",
    "\n",
    "1. A tokenizer splits text into tokens according to a set of rules. \n",
    "1. The tokens are converted into numbers and then tensors, which become the model inputs. \n",
    "1. Any additional inputs required by the model are added by the tokenizer.\n",
    "\n",
    "***Tip:*** If you plan on using a pretrained model, it‚Äôs important to use the associated pretrained tokenizer. This ensures the text is split the same way as the pretraining corpus, and uses the same corresponding tokens-to-index (usually referred to as the vocab) during pretraining.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Step 1.1: Loading a pretrained tokenizer</span>\n",
    "\n",
    "\n",
    "Get started by loading a pretrained tokenizer with the AutoTokenizer.from_pretrained() method. This downloads the vocab a model was pretrained with:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ceebe62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:05.226397Z",
     "iopub.status.busy": "2024-01-18T17:12:05.225994Z",
     "iopub.status.idle": "2024-01-18T17:12:06.977389Z",
     "shell.execute_reply": "2024-01-18T17:12:06.976242Z"
    },
    "papermill": {
     "duration": 1.788525,
     "end_time": "2024-01-18T17:12:06.979861",
     "exception": false,
     "start_time": "2024-01-18T17:12:05.191336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d7b022b4ff42a0b42b38108840e825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d1af7c0abc4e3286d64d0aa2cce210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e2ca1d844343cfa4b8f78ab7ff052c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./tokenizer/tokenizer_config.json',\n",
       " './tokenizer/special_tokens_map.json',\n",
       " './tokenizer/spm.model',\n",
       " './tokenizer/added_tokens.json',\n",
       " './tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "OUTPUT_DIR = \"./\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n",
    "tokenizer.save_pretrained(OUTPUT_DIR + \"tokenizer/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfef52b2",
   "metadata": {
    "papermill": {
     "duration": 0.032451,
     "end_time": "2024-01-18T17:12:07.046700",
     "exception": false,
     "start_time": "2024-01-18T17:12:07.014249",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color: #7b6b59;\">Step 1.2: Then pass your text to the tokenizer</span>\n",
    "\n",
    "\n",
    "The tokenizer returns a dictionary with three important items:\n",
    "\n",
    "1. **input_ids** are the indices corresponding to each token in the sentence.\n",
    "1. **attention_mask** indicates whether a token should be attended to or not.\n",
    "1. **token_type_ids** identifies which sequence a token belongs to when there is more than one sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5920e51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:07.113538Z",
     "iopub.status.busy": "2024-01-18T17:12:07.112596Z",
     "iopub.status.idle": "2024-01-18T17:12:07.118331Z",
     "shell.execute_reply": "2024-01-18T17:12:07.117403Z"
    },
    "papermill": {
     "duration": 0.0419,
     "end_time": "2024-01-18T17:12:07.120699",
     "exception": false,
     "start_time": "2024-01-18T17:12:07.078799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 771, 298, 57249, 267, 262, 6303, 265, 41267, 261, 270, 306, 281, 6245, 263, 1538, 264, 5693, 260, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
    "print(encoded_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57430414",
   "metadata": {
    "papermill": {
     "duration": 0.034207,
     "end_time": "2024-01-18T17:12:07.188911",
     "exception": false,
     "start_time": "2024-01-18T17:12:07.154704",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Return your input by decoding the input_ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d52bf306",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:07.261174Z",
     "iopub.status.busy": "2024-01-18T17:12:07.260807Z",
     "iopub.status.idle": "2024-01-18T17:12:07.268346Z",
     "shell.execute_reply": "2024-01-18T17:12:07.267388Z"
    },
    "papermill": {
     "duration": 0.047652,
     "end_time": "2024-01-18T17:12:07.270456",
     "exception": false,
     "start_time": "2024-01-18T17:12:07.222804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Do not meddle in the affairs of wizards, for they are subtle and quick to anger.[SEP]'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4b943d",
   "metadata": {
    "papermill": {
     "duration": 0.033878,
     "end_time": "2024-01-18T17:12:07.338746",
     "exception": false,
     "start_time": "2024-01-18T17:12:07.304868",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As you can see, the tokenizer added two special tokens - `CLS` and `SEP` (classifier and separator) - to the sentence. Not all models need special tokens, but if they do, the tokenizer automatically adds them for you.\n",
    "\n",
    "If there are several sentences you want to preprocess, pass them as a list to the tokenizer:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf34b1e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:07.413067Z",
     "iopub.status.busy": "2024-01-18T17:12:07.412144Z",
     "iopub.status.idle": "2024-01-18T17:12:07.418980Z",
     "shell.execute_reply": "2024-01-18T17:12:07.417965Z"
    },
    "papermill": {
     "duration": 0.050063,
     "end_time": "2024-01-18T17:12:07.421106",
     "exception": false,
     "start_time": "2024-01-18T17:12:07.371043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[1, 420, 339, 314, 567, 2962, 302, 2], [1, 1310, 280, 297, 428, 313, 2212, 314, 567, 2962, 261, 31663, 260, 2], [1, 458, 314, 11583, 268, 3933, 302, 2]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "batch_sentences = [\n",
    "    \"But what about second breakfast?\",\n",
    "    \"Don't think he knows about second breakfast, Pip.\",\n",
    "    \"What about elevensies?\",\n",
    "]\n",
    "encoded_inputs = tokenizer(batch_sentences)\n",
    "print(encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde4878",
   "metadata": {
    "papermill": {
     "duration": 0.034011,
     "end_time": "2024-01-18T17:12:07.490357",
     "exception": false,
     "start_time": "2024-01-18T17:12:07.456346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- **Pad:** Sentences aren‚Äôt always the same length which can be an issue because tensors, the model inputs, need to have a uniform shape. Padding is a strategy for ensuring tensors are rectangular by adding a special padding token to shorter sentences.\n",
    "    - Set the `padding` parameter to `True` to pad the shorter sequences in the batch to match the longest sequence.\n",
    "- **Truncation:** On the other end of the spectrum, sometimes a sequence may be too long for a model to handle. In this case, you‚Äôll need to truncate the sequence to a shorter length.\n",
    "    - Set the `truncation` parameter to `True` to truncate a sequence to the maximum length accepted by the model.\n",
    "- **Build tensors:** Finally, you want the tokenizer to return the actual tensors that get fed to the model.\n",
    "    - Set the `return_tensors` parameter to either `pt` for PyTorch, or `tf` for TensorFlow:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1649a75a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:07.560491Z",
     "iopub.status.busy": "2024-01-18T17:12:07.559359Z",
     "iopub.status.idle": "2024-01-18T17:12:07.569564Z",
     "shell.execute_reply": "2024-01-18T17:12:07.568319Z"
    },
    "papermill": {
     "duration": 0.047717,
     "end_time": "2024-01-18T17:12:07.571786",
     "exception": false,
     "start_time": "2024-01-18T17:12:07.524069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,   420,   339,   314,   567,  2962,   302,     2,     0,     0,\n",
      "             0,     0,     0,     0],\n",
      "        [    1,  1310,   280,   297,   428,   313,  2212,   314,   567,  2962,\n",
      "           261, 31663,   260,     2],\n",
      "        [    1,   458,   314, 11583,   268,  3933,   302,     2,     0,     0,\n",
      "             0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "44045920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:07.644885Z",
     "iopub.status.busy": "2024-01-18T17:12:07.644490Z",
     "iopub.status.idle": "2024-01-18T17:12:07.651029Z",
     "shell.execute_reply": "2024-01-18T17:12:07.650050Z"
    },
    "papermill": {
     "duration": 0.045807,
     "end_time": "2024-01-18T17:12:07.653391",
     "exception": false,
     "start_time": "2024-01-18T17:12:07.607584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\"\n",
    "inputs = tokenizer.encode_plus(\n",
    "    text, \n",
    "    return_tensors=\"pt\", \n",
    "    add_special_tokens=True, \n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1297686",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:07.726612Z",
     "iopub.status.busy": "2024-01-18T17:12:07.726181Z",
     "iopub.status.idle": "2024-01-18T17:12:07.734647Z",
     "shell.execute_reply": "2024-01-18T17:12:07.733436Z"
    },
    "papermill": {
     "duration": 0.047958,
     "end_time": "2024-01-18T17:12:07.737105",
     "exception": false,
     "start_time": "2024-01-18T17:12:07.689147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 771, 298, 57249, 267, 262, 6303, 265, 41267, 261, 270, 306, 281, 6245, 263, 1538, 264, 5693, 260, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    \"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\", return_tensors=None, \n",
    "    add_special_tokens=True, \n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d1c112",
   "metadata": {
    "papermill": {
     "duration": 0.034664,
     "end_time": "2024-01-18T17:12:07.807654",
     "exception": false,
     "start_time": "2024-01-18T17:12:07.772990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I typically use the `tokenizer.encode_plus()` function to tokenize my input, but there is another function that can be used to tokenize input, and this `tokenizer.encode()`. The main difference between `tokenizer.encode_plus()` and `tokenizer.encode()` is that `tokenizer.encode_plus()` returns more information. Specifically, it returns the actual input ids, the attention masks, and the token type ids, and it returns all of these in a dictionary. `tokenizer.encode()` **only returns the input ids**, and it returns this either as a list or a tensor depending on the parameter, `return_tensors = ‚Äúpt‚Äù`.\n",
    "\n",
    "In Hugging Face's Transformers library, the difference between `tokenizer(input)` and `tokenizer.encode_plus(...)` lies in their functionality and the level of control they offer. In summary, `tokenizer(input)` is a simpler method for basic tokenization, while `tokenizer.encode_plus(...)` provides more options and is suitable for scenarios where you need to customize the tokenization process to fit specific model requirements. When you call `tokenizer(inputs)` in Hugging Face's Transformers library, it essentially acts as a high-level wrapper that internally calls methods like `encode_plus` or similar functionalities, depending on the specific tokenizer implementation. The `encode_plus` method is one of the comprehensive methods for encoding text, handling various tasks like tokenization, conversion to token IDs, adding special tokens, creating attention masks, and managing sequence length (truncation and padding).\n",
    "\n",
    "So, in the background, when you use `tokenizer(inputs)`, it's likely invoking `encode_plus `or a functionally equivalent method, carrying out a series of steps to prepare the input text for processing by the model. The exact methods called can vary between different tokenizer classes, but they generally perform similar tasks to prepare and format the input data appropriately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "137dae4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:07.876485Z",
     "iopub.status.busy": "2024-01-18T17:12:07.876103Z",
     "iopub.status.idle": "2024-01-18T17:12:07.881389Z",
     "shell.execute_reply": "2024-01-18T17:12:07.880290Z"
    },
    "papermill": {
     "duration": 0.042876,
     "end_time": "2024-01-18T17:12:07.883689",
     "exception": false,
     "start_time": "2024-01-18T17:12:07.840813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_text = train_df['text'][:16].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5fa7dbc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:07.956321Z",
     "iopub.status.busy": "2024-01-18T17:12:07.955621Z",
     "iopub.status.idle": "2024-01-18T17:12:07.987019Z",
     "shell.execute_reply": "2024-01-18T17:12:07.985896Z"
    },
    "papermill": {
     "duration": 0.070576,
     "end_time": "2024-01-18T17:12:07.989441",
     "exception": false,
     "start_time": "2024-01-18T17:12:07.918865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = tokenizer.batch_encode_plus(\n",
    "    train_text, \n",
    "    return_tensors=\"pt\", \n",
    "    add_special_tokens=True, \n",
    "    padding=\"max_length\",\n",
    "    max_length=512,\n",
    "    truncation=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c00f921",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:08.058231Z",
     "iopub.status.busy": "2024-01-18T17:12:08.057350Z",
     "iopub.status.idle": "2024-01-18T17:12:08.063622Z",
     "shell.execute_reply": "2024-01-18T17:12:08.062720Z"
    },
    "papermill": {
     "duration": 0.043,
     "end_time": "2024-01-18T17:12:08.065738",
     "exception": false,
     "start_time": "2024-01-18T17:12:08.022738",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f34802db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:08.141702Z",
     "iopub.status.busy": "2024-01-18T17:12:08.141263Z",
     "iopub.status.idle": "2024-01-18T17:12:08.153820Z",
     "shell.execute_reply": "2024-01-18T17:12:08.152719Z"
    },
    "papermill": {
     "duration": 0.056155,
     "end_time": "2024-01-18T17:12:08.156089",
     "exception": false,
     "start_time": "2024-01-18T17:12:08.099934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>source</th>\n",
       "      <th>RDizzl3_seven</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phones\\n\\nModern humans today are always on th...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This essay will explain if drivers should or s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Driving while the use of cellular devices\\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phones &amp; Driving\\n\\nDrivers should not be able...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cell Phone Operation While Driving\\n\\nThe abil...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Phones\\n\\nModern humans today are always on th...      0   \n",
       "1  This essay will explain if drivers should or s...      0   \n",
       "2  Driving while the use of cellular devices\\n\\nT...      0   \n",
       "3  Phones & Driving\\n\\nDrivers should not be able...      0   \n",
       "4  Cell Phone Operation While Driving\\n\\nThe abil...      0   \n",
       "\n",
       "          prompt_name           source  RDizzl3_seven  \n",
       "0  Phones and driving  persuade_corpus          False  \n",
       "1  Phones and driving  persuade_corpus          False  \n",
       "2  Phones and driving  persuade_corpus          False  \n",
       "3  Phones and driving  persuade_corpus          False  \n",
       "4  Phones and driving  persuade_corpus          False  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d7800165",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:08.225749Z",
     "iopub.status.busy": "2024-01-18T17:12:08.225370Z",
     "iopub.status.idle": "2024-01-18T17:12:08.232052Z",
     "shell.execute_reply": "2024-01-18T17:12:08.231012Z"
    },
    "papermill": {
     "duration": 0.044285,
     "end_time": "2024-01-18T17:12:08.234296",
     "exception": false,
     "start_time": "2024-01-18T17:12:08.190011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[\"input_ids\"].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ec0eae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:08.318624Z",
     "iopub.status.busy": "2024-01-18T17:12:08.317707Z",
     "iopub.status.idle": "2024-01-18T17:12:08.323773Z",
     "shell.execute_reply": "2024-01-18T17:12:08.322953Z"
    },
    "papermill": {
     "duration": 0.056978,
     "end_time": "2024-01-18T17:12:08.326065",
     "exception": false,
     "start_time": "2024-01-18T17:12:08.269087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_input(text):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        add_special_tokens=True, \n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        truncation=True\n",
    "    )\n",
    "  \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4a1688",
   "metadata": {
    "papermill": {
     "duration": 0.032497,
     "end_time": "2024-01-18T17:12:08.396695",
     "exception": false,
     "start_time": "2024-01-18T17:12:08.364198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color: #7b6b59;\">Step 2: Prepare the Training Data</span>\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Cross-Validation Split</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b0988789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:08.467247Z",
     "iopub.status.busy": "2024-01-18T17:12:08.466262Z",
     "iopub.status.idle": "2024-01-18T17:12:08.478393Z",
     "shell.execute_reply": "2024-01-18T17:12:08.477306Z"
    },
    "papermill": {
     "duration": 0.049765,
     "end_time": "2024-01-18T17:12:08.480880",
     "exception": false,
     "start_time": "2024-01-18T17:12:08.431115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>source</th>\n",
       "      <th>RDizzl3_seven</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phones\\n\\nModern humans today are always on th...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This essay will explain if drivers should or s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Driving while the use of cellular devices\\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phones &amp; Driving\\n\\nDrivers should not be able...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cell Phone Operation While Driving\\n\\nThe abil...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Phones\\n\\nModern humans today are always on th...      0   \n",
       "1  This essay will explain if drivers should or s...      0   \n",
       "2  Driving while the use of cellular devices\\n\\nT...      0   \n",
       "3  Phones & Driving\\n\\nDrivers should not be able...      0   \n",
       "4  Cell Phone Operation While Driving\\n\\nThe abil...      0   \n",
       "\n",
       "          prompt_name           source  RDizzl3_seven  \n",
       "0  Phones and driving  persuade_corpus          False  \n",
       "1  Phones and driving  persuade_corpus          False  \n",
       "2  Phones and driving  persuade_corpus          False  \n",
       "3  Phones and driving  persuade_corpus          False  \n",
       "4  Phones and driving  persuade_corpus          False  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "96236919",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:08.552157Z",
     "iopub.status.busy": "2024-01-18T17:12:08.551742Z",
     "iopub.status.idle": "2024-01-18T17:12:08.591538Z",
     "shell.execute_reply": "2024-01-18T17:12:08.590510Z"
    },
    "papermill": {
     "duration": 0.07783,
     "end_time": "2024-01-18T17:12:08.593732",
     "exception": false,
     "start_time": "2024-01-18T17:12:08.515902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    10038\n",
       "1    10038\n",
       "2    10038\n",
       "3    10037\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_folds = 4\n",
    "train_folds = [0, 1, 2, 3]\n",
    "\n",
    "stratified_k_fold = StratifiedKFold(\n",
    "    n_splits=n_folds,\n",
    "    shuffle=True, \n",
    "    random_state=123\n",
    ")\n",
    "\n",
    "# Iterating Over Each Fold\n",
    "# The enumerate function is used to iterate over the fold splits. \n",
    "# It provides two pieces of information for each iteration:\n",
    "# fold: The current fold number (starting from 0).\n",
    "# (train_index, val_index): Two arrays containing indices of the training and validation data for the current fold\n",
    "for fold, (train_index, val_index) in enumerate(stratified_k_fold.split(train_df, train_df[\"label\"])): # It generates indices for training and validation sets for each fold.\n",
    "    \n",
    "    # Inside the loop, for each fold, the validation indices (val_index) are used \n",
    "    # to assign the fold number to the corresponding rows in train_df.\n",
    "    # This line does the assignment. It sets the 'fold' column of the DataFrame for rows in val_index to the current fold number.\n",
    "    # This effectively tags each data point with the fold number it will be a part of in the validation set.\n",
    "    # The purpose of this assignment is to keep track of which data points should be in the validation set for each fold.\n",
    "    # When you actually train the model, you can easily filter the DataFrame to get the appropriate training and validation sets based on these fold numbers.\n",
    "\n",
    "    train_df.loc[val_index, 'fold'] = int(fold)\n",
    "\n",
    "train_df['fold'] = train_df['fold'].astype(int)\n",
    "display(train_df.groupby('fold').size())                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c10de75e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:08.667018Z",
     "iopub.status.busy": "2024-01-18T17:12:08.666566Z",
     "iopub.status.idle": "2024-01-18T17:12:08.680448Z",
     "shell.execute_reply": "2024-01-18T17:12:08.679320Z"
    },
    "papermill": {
     "duration": 0.053299,
     "end_time": "2024-01-18T17:12:08.682870",
     "exception": false,
     "start_time": "2024-01-18T17:12:08.629571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>source</th>\n",
       "      <th>RDizzl3_seven</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Phones\\n\\nModern humans today are always on th...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This essay will explain if drivers should or s...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Driving while the use of cellular devices\\n\\nT...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phones &amp; Driving\\n\\nDrivers should not be able...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cell Phone Operation While Driving\\n\\nThe abil...</td>\n",
       "      <td>0</td>\n",
       "      <td>Phones and driving</td>\n",
       "      <td>persuade_corpus</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  Phones\\n\\nModern humans today are always on th...      0   \n",
       "1  This essay will explain if drivers should or s...      0   \n",
       "2  Driving while the use of cellular devices\\n\\nT...      0   \n",
       "3  Phones & Driving\\n\\nDrivers should not be able...      0   \n",
       "4  Cell Phone Operation While Driving\\n\\nThe abil...      0   \n",
       "\n",
       "          prompt_name           source  RDizzl3_seven  fold  \n",
       "0  Phones and driving  persuade_corpus          False     3  \n",
       "1  Phones and driving  persuade_corpus          False     0  \n",
       "2  Phones and driving  persuade_corpus          False     0  \n",
       "3  Phones and driving  persuade_corpus          False     3  \n",
       "4  Phones and driving  persuade_corpus          False     1  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eebb37de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:08.759247Z",
     "iopub.status.busy": "2024-01-18T17:12:08.758837Z",
     "iopub.status.idle": "2024-01-18T17:12:08.798642Z",
     "shell.execute_reply": "2024-01-18T17:12:08.797461Z"
    },
    "papermill": {
     "duration": 0.081248,
     "end_time": "2024-01-18T17:12:08.801272",
     "exception": false,
     "start_time": "2024-01-18T17:12:08.720024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10038, 6)\n",
      "(10038, 6)\n",
      "(10038, 6)\n",
      "(10037, 6)\n"
     ]
    }
   ],
   "source": [
    "for fold in range(n_folds):\n",
    "    \n",
    "    train_folds = train_df[train_df[\"fold\"] != fold].reset_index(drop=True)\n",
    "    valid_folds = train_df[train_df[\"fold\"] == fold].reset_index(drop=True)\n",
    "    print(valid_folds.shape)\n",
    "    valid_labels = valid_folds[\"label\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d657687",
   "metadata": {
    "papermill": {
     "duration": 0.036987,
     "end_time": "2024-01-18T17:12:08.877856",
     "exception": false,
     "start_time": "2024-01-18T17:12:08.840869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "### <span style=\"color: #7b6b59;\">Introduction to PyTorch Dataset and DataLoader</span>\n",
    "\n",
    "Code for processing data samples can get messy and hard to maintain; we ideally want our dataset code to be decoupled from our model training code for better readability and modularity. PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. **Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples.**\n",
    "\n",
    "Your training pipeline should be as modular as possible in order to aid quick prototyping and maintaining usability. Using a poorly-written data loader / not using a data loader (using a Python generator or a function) can affect the parallelization ability of your code. \n",
    "\n",
    "Dataset processing is a highly important part of any training pipeline and should be kept separate from modeling. \n",
    "\n",
    "***How to use `Datasets` and `DataLoader` in PyTorch for custom text data***\n",
    "\n",
    "In this section, we'll go through the PyTorch data primitives, namely `torch.utils.data.DataLoader` and `torch.utils.data.Dataset`, and understand how to create our own DataLoader and Datasets by subclassing these modules. \n",
    "\n",
    "We will learn how to make a custom Dataset and manage it with DataLoader in PyTorch. Creating a PyTorch `Dataset` and managing it with `Dataloader` keeps your data manageable and helps to simplify your machine learning pipeline. **A Dataset stores all your data, and Dataloader is can be used to iterate through the data, manage batches, transform the data, and much more.**\n",
    "\n",
    "\n",
    "- **Pandas** is not essential to create a Dataset object. However, it‚Äôs a powerful tool for managing data so i‚Äôm going to use it.\n",
    "\n",
    "- **`torch.utils.data`** imports the required functions we need to create and use Dataset and DataLoader.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d5a4e6a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:08.952543Z",
     "iopub.status.busy": "2024-01-18T17:12:08.952180Z",
     "iopub.status.idle": "2024-01-18T17:12:08.956964Z",
     "shell.execute_reply": "2024-01-18T17:12:08.955878Z"
    },
    "papermill": {
     "duration": 0.045224,
     "end_time": "2024-01-18T17:12:08.959247",
     "exception": false,
     "start_time": "2024-01-18T17:12:08.914023",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a4bea",
   "metadata": {
    "papermill": {
     "duration": 0.034212,
     "end_time": "2024-01-18T17:12:09.028096",
     "exception": false,
     "start_time": "2024-01-18T17:12:08.993884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color: #7b6b59;\">Implementing A Custom Dataset In PyTorch</span>\n",
    "\n",
    "A dataset is an abstract class in PyTorch that represents a collection of data. It is responsible for loading and preprocessing data from a source and returning it in the form of a PyTorch tensor.\n",
    "\n",
    "\n",
    "Now, for most purposes, you will need to write your own implementation of a `Dataset`. So let's see how you can write a custom dataset by subclassing `torch.utils.data.Dataset`.\n",
    "\n",
    "You'll need to implement 3 functions. The Dataset class provides 3 main methods:\n",
    "\n",
    "1. **`__init__`**: This function is called when instancing the object. It's typically used to store some essential locations like file paths and image transforms. `class TextDataset(Dataset)`: Create a class called ‚ÄòTextDataset‚Äô, this can be called whatever you want. Passed in to the class is the dataset module which we imported earlier. `def __init__(self, text, labels)`: When you initialise the class you need to import two variables. In this case, the variables are called ‚Äòtext‚Äô and ‚Äòlabels‚Äô to match the data which will be added.\n",
    "\n",
    "1. **`__len__`**: This function returns the length of the dataset. `self.labels = labels` & `self.text = text`: The imported variables can now be used in functions within the class by using self.text or self.labels. `def __len__(self)`: This function just returns the length of the labels when called. E.g., if you had a dataset with 5 labels, then the integer 5 would be returned.\n",
    "\n",
    "1. **`__getitem__`**: This is the big kahuna üèÖ. This function is responsible for returning a sample from the dataset based on the index provided. returns a single data point from the dataset at a given index. The getitem method is where the actual data loading and preprocessing takes place. It takes an index as input and returns a data point, which can be a tensor or a dictionary of tensors. This method is used by the DataLoader class to load and preprocess the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6770521c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:09.100548Z",
     "iopub.status.busy": "2024-01-18T17:12:09.100150Z",
     "iopub.status.idle": "2024-01-18T17:12:09.107276Z",
     "shell.execute_reply": "2024-01-18T17:12:09.106237Z"
    },
    "papermill": {
     "duration": 0.04592,
     "end_time": "2024-01-18T17:12:09.109467",
     "exception": false,
     "start_time": "2024-01-18T17:12:09.063547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df[\"text\"].values\n",
    "        self.labels = df[\"label\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = prepare_input(self.texts[idx])\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return inputs, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e9474612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:09.182980Z",
     "iopub.status.busy": "2024-01-18T17:12:09.182489Z",
     "iopub.status.idle": "2024-01-18T17:12:09.193292Z",
     "shell.execute_reply": "2024-01-18T17:12:09.192255Z"
    },
    "papermill": {
     "duration": 0.049617,
     "end_time": "2024-01-18T17:12:09.195688",
     "exception": false,
     "start_time": "2024-01-18T17:12:09.146071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = CustomTextDataset(train_df[:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b55a84",
   "metadata": {
    "papermill": {
     "duration": 0.036643,
     "end_time": "2024-01-18T17:12:09.267981",
     "exception": false,
     "start_time": "2024-01-18T17:12:09.231338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### <span style=\"color: #7b6b59;\">PyTorch DataLoader: A Complete Guide</span>\n",
    "\n",
    "PyTorch provides an intuitive and incredibly versatile tool, the DataLoader class, to load data in meaningful ways. Because data preparation is a critical step to any type of data work, being able to work with, and understand, DataLoaders is an important step in your deep learning journey. The Dataset retrieves our dataset‚Äôs features and labels one sample at a time. While training a model, we typically want to pass samples in ‚Äúminibatches‚Äù, reshuffle the data at every epoch to reduce model overfitting, and use Python‚Äôs multiprocessing to speed up data retrieval.\n",
    "\n",
    "**DataLoader is an iterable that abstracts this complexity for us in an easy API.**\n",
    "\n",
    "The PyTorch DataLoader class is built on top of the PyTorch Dataset class, which provides a standard interface for accessing data. The DataLoader class takes in a Dataset object and provides a way to iterate over the data in batches. This allows for efficient processing of large datasets by allowing parallelization of data loading and preprocessing.\n",
    "\n",
    "\n",
    "\n",
    "**What Does a PyTorch DataLoader Do?**\n",
    "\n",
    "The PyTorch DataLoader class is an important tool to help you prepare, manage, and serve your data to your deep learning networks. Because many of the pre-processing steps you will need to do before beginning training a model, finding ways to standardize these processes is critical for the readability and maintainability of your code.\n",
    "\n",
    "The PyTorch DataLoader allows you to:\n",
    "\n",
    "- **Define a dataset to work with:** identifying where the data is coming from and how it should be accessed.\n",
    "- **Batch the data:** define how many training or testing samples to use in a single iteration. Because data are often split across training and testing sets of large sizes, being able to work with batches of data can allow your training and testing processes to be more manageable.\n",
    "- **Shuffle the data:** PyTorch can handle shuffling data for you as it loads data into batches. This can increase representativeness in your dataset and prevent accidental skewness.\n",
    "- **Support multi-processing:** PyTorch is optimized to run multiple processes at once in order to make better use of modern CPUs and GPUs and to save time in training and testing your data. The DataLoader class lets you define how many workers should go at once.\n",
    "- **Merge datasets together:** optionally, PyTorch also allows you to merge multiple datasets together. While this may not be a common task, having it available to you is an a great feature.\n",
    "- **Load data directly on CUDA tensors:** because PyTorch can run on the GPU, you can load the data directly onto the CUDA before they‚Äôre returned.\n",
    "\n",
    "The DataLoader is a PyTorch utility class that provides a way to iterate over a Dataset object in batches. It is designed to handle large datasets efficiently and can be configured to load data in parallel, preprocess data on the fly, and shuffle data for each epoch.\n",
    "\n",
    "The DataLoader takes in a Dataset object and provides a number of configuration options, including batch size, shuffling, and number of worker processes for parallel data loading. The DataLoader class is responsible for batching the data and returning it in a format that can be consumed by the model\n",
    "\n",
    "\n",
    "`DataLoader` class has a lot of different parameters available. Of course, one of the most important parameters is the actual dataset. Generally, you‚Äôll be working with at least a training and a testing dataset. **Because of this, it‚Äôs a convention that you‚Äôll have at least two DataLoaders, to be able to load data for both your training and testing data.**\n",
    "\n",
    "PyTorch lets you define many different parameters to influence how data are loaded. This can have a big impact on the speed at which your model can train, how well it can train, and ensuring that data are sampled appropriately.\n",
    "\n",
    "We have loaded that dataset into the DataLoader and can iterate through the dataset as needed. Each iteration below returns a batch of train_features and train_labels (containing `batch_size=8` features and labels respectively). Because we specified `shuffle=True`, after we iterate over all batches the data is shuffled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9eea387a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:09.343315Z",
     "iopub.status.busy": "2024-01-18T17:12:09.342896Z",
     "iopub.status.idle": "2024-01-18T17:12:09.348580Z",
     "shell.execute_reply": "2024-01-18T17:12:09.347579Z"
    },
    "papermill": {
     "duration": 0.046186,
     "end_time": "2024-01-18T17:12:09.350688",
     "exception": false,
     "start_time": "2024-01-18T17:12:09.304502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, # expects a PyTorch Dataset from which to load the data\n",
    "    batch_size=8, # represents how many samples per batch to load\n",
    "    shuffle=True, # indicates whether data should be shuffled at every epoch you run\n",
    "    num_workers=4, # represents how many subprocesses to use for loading data.\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "87df59b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:09.425087Z",
     "iopub.status.busy": "2024-01-18T17:12:09.424711Z",
     "iopub.status.idle": "2024-01-18T17:12:09.793047Z",
     "shell.execute_reply": "2024-01-18T17:12:09.791624Z"
    },
    "papermill": {
     "duration": 0.409087,
     "end_time": "2024-01-18T17:12:09.795514",
     "exception": false,
     "start_time": "2024-01-18T17:12:09.386427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([8])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "torch.Size([8, 1, 512])\n",
      "torch.Size([1, 512])\n",
      "1\n",
      "torch.Size([8])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "torch.Size([8, 1, 512])\n",
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "# Conventionally, you will load both the index of a batch and the items in the batch.\n",
    "# We can do this using the enumerate() function\n",
    "# DataLoader will return an object that contains both the data and the target (if the dataset contains both). \n",
    "# We can access each item and its labels by iterating over the batches.\n",
    "for step, (inputs, labels) in enumerate(train_loader):\n",
    "    print(step)\n",
    "    print(labels.size())\n",
    "    print(labels)\n",
    "    print(inputs[\"input_ids\"].size())\n",
    "    print(inputs[\"input_ids\"][0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d47759",
   "metadata": {
    "papermill": {
     "duration": 0.037627,
     "end_time": "2024-01-18T17:12:09.871255",
     "exception": false,
     "start_time": "2024-01-18T17:12:09.833628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color: #7b6b59;\">Step 3: Modelling with Hugging Face ü§ó Transformers</span>\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "\n",
    "In Hugging Face Transformers there are 2 main outputs and 3 if configured; that we receive after giving input_ids and attention_mask as input.\n",
    "\n",
    "- **pooler output (batch size, hidden size)**: Last layer hidden-state of the first token of the sequence\n",
    "- **last hidden state (batch size, seq Len, hidden size)**: which is the sequence of hidden states at the output of the last layer.\n",
    "- **hidden states (n layers, batch size, seq Len, hidden size)**: Hidden states for all layers and for all ids.\n",
    "\n",
    "In this notebook, we will show many different ways these outputs and hidden representations can be utilized to do much more than just adding an output layer. Below are the various techniques we will be implementing.\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Last Hidden State Output</span>\n",
    "\n",
    "\n",
    "<img width=\"894\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/923fb8e9-58b0-4984-a4e3-d097afde3b88\">\n",
    "\n",
    "**This is the first and default output from models.**\n",
    "\n",
    "Last Hidden State output is the sequence of hidden-states at the output of the last layer of the model. The output is usually `[batch, maxlen, hidden_state]`, it can be narrowed down to `[batch, 1, hidden_state]` for `[CLS]` token, as the `[CLS]` token is 1st token in the sequence. Here , `[batch, 1, hidden_state]` can be equivalently considered as `[batch, hidden_state]`.\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Implementation Details</span>\n",
    "\n",
    "\n",
    "All models have outputs that are instances of subclasses of [`ModelOutput`](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/output#transformers.utils.ModelOutput). Those are data structures containing all the information returned by the model, but that can also be used as tuples or dictionaries.\n",
    "\n",
    "```python\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "\n",
    "```\n",
    "When considering our outputs object as tuple, it only considers the attributes that don‚Äôt have None values. For instance, it has two elements, loss then logits, so will return the `tuple (outputs.loss, outputs.logits)`. `outputs[:2]`\n",
    "\n",
    "\n",
    "In Hugging Face Transformers, when you use a model from the `AutoModel` class with `AutoModel.from_pretrained`, the specific subclass of `ModelOutput` that the model returns depends on the type of model you are using (e.g., BERT, GPT-2, T5, etc.) and the nature of the task (e.g., sequence classification, token classification, language modeling, etc.).\n",
    "\n",
    "To determine which subclass of `ModelOutput` is returned, you should consider the following:\n",
    "\n",
    "1. **Model Type:** Different models are designed for different kinds of tasks. For instance, BERT-like models might return `BaseModelOutput` or `SequenceClassifierOutput`, while GPT-like models might return `CausalLMOutput`.\n",
    "\n",
    "1. **Task:** The nature of the task also influences the output type. For example:\n",
    "\n",
    "    - For sequence classification tasks, models often return SequenceClassifierOutput.\n",
    "    - For token classification tasks (like Named Entity Recognition), models might return TokenClassifierOutput.\n",
    "    - For language modeling tasks, models could return CausalLMOutput or MaskedLMOutput.\n",
    "\n",
    "1. **Documentation::** The best way to know for sure is to refer to the Hugging Face documentation for the specific model you are using. The documentation usually specifies the output format for each model.\n",
    "\n",
    "1. **Inspecting the Output:** You can programmatically inspect the output to determine its type. For example, after running `outputs = self.model(**inputs)`, you can check `type(outputs)` to see the class of the output.\n",
    "\n",
    "1. **Common Attributes:** Most ModelOutput subclasses have common attributes like `loss`, `logits`, `hidden_states`, and `attentions`, but the presence and relevance of these attributes can vary. The exact composition of the output object will align with the requirements of the model's intended task.\n",
    "\n",
    "1. **Configuration:** Sometimes, the configuration of the model (self.config) can give you hints about the expected output type, especially if it contains task-specific configurations.\n",
    "\n",
    "Remember that Hugging Face's design philosophy with `ModelOutput` is to provide flexibility and convenience, allowing outputs to be used like tuples, dictionaries, or objects with named attributes. This makes it easier to access the information you need for your specific application.\n",
    "\n",
    "For instance, if we have a look on the [documentation for the Deberta Model](https://huggingface.co/docs/transformers/model_doc/deberta#transformers.DebertaModel.forward) in the `forward` method  \n",
    "we will see that \"**Returns `transformers.modeling_outputs.BaseModelOutput` or `tuple(torch.FloatTensor)`**\". Now if we jump to the [BaseModelOutput documentation](https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/output#transformers.modeling_outputs.BaseModelOutput) we'll get \n",
    "\n",
    "<img width=\"1044\" alt=\"image\" src=\"https://github.com/microsoft/DeBERTa/assets/28102493/dbe927a5-4f60-4673-bf28-29a8a96e05aa\">\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Mean Pooling</span>\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "Since Transformers are contextual model, the idea is `[CLS]` token would have captured the entire context and would be sufficient for simple downstream tasks such as classification. Hence, for tasks such as classification using sentence representations, you can use `[batch, hidden_state]`.\n",
    "\n",
    "We can also consider the last hidden state `[batch, maxlen, hidden_state]`, the average across maxlen dimensions to get averaged/mean embeddings.\n",
    "\n",
    "There are multiple different ways to do this. We can simply take `torch.mean(last_hidden_state, 1)` but rather we will be implementing something different. We will make use of attention masks as well so that we can ignore padding tokens which is a better way of implementing average embeddings.\n",
    "\n",
    "***What is pooling in Transformer models?***\n",
    "\n",
    "In the context of transformers, pooling refers to the process of summarizing the outputs of the transformer layers into a fixed-size vector, often used for downstream tasks such as classification.\n",
    "\n",
    "In a transformer architecture, the input sequence is processed by a series of self-attention and feedforward layers. Each layer produces a sequence of output vectors, which encode the input sequence in a higher-level representation. Pooling involves taking the output vectors from one or more of these layers and aggregating them into a single vector.\n",
    "\n",
    "There are different types of pooling mechanisms used in transformer architectures, including:\n",
    "\n",
    "\n",
    "1. **Max Pooling:** where the maximum value across the sequence of output vectors is selected as the summary representation.\n",
    "\n",
    "1. **Mean Pooling:** where the average of the output vectors is taken as the summary representation.\n",
    "\n",
    "1. **Last Hidden State:** where the final output vector of the transformer is used as the summary representation.\n",
    "\n",
    "1. **Self-Attention Pooling:** where a weighted sum of the output vectors is computed, with the weights determined by a learned attention mechanism.\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Neural Networks: Pooling Layers</span>\n",
    "\n",
    "In this section, we‚Äôll walk through **pooling**, a machine-learning technique ***widely used that reduces the size of the input and, thus the complexity of deep learning models while preserving important features and relationships in the input data***. In particular, we‚Äôll introduce pooling, explain its usage, highlight its importance, and give brief examples of how it works.\n",
    "\n",
    "***What Are Pooling Layers?***\n",
    "\n",
    "In machine learning and neural networks, the dimensions of the input data and the parameters of the neural network play a crucial role. So this number can be controlled by the stacking of one or more pooling layers. Depending on the type of the pooling layer, an operation is performed on each channel of the input data independently to summarize its values into a single one and thus keep the most important features. These values are driven as input to the next layer of the model and so on. The pooling process may be repeated several times, and each iteration reduces the spatial dimensions. The value aggregation can be performed by using different techniques.\n",
    "\n",
    "***Types of Pooling Layers***\n",
    "\n",
    "There are many pooling operations and different extensions that have been developed to address specific challenges in different applications.\n",
    "\n",
    "\n",
    "1. **Max Pooling:** Max pooling is a convolution technique that chooses the maximum value from the patch of the input data and summarizes these values into a feature map: This method maintains the most significant features of the input by reducing its dimensions.\n",
    "    <img width=\"660\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/4e844e1d-8d59-4f97-beb8-00845cc7e45a\">\n",
    "    \n",
    "1. **Average Pooling:** Average pooling calculates the average value from a patch of input data and summarizes these values into a feature map: This method is preferable in cases in which smoothing the input data is necessary as it helps to identify the presence of outliers. \n",
    "     <img width=\"625\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/af4b21e0-10f8-4518-9a5c-dbdf960f48fa\">\n",
    "\n",
    "1. **Global Pooling:** Global pooling summarizes the values of all neurons for each patch of the input data into a feature map, regardless of their spatial location. This technique is also used to reduce the dimensionality of the input and can be performed either by using the maximum or average pooling operation. \n",
    "\n",
    "1. **Stochastic Pooling:** Stochastic pooling is a deterministic pooling operation that introduces randomness into the max pooling process. This technique helps in improving the robustness of the model to small variations in the input data.\n",
    "\n",
    "***Advantages and Disadvantages***\n",
    "\n",
    "In machine learning, pooling layers offer several advantages and disadvantages as well.\n",
    "\n",
    "First of all, pooling layers help in keeping the most important characteristics of the input data. Furthermore, the addition of pooling layers in the neural network offers translation invariance, which means that the model can generate the same outputs regardless of small changes in the input. Moreover, these techniques help in reducing the impact of outliers.\n",
    "\n",
    "On the other hand, the pooling processes may lead to information loss, increased training complexity, and limited model interpretability.\n",
    "\n",
    "***Usages of Pooling Layers in Machine Learning***\n",
    "\n",
    "Pooling layers play a critical role in the size and complexity of the model and are widely used in several machine-learning tasks. They are usually employed after the convolutional layers in the convolutional neural network‚Äôs structure and are mainly used for downsampling the output.\n",
    "\n",
    "These techniques are commonly used in convolutional neural networks and deep learning models of computer vision, speech recognition, and natural language processing.\n",
    "\n",
    "\n",
    "***In conclusion, pooling layers play a critical role in reducing the size and complexity of deep learning models while preserving important features and relationships in the input data.***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "610a75ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:09.952095Z",
     "iopub.status.busy": "2024-01-18T17:12:09.950684Z",
     "iopub.status.idle": "2024-01-18T17:12:17.671573Z",
     "shell.execute_reply": "2024-01-18T17:12:17.670602Z"
    },
    "papermill": {
     "duration": 7.765224,
     "end_time": "2024-01-18T17:12:17.674169",
     "exception": false,
     "start_time": "2024-01-18T17:12:09.908945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40aef2af7449492694ba59d8d92c5de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"microsoft/deberta-v3-base\", output_hidden_states=True)\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-base\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "247c2b07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:17.755671Z",
     "iopub.status.busy": "2024-01-18T17:12:17.755281Z",
     "iopub.status.idle": "2024-01-18T17:12:45.663734Z",
     "shell.execute_reply": "2024-01-18T17:12:45.662527Z"
    },
    "papermill": {
     "duration": 27.950897,
     "end_time": "2024-01-18T17:12:45.666574",
     "exception": false,
     "start_time": "2024-01-18T17:12:17.715677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(features['input_ids'], features['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f0b6f14c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:45.754423Z",
     "iopub.status.busy": "2024-01-18T17:12:45.752329Z",
     "iopub.status.idle": "2024-01-18T17:12:45.760246Z",
     "shell.execute_reply": "2024-01-18T17:12:45.758522Z"
    },
    "papermill": {
     "duration": 0.055207,
     "end_time": "2024-01-18T17:12:45.762497",
     "exception": false,
     "start_time": "2024-01-18T17:12:45.707290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_hidden_state = outputs[0]\n",
    "attention_mask = features['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8d4bbe14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:45.843740Z",
     "iopub.status.busy": "2024-01-18T17:12:45.843303Z",
     "iopub.status.idle": "2024-01-18T17:12:45.850657Z",
     "shell.execute_reply": "2024-01-18T17:12:45.849441Z"
    },
    "papermill": {
     "duration": 0.050133,
     "end_time": "2024-01-18T17:12:45.852993",
     "exception": false,
     "start_time": "2024-01-18T17:12:45.802860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512, 768])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c162e15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:45.933185Z",
     "iopub.status.busy": "2024-01-18T17:12:45.931955Z",
     "iopub.status.idle": "2024-01-18T17:12:45.940122Z",
     "shell.execute_reply": "2024-01-18T17:12:45.939020Z"
    },
    "papermill": {
     "duration": 0.050808,
     "end_time": "2024-01-18T17:12:45.942497",
     "exception": false,
     "start_time": "2024-01-18T17:12:45.891689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 512])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2785ca",
   "metadata": {
    "papermill": {
     "duration": 0.039634,
     "end_time": "2024-01-18T17:12:46.021499",
     "exception": false,
     "start_time": "2024-01-18T17:12:45.981865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## <span style=\"color: #7b6b59;\">Step 4: Train the Model - Training Loop</span>\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Automatic Mixed Precision (AMP) for Deep Learning</span>\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "Deep Neural Network training has traditionally relied on IEEE single-precision format, however with mixed precision, you can train with half precision while maintaining the network accuracy achieved with single precision. This technique of using both single- and half-precision representations is referred to as **mixed precision technique**. Mixed precision methods combine the use of different numerical formats in one computational workload. This document describes the application of mixed precision to deep neural network training.\n",
    "\n",
    "- **IEEE single-precision floating point computer numbering format**, is a binary computing format that occupies **4 bytes** (32 bits) in computer memory\n",
    "- In computing, **half precision** is a binary floating-point computer number format that occupies 16 bits in computer memory.\n",
    "\n",
    "There are numerous benefits to using numerical formats with lower precision than 32-bit floating point. First, they require less memory, enabling the training and deployment of larger neural networks. Second, they require less memory bandwidth which speeds up data transfer operations. Third, math operations run much faster in reduced precision, especially on GPUs with **Tensor Core** support for that precision. Mixed precision training achieves all these benefits while ensuring that no task-specific accuracy is lost compared to full precision training. ***It does so by identifying the steps that require full precision and using 32-bit floating point for only those steps while using 16-bit floating point everywhere else.***\n",
    "\n",
    "**Benefits of Mixed precision training**\n",
    "\n",
    "- Speeds up math-intensive operations, such as linear and convolution layers, by using Tensor Cores.\n",
    "- Speeds up memory-limited operations by accessing half the bytes compared to single-precision.\n",
    "- Reduces memory requirements for training models, enabling larger models or larger minibatches.\n",
    "\n",
    "*Nuance Research advances and applies conversational AI technologies to power solutions that redefine how humans and computers interact. The rate of our advances reflects the speed at which we train and assess deep learning models. With Automatic Mixed Precision, we‚Äôve realized a 50% speedup in TensorFlow-based ASR model training without loss of accuracy via a minimal code change. We‚Äôre eager to achieve a similar impact in our other deep learning language processing applications.*, Wenxuan Teng, Senior Research Manager, Nuance Communications\n",
    "\n",
    "#### Mixed Precision Training\n",
    "\n",
    "**Mixed precision** training offers significant computational speedup by performing operations in half-precision format, while storing minimal information in single-precision to retain as much information as possible in critical parts of the network. Since the introduction of Tensor Cores in the Volta and Turing architectures, significant training speedups are experienced by switching to mixed precision -- up to 3x overall speedup on the most arithmetically intense model architectures. Using mixed precision training requires two steps (Enabling mixed precision involves two steps):\n",
    "\n",
    "1. Porting the model to use the FP16 data type where appropriate.\n",
    "1. Adding loss scaling to preserve small gradient values.\n",
    "\n",
    "The ability to train deep learning networks with lower precision was introduced in the Pascal architecture and first supported in CUDA 8 in the NVIDIA Deep Learning SDK.\n",
    "\n",
    "Deep learning researchers and engineers can easily get started enabling this feature on **Ampere**, **Volta** and **Turing** GPUs. On Ampere GPUs, automatic mixed precision uses FP16 to deliver a performance boost of 3X versus TF32, the new format which is already ~6x faster than FP32. On Volta and Turing GPUs, automatic mixed precision delivers up to 3X higher performance vs FP32 with just a few lines of code.\n",
    "\n",
    "<img width=\"1121\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/ec6bbedf-82e8-43e2-bf0f-a3d0d3b2ee72\">\n",
    "\n",
    "\n",
    "Mixed precision is the combined use of different numerical precisions in a computational method.\n",
    "\n",
    "- **Half precision (also known as FP16)** data compared to higher precision FP32 vs FP64 reduces memory usage of the neural network, allowing training and deployment of larger networks, and FP16 data transfers take less time than FP32 or FP64 transfers.\n",
    "\n",
    "- **Single precision (also known as 32-bit)** is a common floating point format (float in C-derived programming languages), and 64-bit, known as double precision (double). \n",
    "\n",
    "Deep Neural Networks (DNNs) have led to breakthroughs in a number of areas, including:\n",
    "\n",
    "    - image processing and understanding\n",
    "    - language modeling\n",
    "    - language translation\n",
    "    - speech processing\n",
    "    - game playing, and many others.\n",
    "\n",
    "DNN complexity has been increasing to achieve these results, which in turn has increased the computational resources required to train these networks. One way to lower the required resources is to use lower-precision arithmetic, which has the following benefits.\n",
    "\n",
    "1. **Decrease the required amount of memory.** Half-precision floating point format (FP16) uses 16 bits, compared to 32 bits for single precision (FP32). Lowering the required memory enables training of larger models or training with larger mini-batches.\n",
    "\n",
    "1. **Shorten the training or inference time.** Execution time can be sensitive to memory or arithmetic bandwidth. Half-precision halves the number of bytes accessed, thus reducing the time spent in memory-limited layers. NVIDIA GPUs offer up to 8x more half precision arithmetic throughput when compared to single-precision, thus speeding up math-limited layers.\n",
    "\n",
    "*Figure 1. Training curves for the bigLSTM English language model shows the benefits of the mixed-precision training techniques. The Y-axis is training loss. Mixed precision without loss scaling (grey) diverges after a while, whereas mixed precision with loss scaling (green) matches the single precision model (black).*\n",
    "\n",
    "<img width=\"891\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/4ccef941-d4fb-4c3b-8f72-0e602c729743\">\n",
    "\n",
    "**Since DNN training has traditionally relied on IEEE single-precision format, this guide will focus on how to train with half precision while maintaining the network accuracy achieved with single precision (as Figure 1). This technique is called mixed-precision training since it uses both single and half-precision representations.**\n",
    "\n",
    "\n",
    "#### Using Automatic Mixed Precision for Major Deep Learning Frameworks - PyTorch\n",
    "\n",
    "Automatic Mixed Precision feature is available in the Apex repository on GitHub. To enable, add these two lines of code into your existing training script:\n",
    "\n",
    "```python\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "with autocast():\n",
    "    output = model(input)\n",
    "    loss = loss_fn(output, target)\n",
    "\n",
    "scaler.scale(loss).backward()\n",
    "\n",
    "scaler.step(optimizer)\n",
    "\n",
    "scaler.update()\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252dfc0",
   "metadata": {
    "papermill": {
     "duration": 0.038593,
     "end_time": "2024-01-18T17:12:46.099992",
     "exception": false,
     "start_time": "2024-01-18T17:12:46.061399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">Train your own Tokenizer</div>\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "Large Language Generative AI models are developed mostly working with large amounts of text data. For this reason anyone working in this area should have specific skills in text processing. To enable AI models to learn from text data effectively we must first preprocess text into a format which is understandable to machines. Tokenization and Vectorization are two of the most important steps in this procedure. \n",
    "\n",
    "Before our data can be fed to a model, it needs to be transformed to a format the model can understand. Machine learning algorithms take numbers as inputs. This means that we will need to convert the texts into numerical vectors. There are two steps to this process:\n",
    "\n",
    "1. **Tokenization:** Divide the texts into words or smaller sub-texts, which will enable good generalization of relationship between the texts and the labels. This determines the ‚Äúvocabulary‚Äù of the dataset (set of unique tokens present in the data). (Splitting text into smaller units such as words or phrases.)\n",
    "\n",
    "1. **Vectorization:** Define a good numerical measure to characterize these texts. Converting text into numerical representations for ML models.\n",
    "\n",
    "In summary, the typical order is tokenization first to break down the text into understandable units and then vectorization to turn those units into a numerical format suitable for machine learning models.\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Tokenization</span>\n",
    "\n",
    "Text tokenization is the process of reformatting a piece of text into smaller units called ‚Äútokens.‚Äù It transforms unstructured text into structured data that models can understand. The goal of tokenization is to break down text into meaningful units like words, phrases, sentences, etc. which can then be inputted into machine learning models. It‚Äôs one of the first and most important steps in natural language preprocessing, and often goes hand-in-hand with text vectorization.\n",
    "\n",
    "Tokenization enables natural language processing tasks like part-of-speech tagging (identifying verbs vs nouns, etc.), named entity recognition (categories like person, organization, location), and relationship extraction (family relationships, professional relationships, etc.).\n",
    "\n",
    "There are a number of different tokenization methods; some of the simpler ones include splitting text on whitespace or punctuation. Advanced techniques use language rules to identify word boundaries and tokenize text into linguistic units; this can split words into sub-word tokens (such as prefixes, or based on syllables), or even combine certain tokens into larger units based on language semantics. The goal is to produce tokens that best represent the original text for ML purposes.\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Vectorization</span>\n",
    "\n",
    "Now since most Large language models today are based on Transformers and Deep Learning architectures, they still work best with numbers, so to enable them to learn from text we should also convert the tokens to numbers, so each word will be represented with a number instead of sequence of letters. After tokenization, the tokens can then be converted into numerical format through vectorization, which is necessary because machine learning models don't understand text directly; they understand numbers. Vectorization represents the tokens in a way that the model can understand, often as vectors in a high-dimensional space. There are several methods of vectorization, including **Bag of Words**, **TF-IDF**, and **word embeddings** like **Word2Vec** or **GloVe**.\n",
    "\n",
    "Text vectorization is the process of converting text into numerical representations (or ‚Äúvectors‚Äù) that can be understood by ML models. It transforms unstructured text into structured numeric data with the goal to represent the semantic meaning of text in a mathematical format.\n",
    "\n",
    "Text vectorization allows for a variety of NLP tasks like document classification (checking whether something is an email or an essay, etc.), sentiment analysis (opinions or attitudes of the text, etc.), enhancing search engines, and so on.\n",
    "\n",
    "Common text vectorization methods include **one-hot encoding** (assigning a unique integer value to each word), **bag-of-words** (counting the occurrence of words within each document), and **word embeddings** (mapping words to vectors so as to capturing meaning). ***The vector space allows words with similar meanings to have similar representations.***\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">Tokenizers</span>\n",
    "\n",
    "Before you can use your data in a model, the data needs to be processed into an acceptable format for the model. A model does not understand raw text, images or audio. These inputs need to be converted into numbers and assembled into tensors.\n",
    "\n",
    "**The main tool for processing textual data is a tokenizer. A tokenizer starts by splitting text into tokens according to a set of rules. The tokens are converted into numbers, which are used to build tensors as input to a model. Any additional inputs required by a model are also added by the tokenizer.**\n",
    "\n",
    "\n",
    "On this section, we will have a closer look at tokenization. As we saw, tokenizing a text is splitting it into **words** or **subwords**, which then are converted to ids through a look-up table. Converting words or subwords to ids is straightforward, so in this summary, we will focus on splitting a text into words or subwords (i.e. tokenizing a text). \n",
    "More specifically, we will look at the three main types of tokenizers used in ü§ó Transformers: \n",
    "\n",
    "1. **Byte-Pair Encoding (BPE)**, \n",
    "1. **WordPiece**, \n",
    "1. and **SentencePiece**, and show examples of which tokenizer type is used by which model.\n",
    "\n",
    "Note that on each model page, you can look at the documentation of the associated tokenizer to know which tokenizer type was used by the pretrained model. For instance, if we look at [BertTokenizer](https://huggingface.co/docs/transformers/v4.36.1/en/model_doc/bert#transformers.BertTokenizer), we can see that the model uses WordPiece.\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "***What is a tokenizer?***\n",
    "\n",
    "The definition of tokenization, as given by Stanford NLP group is:\n",
    "\n",
    "‚ÄúGiven a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation‚Äù\n",
    "\n",
    "Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data.\n",
    "\n",
    "The goal is to find the most meaningful representation ‚Äî that is, the one that makes the most sense to the model ‚Äî and, if possible, the smallest representation.\n",
    "\n",
    "There are different solutions available: **word-based**, **character-based** but the one used by the state-of-the-art transformer models are **sub-word tokenizers**: Byte-level BPE(GPT-2), WordPiece(BERT) etc.\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Space & Punctuation Tokenization</span>\n",
    "\n",
    "Splitting a text into smaller chunks is a task that is harder than it looks, and there are multiple ways of doing so. For instance, let‚Äôs look at the sentence `\"Don't you love ü§ó Transformers? We sure do.\"`\n",
    "\n",
    "A simple way of tokenizing this text is to split it by spaces, which would give:\n",
    "\n",
    "`[\"Don't\", \"you\", \"love\", \"ü§ó\", \"Transformers?\", \"We\", \"sure\", \"do.\"]`\n",
    "\n",
    "This is a sensible first step, but if we look at the tokens \"Transformers?\" and \"do.\", we notice that the punctuation is attached to the words \"Transformer\" and \"do\", which is suboptimal. We should take the punctuation into account so that a model does not have to learn a different representation of a word and every possible punctuation symbol that could follow it, which would explode the number of representations the model has to learn. Taking punctuation into account, tokenizing our exemplary text would give:\n",
    "\n",
    "`[\"Don\", \"'\", \"t\", \"you\", \"love\", \"ü§ó\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]`\n",
    "\n",
    "Better. However, it is disadvantageous, how the tokenization dealt with the word \"Don't\". \"Don't\" stands for \"do not\", so it would be better tokenized as `[\"Do\", \"n't\"]`. This is where things start getting complicated, and part of the reason each model has its own tokenizer type. Depending on the rules we apply for tokenizing a text, a different tokenized output is generated for the same text. A pretrained model only performs properly if you feed it an input that was tokenized with the same rules that were used to tokenize its training data.\n",
    "\n",
    "**spaCy** and **Moses** are two popular **rule-based tokenizers**. Applying them on our example, spaCy and Moses would output something like:\n",
    "\n",
    "`[\"Do\", \"n't\", \"you\", \"love\", \"ü§ó\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]`\n",
    "\n",
    "As can be seen space and punctuation tokenization, as well as rule-based tokenization, is used here. Space and punctuation tokenization and rule-based tokenization are both examples of word tokenization, which is loosely defined as splitting sentences into words. While it‚Äôs the most intuitive way to split texts into smaller chunks, this tokenization method can lead to problems for massive text corpora. In this case, space and punctuation tokenization usually generates a very big vocabulary (the set of all unique words and tokens used). E.g., **Transformer XL uses space and punctuation tokenization**, resulting in a vocabulary size of 267,735!\n",
    "\n",
    "Such a big vocabulary size forces the model to have an enormous embedding matrix as the input and output layer, which causes both an increased memory and time complexity. In general, **transformers models rarely have a vocabulary size greater than 50,000, especially if they are pretrained only on a single language.**\n",
    "\n",
    "So if simple space and punctuation tokenization is unsatisfactory, why not simply tokenize on characters?\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Character Tokenization</span>\n",
    "\n",
    "While character tokenization is very simple and would greatly reduce memory and time complexity it makes it much harder for the model to learn meaningful input representations. E.g. learning a meaningful context-independent representation for the letter \"t\" is much harder than learning a context-independent representation for the word \"today\". Therefore, character tokenization is often accompanied by a loss of performance. **So to get the best of both worlds, transformers models use a hybrid between word-level and character-level tokenization called subword tokenization.**\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Subword Tokenization</span>\n",
    "Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. For instance \"annoyingly\" might be considered a rare word and could be decomposed into \"annoying\" and \"ly\". Both \"annoying\" and \"ly\" as stand-alone subwords would appear more frequently while at the same time the meaning of \"annoyingly\" is kept by the composite meaning of \"annoying\" and \"ly\". This is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.\n",
    "\n",
    "Subword tokenization allows the model to have a reasonable vocabulary size while being able to learn meaningful context-independent representations. In addition, subword tokenization enables the model to process words it has never seen before, by decomposing them into known subwords. For instance, the BertTokenizer tokenizes \"I have a new GPU!\" as follows:\n",
    "\n",
    "`[\"i\", \"have\", \"a\", \"new\", \"gp\", \"##u\", \"!\"]`\n",
    "\n",
    "Because we are considering the uncased model, the sentence was lowercased first. We can see that the words `[\"i\", \"have\", \"a\", \"new\"]` are present in the tokenizer‚Äôs vocabulary, but the word \"gpu\" is not. Consequently, the tokenizer splits \"gpu\" into known subwords: `[\"gp\" and \"##u\"]`. \"##\" means that the rest of the token should be attached to the previous one, without space (for decoding or reversal of the tokenization).\n",
    "As another example, XLNetTokenizer tokenizes our previously exemplary text as follows:\n",
    "\n",
    "`[\"‚ñÅDon\", \"'\", \"t\", \"‚ñÅyou\", \"‚ñÅlove\", \"‚ñÅ\", \"ü§ó\", \"‚ñÅ\", \"Transform\", \"ers\", \"?\", \"‚ñÅWe\", \"‚ñÅsure\", \"‚ñÅdo\", \".\"]`\n",
    "\n",
    "We‚Äôll get back to the meaning of those \"‚ñÅ\" when we look at SentencePiece. As one can see, the rare word \"Transformers\" has been split into the more frequent subwords \"Transform\" and \"ers\".\n",
    "\n",
    "Let‚Äôs now look at how the different subword tokenization algorithms work. Note that all of those tokenization algorithms rely on some form of training which is usually done on the corpus the corresponding model will be trained on.\n",
    "\n",
    "Concepts related to BPE:\n",
    "\n",
    "\n",
    "1. **Vocabulary:** A set of subword units that can be used to represent a text corpus.\n",
    "1. **Byte:** A unit of digital information that typically consists of eight bits.\n",
    "1. **Character:** A symbol that represents a written or printed letter or numeral.\n",
    "1. **Frequency:** The number of times a byte or character occurs in a text corpus.\n",
    "1. **Merge:** The process of combining two consecutive bytes or characters to create a new subword unit.\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Byte-Pair Encoding (BPE)</span>\n",
    "\n",
    "Byte-Pair Encoding (BPE) was introduced in Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015). BPE relies on a pre-tokenizer that splits the training data into words. Pretokenization can be as simple as space tokenization, e.g. GPT-2, RoBERTa. More advanced pre-tokenization include rule-based tokenization, e.g. XLM, FlauBERT which uses Moses for most languages, or GPT which uses Spacy and ftfy, to count the frequency of each word in the training corpus.\n",
    "\n",
    "After pre-tokenization, a set of unique words has been created and the frequency with which each word occurred in the training data has been determined. Next, BPE creates a base vocabulary consisting of all symbols that occur in the set of unique words and learns merge rules to form a new symbol from two symbols of the base vocabulary. It does so until the vocabulary has attained the desired vocabulary size. Note that the desired vocabulary size is a hyperparameter to define before training the tokenizer.\n",
    "\n",
    "As an example, let‚Äôs assume that after pre-tokenization, the following set of words including their frequency has been determined:\n",
    "\n",
    "`(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)`\n",
    "\n",
    "Consequently, the base vocabulary is `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]`. Splitting all words into symbols of the base vocabulary, we obtain:\n",
    "\n",
    "`(\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)`\n",
    "\n",
    "BPE then counts the frequency of each possible symbol pair and picks the symbol pair that occurs most frequently. In the example above \"h\" followed by \"u\" is present 10 + 5 = 15 times (10 times in the 10 occurrences of \"hug\", 5 times in the 5 occurrences of \"hugs\"). However, the most frequent symbol pair is \"u\" followed by \"g\", occurring 10 + 5 + 5 = 20 times in total. Thus, the first merge rule the tokenizer learns is to group all \"u\" symbols followed by a \"g\" symbol together. Next, \"ug\" is added to the vocabulary. The set of words then becomes\n",
    "\n",
    "`(\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)`\n",
    "\n",
    "BPE then identifies the next most common symbol pair. It‚Äôs \"u\" followed by \"n\", which occurs 16 times. \"u\", \"n\" is merged to \"un\" and added to the vocabulary. The next most frequent symbol pair is \"h\" followed by \"ug\", occurring 15 times. Again the pair is merged and \"hug\" can be added to the vocabulary.\n",
    "\n",
    "At this stage, the vocabulary is `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]` and our set of unique words is represented as\n",
    "\n",
    "`(\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)`\n",
    "\n",
    "Assuming, that the Byte-Pair Encoding training would stop at this point, the learned merge rules would then be applied to new words (as long as those new words do not include symbols that were not in the base vocabulary). For instance, the word \"bug\" would be tokenized to `[\"b\", \"ug\"]` but \"mug\" would be tokenized as `[\"<unk>\", \"ug\"]` since the symbol `\"m\"` is not in the base vocabulary. In general, single letters such as `\"m\"` are not replaced by the `\"<unk>\"` symbol because the training data usually includes at least one occurrence of each letter, but it is likely to happen for very special characters like emojis.\n",
    "\n",
    "As mentioned earlier, the vocabulary size, i.e. the base vocabulary size + the number of merges, is a hyperparameter to choose. For instance GPT has a vocabulary size of 40,478 since they have 478 base characters and chose to stop training after 40,000 merges.\n",
    "\n",
    "**Recap:** Steps involved in BPE:\n",
    "\n",
    "1. Initialize the vocabulary with all the bytes or characters in the text corpus\n",
    "1. Calculate the frequency of each byte or character in the text corpus.\n",
    "1. Repeat the following steps until the desired vocabulary size is reached:\n",
    "    - Find the most frequent pair of consecutive bytes or characters in the text corpus\n",
    "    - Merge the pair to create a new subword unit.\n",
    "    - Update the frequency counts of all the bytes or characters that contain the merged pair.\n",
    "    - Add the new subword unit to the vocabulary.\n",
    "\n",
    "1. Represent the text corpus using the subword units in the vocabulary.\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">Byte-level BPE</span>\n",
    "\n",
    "A base vocabulary that includes all possible base characters can be quite large if e.g. all unicode characters are considered as base characters. To have a better base vocabulary, GPT-2 uses bytes as the base vocabulary, which is a clever trick to force the base vocabulary to be of size 256 while ensuring that every base character is included in the vocabulary. With some additional rules to deal with punctuation, the GPT2‚Äôs tokenizer can tokenize every text without the need for the `<unk>` symbol. GPT-2 has a vocabulary size of 50,257, which corresponds to the 256 bytes base tokens, a special end-of-text token and the symbols learned with 50,000 merges.\n",
    "\n",
    "**Understanding Characters and Bytes:**\n",
    "\n",
    "1. **Characters:** These are the basic units of text (like 'A', '7', '!', '√©', '‰∏≠'). In human language, we see these as individual symbols or letters.\n",
    "1. **Bytes:** A byte is a unit of digital information that commonly consists of eight bits. It's a fundamental concept in computer science and is used to represent data.\n",
    "\n",
    "**Character Encoding:**\n",
    "\n",
    "Characters are represented in computers using various encoding systems, which map characters to specific byte sequences. Two common encodings are ASCII and UTF-8:\n",
    "\n",
    "1. **ASCII (American Standard Code for Information Interchange):** This is one of the oldest character encoding standards. It uses one byte (8 bits) per character and can represent up to 256 different symbols (0-255). ASCII is limited to English characters and some control characters and symbols. The maximum of 256 different symbols in ASCII is due to its use of one byte per character, and a byte consists of 8 bits. Here's a breakdown of why this limits it to 256 symbols. When you have a single bit, you have two possible values (0 or 1). With two bits, you can have 4 possible combinations (00, 01, 10, 11). For 8 bits (1 byte), the number of possible combinations is  2^8 = 256. This range is from 0 to 255, which gives 256 total possible values.\n",
    "\n",
    "1. **UTF-8 (8-bit Unicode Transformation Format):** This is a more modern and versatile encoding standard capable of representing a vast array of characters from virtually all written languages. UTF-8 is backward compatible with ASCII but can use one to four bytes per character, allowing it to cover much more than the basic ASCII set.\n",
    "\n",
    "\n",
    "For the GPT models, OpenAI uses a method known as byte-level byte pair encoding, instead of alphabets or ASCII, the base vocabulary is defined in bytes. Since every character in any encoding on a computer is created from bytes, the base vocabulary contains every possible combination of byte, and the tokenizer never runs into an unknown token.\n",
    "\n",
    "**Byte-Level:** \n",
    "\n",
    "Instead of starting with a vocabulary of words or characters (like alphabets or ASCII characters), byte-level BPE operates on bytes, which are essentially the smallest addressable group of bits in a computer (usually 8 bits). This means the base vocabulary consists of all 256 possible byte values (from 0 to 255).\n",
    "In traditional BPE or other tokenization methods that start with characters, the process involves looking at the text's character-level representation. For example, the word \"hello\" would be considered as 'h', 'e', 'l', 'l', 'o' ‚Äì five separate characters.\n",
    "\n",
    "In Byte-Level BPE, instead of looking at characters, we consider the byte representation of the text. This approach doesn't start with an understanding of \"characters\" per se but with the bytes that encode these characters. Here's why it's significant:\n",
    "\n",
    "\n",
    "**Why Byte-Level?:**\n",
    "\n",
    "1. **All-Inclusive:** Since every character (no matter the language or symbol) can be broken down into bytes, starting with bytes ensures that the vocabulary can represent any text without missing symbols or needing placeholders for unknowns.\n",
    "\n",
    "1. **Simplifies Vocabulary:** Instead of potentially needing thousands of character tokens to cover various languages and symbols, Byte-Level BPE only needs 256 base tokens, corresponding to all possible values of a byte (0-255). This drastically simplifies the model's vocabulary.\n",
    "\n",
    "1. **Handles Varied Text:** By using bytes, the tokenizer can handle texts in ASCII (like English text) and texts in more complex encodings like UTF-8 (which can represent virtually all human languages) without needing separate mechanisms or special handling for different languages or symbol sets.\n",
    "\n",
    "1. **Universality:** Bytes are the fundamental building blocks of digital data. By using bytes, the model can represent any character in any language or even other forms of data like emojis or special symbols without being restricted to a specific character set. This universality means that it can process text in virtually any language or symbol system.\n",
    "\n",
    "1. **No Unknown Tokens:** Traditional tokenizers might encounter characters or words they have never seen before (out-of-vocabulary words), leading to the use of a special \"unknown\" token. Byte-level BPE virtually eliminates this problem because every piece of text can be broken down into bytes, which are always within the model's vocabulary. Thus, the tokenizer is capable of handling any text input without encountering unknown tokens.\n",
    "\n",
    "\n",
    "So, when we say a \"character is a byte\" in the context of Byte-Level BPE, it's a bit of a simplification. A more accurate statement would be: \"All characters can be represented as sequences of bytes, and Byte-Level BPE uses these byte sequences as the foundational elements of its vocabulary.\" This means each character in text is represented by one or more bytes, depending on its encoding, and these bytes are the building blocks for the tokenizer's vocabulary and subsequent text processing. In summary, byte-level BPE is a way of preparing text for machine learning models like GPT that is both highly versatile and capable of handling a wide variety of languages and symbols without running into the issue of unknown tokens. It's a foundational aspect of how these models process and understand the text data they're trained on and generate.\n",
    "\n",
    "\n",
    "Byte-Level Byte Pair Encoding (BPE) is a tokenization method that builds upon the standard BPE algorithm by using bytes as the fundamental unit for its vocabulary. This approach, as used in models like GPT-2, is particularly effective and efficient for several reasons. Here's a more detailed explanation of how it works and its benefits:\n",
    "\n",
    "**Base Vocabulary:**\n",
    "\n",
    "1. **Standard BPE:** Traditional Byte Pair Encoding starts with a base vocabulary of all unique characters (or tokens) in the training corpus and iteratively combines the most frequent pair of tokens to create new, longer tokens. This process continues for a number of merges, determined beforehand.\n",
    "\n",
    "1. **Byte-Level BPE:** Instead of starting with characters, Byte-Level BPE considers each byte (256 possible values in total, representing all possible single-byte characters) as the base vocabulary. This approach automatically includes all possible characters in ASCII and extends to any byte value that might represent a part of a character in more extensive encoding systems like UTF-8.\n",
    "\n",
    "***Advantages:***\n",
    "\n",
    "1. **Compact and Comprehensive Base Vocabulary:** By using bytes, the base vocabulary is limited to 256 tokens (since there are 256 possible byte values), which is more compact compared to potentially thousands of Unicode characters. Yet, it's comprehensive enough to represent any text because all text can be broken down into bytes.\n",
    "\n",
    "1. **Eliminating `<unk>` Tokens:** Traditional tokenizers might encounter unknown characters or words not present in the vocabulary, often represented by an `<unk>` (unknown) token. Since Byte-Level BPE can tokenize any text into bytes (and subsequentially into byte-level tokens), it theoretically doesn't need an `<unk>` symbol, as every possible byte can be represented in its vocabulary.\n",
    "\n",
    "1. **Handling Diverse Scripts and Symbols:** With the ability to represent any character as a series of bytes, Byte-Level BPE is naturally equipped to handle text in multiple languages, including those with large character sets or special symbols, without needing separate models or token sets for different languages.\n",
    "\n",
    "**GPT-2's Vocabulary:**\n",
    "In the case of GPT-2:\n",
    "\n",
    "- **256 Base Tokens:** Corresponding to all possible byte values.\n",
    "- **Special End-of-Text Token:** Used to signify the end of a text.\n",
    "- **50,000 Merges:** The tokenizer iteratively combines frequent pairs of these byte-level tokens to form higher-level tokens, up to 50,000 merges. These merges are learned from the training corpus and represent common words, subwords, or sequences of characters that appear frequently together.\n",
    "\n",
    "The resulting vocabulary size is 50,257 (256 base tokens + 1 special token + 50,000 merged tokens), which provides a good balance between granularity and coverage. This means GPT-2's tokenizer is capable of handling a wide variety of texts, from different languages and domains, without a substantial increase in vocabulary size, making it efficient and powerful for language understanding and generation tasks.\n",
    "    \n",
    "#### <span style=\"color: #7b6b59;\">WordPiece</span>\n",
    "\n",
    "WordPiece is the subword tokenization algorithm used for BERT, DistilBERT, and Electra. The algorithm was outlined in Japanese and Korean Voice Search (Schuster et al., 2012) and is very similar to BPE. WordPiece first initializes the vocabulary to include every character present in the training data and progressively learns a given number of merge rules. In contrast to BPE, WordPiece does not choose the most frequent symbol pair, but the one that maximizes the likelihood of the training data once added to the vocabulary.\n",
    "\n",
    "So what does this mean exactly? Referring to the previous example, maximizing the likelihood of the training data is equivalent to finding the symbol pair, whose probability divided by the probabilities of its first symbol followed by its second symbol is the greatest among all symbol pairs. E.g. \"u\", followed by \"g\" would have only been merged if the probability of \"ug\" divided by \"u\", \"g\" would have been greater than for any other symbol pair. Intuitively, WordPiece is slightly different to BPE in that it evaluates what it loses by merging two symbols to ensure it‚Äôs worth it.\n",
    "\n",
    "\n",
    "#### <span style=\"color: #7b6b59;\">SentencePiece</span>\n",
    "\n",
    "All tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to separate words. However, not all languages use spaces to separate words. One possible solution is to use language specific pre-tokenizers, e.g. XLM uses a specific Chinese, Japanese, and Thai pre-tokenizer). To solve this problem more generally, SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018) treats the input as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.\n",
    "\n",
    "The XLNetTokenizer uses SentencePiece for example, which is also why in the example earlier the \"‚ñÅ\" character was included in the vocabulary. Decoding with SentencePiece is very easy since all tokens can just be concatenated and \"‚ñÅ\" is replaced by a space.\n",
    "\n",
    "All transformers models in the library that use SentencePiece use it in combination with unigram. Examples of models using SentencePiece are ALBERT, XLNet, Marian, and T5.\n",
    "\n",
    "\n",
    "\n",
    "## <span style=\"color: #7b6b59;\">HuggingFace Tokenizers: `tokenizers` Library</span>\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Introduction</span>\n",
    "\n",
    "Fast State-of-the-art [tokenizers](https://huggingface.co/docs/tokenizers/index), optimized for both research and production\n",
    "\n",
    "[ü§ó Tokenizers](https://github.com/huggingface/tokenizers) provides an implementation of today‚Äôs most used tokenizers, with a focus on performance and versatility. These tokenizers are also used in [ü§ó Transformers](https://github.com/huggingface/transformers).\n",
    "\n",
    "**Main features:**\n",
    "\n",
    "- Train new vocabularies and tokenize, using today‚Äôs most used tokenizers.\n",
    "- Extremely fast (both training and tokenization), thanks to the Rust implementation. Takes less than 20 seconds to tokenize a GB of text on a server‚Äôs CPU.\n",
    "- Easy to use, but also extremely versatile.\n",
    "- Designed for both research and production.\n",
    "- Full alignment tracking. Even with destructive normalization, it‚Äôs always possible to get the part of the original sentence that corresponds to any token.\n",
    "- Does all the pre-processing: Truncation, Padding, add the special tokens your model needs.\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">The tokenization pipeline</span>\n",
    "\n",
    "In this section, we will try to understand the HuggingFace tokenizers in depth and will go through all the parameters and also the outputs returned by a tokenizer. We‚Äôll dive into the AutoTokenizer class and see how to use a pre-trained tokenizer for our data.\n",
    "\n",
    "So, let‚Äôs get started!\n",
    "\n",
    "Hugging Face is a New York based company that has swiftly developed language processing expertise. The company‚Äôs aim is to advance NLP and democratize it for use by practitioners and researchers around the world.\n",
    "\n",
    "In an effort to offer access to fast, state-of-the-art, and easy-to-use tokenization that plays well with modern NLP pipelines, Hugging Face contributors have developed and open-sourced Tokenizers. Tokenizers is, as the name implies, an implementation of today‚Äôs most widely used tokenizers with emphasis on performance and versatility.\n",
    "\n",
    "An implementation of a tokenizer consists of the following pipeline of processes, each applying different transformations to the textual information. When calling Tokenizer.encode or Tokenizer.encode_batch, the input text(s) go through the following pipeline:\n",
    "\n",
    "- normalization\n",
    "- pre-tokenization\n",
    "- model\n",
    "- post-processing\n",
    "\n",
    "We‚Äôll see in details what happens during each of those steps in detail, as well as when you want to decode `<decoding>` some token ids, and how the ü§ó Tokenizers library allows you to customize each of those steps to your needs. \n",
    "\n",
    "Let‚Äôs go through these steps:\n",
    "\n",
    "<img width=\"935\" alt=\"image\" src=\"https://github.com/eraikakou/LLMs-News/assets/28102493/57dbec9a-de4a-4bed-b4a0-491639298f65\">\n",
    "\n",
    "1. **Normalization:** The [normalization step](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.normalizers) involves some general cleanup, such as removing needless whitespace, lowercasing, and/or removing accents. If you‚Äôre familiar with Unicode normalization (such as NFC or NFKC), this is also something the tokenizer may apply. `\"H√©ll√≤ h√¥w are yo√º?\"` Given the input above, the normalization step would transform it into: `\"hello, how are you?\"`. Normalization is, in a nutshell, a set of operations you apply to a raw string to make it less random or ‚Äúcleaner‚Äù. Common operations include stripping whitespace, removing accented characters or lowercasing all text. If you‚Äôre familiar with Unicode normalization, it is also a very common normalization operation applied in most tokenizers. Each normalization operation is represented in the ü§ó Tokenizers library by a `Normalizer`, and you can combine several of those by using a `normalizers.Sequence.` Here is a normalizer applying NFD Unicode normalization and removing accents as an example:\n",
    "\n",
    "    ```python\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents\n",
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "# You can manually test that normalizer by applying it to any string:\n",
    "normalizer.normalize_str(\"H√©ll√≤ h√¥w are √º?\")\n",
    "``` \n",
    "    When building a Tokenizer, you can customize its normalizer by just changing the corresponding attribute: `tokenizer.normalizer = normalizer`. Of course, if you change the way a tokenizer applies normalization, you should probably retrain it from scratch afterward.\n",
    "\n",
    "1. **Pre-tokenization:** A tokenizer cannot be trained on raw text alone. Instead, we first need to split the texts into small entities, like words. That‚Äôs where the pre-tokenization step comes in. A word-based tokenizer can simply split a raw text into words on whitespace and punctuation. Those words will be the boundaries of the subtokens the tokenizer can learn during its training. `\"hello, how are you?\"`. Given this string, the pre-tokenizer‚Äôs output will be something like: `[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]`. As we can see, the tokenizer also keeps track of the offsets. Also, the rules for [pre-tokenization](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.pre_tokenizers) can vary with the tokenizer being used. For instance, BERT will have different set of rules for this step than GPT-2. Pre-tokenization is the act of splitting a text into smaller objects that give an upper bound to what your tokens will be at the end of training. A good way to think of this is that the pre-tokenizer will split your text into ‚Äúwords‚Äù and then, your final tokens will be parts of those words. An easy way to pre-tokenize inputs is to split on spaces and punctuations, which is done by the `pre_tokenizers.Whitespace pre-tokenizer`. Of course, if you change the way the pre-tokenizer, you should probably retrain your tokenizer from scratch afterward. The output is a list of tuples, with each tuple containing one word and its span in the original sentence (which is used to determine the final offsets of our Encoding). Note that splitting on punctuation will split contractions like \"I'm\" in this example. You can combine together any PreTokenizer together. For instance, here is a pre-tokenizer that will split on space, punctuation and digits, separating numbers in their individual digits:\n",
    "`pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])\n",
    "`\n",
    "\n",
    "1. **Modeling:** After normalization and pre-processing steps, we apply [a training algorithm](https://huggingface.co/docs/tokenizers/python/latest/api/reference.html#module-tokenizers.models) to the text data. This output of this step is dependent on the type of training strategy we are going to use. The state-of-the-art models use subword tokenization algorithms, for example BERT uses WordPiece tokenization, GPT, GPT-2 use BPE, AIBERT uses unigram etc. Using a BERT tokenizer, will tokenize the sentence like this: `[\"hello\"; \",\"; \"how\"; \"are\"; \"you\"; \"?\"]`. Once the input texts are normalized and pre-tokenized, the Tokenizer applies the model on the pre-tokens. ***This is the part of the pipeline that needs training on your corpus (or that has been trained if you are using a pretrained tokenizer).*** ***The role of the model is to split your ‚Äúwords‚Äù into tokens, using the rules it has learned.*** It‚Äôs also responsible for mapping those tokens to their corresponding IDs in the vocabulary of the model. This model is passed along when intializing the Tokenizer so you already know how to customize this part. Currently, the ü§ó Tokenizers library supports:\n",
    "    - models.BPE\n",
    "    - models.Unigram\n",
    "    - models.WordLevel\n",
    "    - models.WordPiece\n",
    "\n",
    "1. **Post-processing:** Similar to the modeling part, a number of post-processors are available depending on the training strategy used. They‚Äôre responsible for adding the special tokens to the input sequence as needed by the model. Using a BERT post-processor to our sequence will result in: `[\"CLS\"; \"hello\"; \",\"; \"how\"; \"are\"; \"you\"; \"?\"; \"SEP\"]`. Here, `[CLS]` denotes the classification token, which tells the model that this is a classification task and `[SEP]` denotes the end of sentence and is also used between two sentences. Post-processing is the last step of the tokenization pipeline, to perform any additional transformation to the Encoding before it‚Äôs returned, like adding potential special tokens.\n",
    "\n",
    "Subword tokenization methods, such as Byte Pair Encoding (BPE), WordPiece, or SentencePiece, need to be trained on a specific corpus to learn an efficient and effective way of breaking down words into smaller units (subwords). The training process allows the tokenizer to adapt to the particularities of the text it will be processing.\n",
    "\n",
    "**The Training Process:**\n",
    "\n",
    "During training, a subword tokenizer typically starts with a large corpus of text and performs the following:\n",
    "\n",
    "1. **Initial Vocabulary Creation:** It creates an initial vocabulary, often at the character level or using a simple character or word frequency threshold.\n",
    "\n",
    "1. **Merging Rules Learning:** It iteratively finds the most frequent pairs of characters or subwords and merges them to form a new, longer subword. This process repeats until a set number of merges is reached or the desired vocabulary size is achieved.\n",
    "\n",
    "1. **Final Vocabulary Compilation:** The final vocabulary consists of the original characters plus all the merged subwords, ensuring that any word can be tokenized using this set.\n",
    "\n",
    "In essence, the training of a subword tokenizer is about learning the most efficient and effective way to break down and represent the text it will encounter, taking into account frequency, morphology, and the specific needs of the task or language. This process results in a tokenizer that can handle a wide variety of text inputs, generalize well to new text, and efficiently interface with downstream language models or other NLP tools.\n",
    "\n",
    "\n",
    "### <span style=\"color: #7b6b59;\">Build a tokenizer from scratch</span>\n",
    "\n",
    "To illustrate how fast the ü§ó Tokenizers library is, let‚Äôs train a new tokenizer on wikitext-103 (516M of text) in just a few seconds. In this tour, we will build and train a Byte-Pair Encoding (BPE) tokenizer. Here, training the tokenizer means it will learn merge rules by:\n",
    "\n",
    "1. Start with all the characters present in the training corpus as tokens.\n",
    "1. Identify the most common pair of tokens and merge it into one token.\n",
    "1. Repeat until the vocabulary (e.g., the number of tokens) has reached the size we want.\n",
    "\n",
    "The main API of the library is the class `Tokenizer`, here is how we instantiate one with a BPE model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8c38f98e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:46.182547Z",
     "iopub.status.busy": "2024-01-18T17:12:46.181379Z",
     "iopub.status.idle": "2024-01-18T17:12:46.188744Z",
     "shell.execute_reply": "2024-01-18T17:12:46.187597Z"
    },
    "papermill": {
     "duration": 0.050772,
     "end_time": "2024-01-18T17:12:46.191182",
     "exception": false,
     "start_time": "2024-01-18T17:12:46.140410",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, normalizers\n",
    "from tokenizers.normalizers import NFD, StripAccents, NFC, Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace, ByteLevel\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "\n",
    "LOWERCASE = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d34b7c4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:46.271629Z",
     "iopub.status.busy": "2024-01-18T17:12:46.270778Z",
     "iopub.status.idle": "2024-01-18T17:12:46.408801Z",
     "shell.execute_reply": "2024-01-18T17:12:46.407629Z"
    },
    "papermill": {
     "duration": 0.181385,
     "end_time": "2024-01-18T17:12:46.411604",
     "exception": false,
     "start_time": "2024-01-18T17:12:46.230219",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The main API of the library is the class Tokenizer, here is how we instantiate one with a BPE model:\n",
    "# Creating Byte-Pair Encoding tokenizer\n",
    "# we instantiate a new Tokenizer with this model - BPE\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d71b805b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:46.497516Z",
     "iopub.status.busy": "2024-01-18T17:12:46.496413Z",
     "iopub.status.idle": "2024-01-18T17:12:46.505168Z",
     "shell.execute_reply": "2024-01-18T17:12:46.504192Z"
    },
    "papermill": {
     "duration": 0.053742,
     "end_time": "2024-01-18T17:12:46.507881",
     "exception": false,
     "start_time": "2024-01-18T17:12:46.454139",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello how are u?\n",
      "H√©ll√≤ h√¥w are √º?\n"
     ]
    }
   ],
   "source": [
    "normalizer = normalizers.Sequence([NFD(), StripAccents()])\n",
    "# You can manually test that normalizer by applying it to any string:\n",
    "print(normalizer.normalize_str(\"H√©ll√≤ h√¥w are √º?\"))\n",
    "\n",
    "normalizer = normalizers.Sequence([NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\n",
    "print(normalizer.normalize_str(\"H√©ll√≤ h√¥w are √º?\"))\n",
    "tokenizer.normalizer = normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0d457e06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:46.589523Z",
     "iopub.status.busy": "2024-01-18T17:12:46.589110Z",
     "iopub.status.idle": "2024-01-18T17:12:46.601432Z",
     "shell.execute_reply": "2024-01-18T17:12:46.600404Z"
    },
    "papermill": {
     "duration": 0.056661,
     "end_time": "2024-01-18T17:12:46.604289",
     "exception": false,
     "start_time": "2024-01-18T17:12:46.547628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', (0, 5)), ('!', (5, 6)), ('How', (7, 10)), ('are', (11, 14)), ('you', (15, 18)), ('?', (18, 19)), ('I', (20, 21)), (\"'\", (21, 22)), ('m', (22, 23)), ('fine', (24, 28)), (',', (28, 29)), ('thank', (30, 35)), ('you', (36, 39)), ('.', (39, 40))]\n",
      "[('ƒ†Hello', (0, 5)), ('!', (5, 6)), ('ƒ†How', (6, 10)), ('ƒ†are', (10, 14)), ('ƒ†you', (14, 18)), ('?', (18, 19)), ('ƒ†I', (19, 21)), (\"'m\", (21, 23)), ('ƒ†fine', (23, 28)), (',', (28, 29)), ('ƒ†thank', (29, 35)), ('ƒ†you', (35, 39)), ('.', (39, 40))]\n"
     ]
    }
   ],
   "source": [
    "pre_tokenizer = Whitespace()\n",
    "print(pre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you.\"))\n",
    "pre_tokenizer = ByteLevel()\n",
    "print(pre_tokenizer.pre_tokenize_str(\"Hello! How are you? I'm fine, thank you.\"))\n",
    "\n",
    "# We could train our tokenizer right now, but it wouldn‚Äôt be optimal.\n",
    "# Without a pre-tokenizer that will split our inputs into words, we might get tokens that overlap several words:\n",
    "# for instance we could get an \"it is\" token since those two words often appear next to each other. \n",
    "# Using a pre-tokenizer will ensure no token is bigger than a word returned by the pre-tokenizer.\n",
    "# Here we want to train a subword BPE tokenizer, and we will use the easiest pre-tokenizer possible by splitting on whitespace.\n",
    "# As we saw in the quicktour, you can customize the pre-tokenizer of a Tokenizer by just changing the corresponding attribute:\n",
    "tokenizer.pre_tokenizer = pre_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b987d1b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-18T17:12:46.684355Z",
     "iopub.status.busy": "2024-01-18T17:12:46.683938Z",
     "iopub.status.idle": "2024-01-18T17:12:46.820740Z",
     "shell.execute_reply": "2024-01-18T17:12:46.819618Z"
    },
    "papermill": {
     "duration": 0.179474,
     "end_time": "2024-01-18T17:12:46.823104",
     "exception": false,
     "start_time": "2024-01-18T17:12:46.643630",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# To train our tokenizer on the wikitext files, we will need to instantiate a [trainer]{.title-ref}, in this case a BpeTrainer\n",
    "# We can set the training arguments like vocab_size or min_frequency \n",
    "# but the most important part is to give the special_tokens we plan to use later on (they are not used at all during training) so that they get inserted in the vocabulary.\n",
    "\n",
    "# Adding special tokens and creating trainer instance\n",
    "# The order in which you write the special tokens list matters: here \"[UNK]\" will get the ID 0, \"[PAD]\" will get the ID 1 and so forth.\n",
    "\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "VOCAB_SIZE = 30522\n",
    "trainer = BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n",
    "\n",
    "# Now, we can just call the Tokenizer.train method with any list of files we want to use:\n",
    "\n",
    "#files = [f\"data/wikitext-103-raw/wiki.{split}.raw\" for split in [\"test\", \"train\", \"valid\"]]\n",
    "#tokenizer.train(files, trainer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c197974",
   "metadata": {
    "papermill": {
     "duration": 0.039039,
     "end_time": "2024-01-18T17:12:46.902073",
     "exception": false,
     "start_time": "2024-01-18T17:12:46.863034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">References</div>\n",
    "\n",
    "\n",
    "1. [Text Tokenization and Vectorization in NLP](https://medium.com/@WojtekFulmyk/text-tokenization-and-vectorization-in-nlp-ac5e3eb35b85)\n",
    "1. [Developing LLMs for Generative AI Tokenization and Vectorization\n",
    "](https://www.linkedin.com/pulse/developing-llms-generative-ai-tokenization-darko-medin/)\n",
    "1. [Google Machine Learning Guide](https://developers.google.com/machine-learning/guides/text-classification/step-3#:~:text=Tokenization%3A%20Divide%20the%20texts%20into,measure%20to%20characterize%20these%20texts.)\n",
    "1. [Hugging Face: Understanding tokenizers\n",
    "](https://medium.com/@awaldeep/hugging-face-understanding-tokenizers-1b7e4afdb154)\n",
    "1. [How to use [HuggingFace‚Äôs] Transformers Pre-Trained tokenizers? - To READ](https://nlpiation.medium.com/how-to-use-huggingfaces-transformers-pre-trained-tokenizers-e029e8d6d1fa)\n",
    "1. [Byte-Pair Encoding (BPE) in NLP](https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/)\n",
    "1. https://neptune.ai/blog/vectorization-techniques-in-nlp-guide\n",
    "1. [Summary of the tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary)\n",
    "1. [Build a tokenizer from scratch](https://huggingface.co/docs/tokenizers/quicktour)\n",
    "1. https://huggingface.co/blog/how-to-train\n",
    "1. [HuggingFace Tokenizers](https://huggingface.co/docs/tokenizers/index)\n",
    "1. [Adding Custom Layers on Top of a Hugging Face Model](https://towardsdatascience.com/adding-custom-layers-on-top-of-a-hugging-face-model-f1ccdfc257bd)\n",
    "1. [Add dense layer on top of Huggingface BERT model](https://stackoverflow.com/questions/64156202/add-dense-layer-on-top-of-huggingface-bert-model)\n",
    "1. [FINE-TUNING PRE-TRAINED MODELS FOR GENERATIVE AI APPLICATIONS](https://www.leewayhertz.com/fine-tuning-pre-trained-models/)\n",
    "1. [Fine-Tuning the Model: What, Why, and How\n",
    "](https://medium.com/@amanatulla1606/fine-tuning-the-model-what-why-and-how-e7fa52bc8ddf)\n",
    "1. https://rumn.medium.com/part-1-ultimate-guide-to-fine-tuning-in-pytorch-pre-trained-model-and-its-configuration-8990194b71e\n",
    "1. https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n",
    "1. https://medium.com/@alexmriggio/bert-for-sequence-classification-from-scratch-code-and-theory-fb88053800fa\n",
    "1. https://mccormickml.com/2019/07/22/BERT-fine-tuning/#4-train-our-classification-model\n",
    "1. https://huggingface.co/transformers/v2.2.0/model_doc/bert.html\n",
    "1. https://towardsdatascience.com/fine-tuning-pretrained-nlp-models-with-huggingfaces-trainer-6326a4456e7b\n",
    "1. [Fine-tune a pretrained model](https://huggingface.co/docs/transformers/training#train-with-pytorch-trainer)\n",
    "1. [What‚Äôs in the Dataset object](https://huggingface.co/docs/datasets/v1.2.1/exploring.html)\n",
    "1. [Loading a Dataset](https://huggingface.co/docs/datasets/v1.1.1/loading_datasets.html)\n",
    "1. [The Dataset object](https://huggingface.co/docs/datasets/v2.2.1/en/access)\n",
    "1. [Create a dataset](https://huggingface.co/docs/datasets/create_dataset)\n",
    "1. [BertForSequenceClassification source code](https://huggingface.co/transformers/v3.0.2/_modules/transformers/modeling_bert.html#BertForSequenceClassification)\n",
    "1. [7 Text Classification Techniques for Any Scenario](https://blog.dataiku.com/7-text-classification-techniques-for-any-scenario#:~:text=A%20simple%20approach%20for%20text,regression%20or%20tree%2Dbased%20models.)\n",
    "1. [TF-IDF Simplified](https://towardsdatascience.com/tf-idf-simplified-aba19d5f5530)\n",
    "1. [Understanding TF-IDF for Machine Learning](https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/)\n",
    "1. [Understanding TF-IDF in NLP: A Comprehensive Guide\n",
    "](https://medium.com/@er.iit.pradeep09/understanding-tf-idf-in-nlp-a-comprehensive-guide-26707db0cec5)\n",
    "1. [TF-IDF Guide: Using scikit-learn for TF-IDF implementation](https://www.capitalone.com/tech/machine-learning/scikit-tfidf-implementation/)\n",
    "1. [Creating BERT Embeddings with Hugging Face Transformers](https://www.analyticsvidhya.com/blog/2023/08/bert-embeddings/)\n",
    "1. [How to use embeddings for feature extraction?](https://medium.com/mlearning-ai/how-to-use-embeddings-for-feature-extraction-4956db52b5f5)\n",
    "1. [Feedback Prize - English Language Learning](https://www.kaggle.com/competitions/feedback-prize-english-language-learning/code?competitionId=38321&sortBy=voteCount)\n",
    "1. [Tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer)\n",
    "1. [Summary of the tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary)\n",
    "1. [The tokenization pipeline](https://huggingface.co/docs/tokenizers/pipeline)\n",
    "1. [Preprocess](https://huggingface.co/docs/transformers/preprocessing)\n",
    "1. [How to use BERT from the Hugging Face transformer library\n",
    "](https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209)\n",
    "1. [Neural Networks: Pooling Layers](https://www.baeldung.com/cs/neural-networks-pooling-layers)\n",
    "1. [Understanding Pooling in Transformer Architecture, Aggregating Outputs for Downstream Tasks](https://www.datasciencebyexample.com/2023/04/30/what-is-pooling-in-transformer-model/)\n",
    "1. https://huggingface.co/docs/transformers/main_classes/output\n",
    "1. https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/output#transformers.utils.ModelOutput\n",
    "1. [Deep learning basics ‚Äî weight decay](https://medium.com/analytics-vidhya/deep-learning-basics-weight-decay-3c68eb4344e9)\n",
    "1. [How do you compare weight decay with other regularization methods for neural networks?\n",
    "](https://www.linkedin.com/advice/3/how-do-you-compare-weight-decay-other#:~:text=Weight%20decay%20is%20a%20form,them%20from%20growing%20too%20large.)\n",
    "1. [Zero-Weight Decay on BatchNorm and Bias\n",
    "](https://deci.ai/deep-learning-glossary/zero-weight-decay-on-batchnorm-and-bias/)\n",
    "1. [Various Optimization Algorithms For Training Neural Network\n",
    "](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6)\n",
    "1. [Optimizers in Deep Learning\n",
    "](https://medium.com/mlearning-ai/optimizers-in-deep-learning-7bf81fed78a0)\n",
    "1. [DATASETS & DATALOADERS\n",
    "](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n",
    "1. [An Introduction to Datasets and DataLoader in PyTorch\n",
    "](https://wandb.ai/sauravmaheshkar/Dataset-DataLoader/reports/An-Introduction-to-Datasets-and-DataLoader-in-PyTorch--VmlldzoxMDI5MTY2)\n",
    "1. [PyTorch DataLoader: Features, Benefits, and How to Use it\n",
    "](https://saturncloud.io/blog/pytorch-dataloader-features-benefits-and-how-to-use-it/#:~:text=The%20basic%20architecture%20of%20PyTorch%20DataLoader&text=The%20DataLoader%20class%20takes%20in,of%20data%20loading%20and%20preprocessing.)\n",
    "1. [A detailed example of how to generate your data in parallel with PyTorch\n",
    "](https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel)\n",
    "1. [PyTorch DataLoader: A Complete Guide](https://datagy.io/pytorch-dataloader/)\n",
    "1. [How does DataLoader work in PyTorch?](https://medium.com/noumena/how-does-dataloader-work-in-pytorch-8c363a8ee6c1)\n",
    "1. [How to use Datasets and DataLoader in PyTorch for custom text data\n",
    "](https://towardsdatascience.com/how-to-use-datasets-and-dataloader-in-pytorch-for-custom-text-data-270eed7f7c00)\n",
    "1. [Playing with PyTorch and Datasets\n",
    "](https://fede-bianchi.medium.com/playing-with-pytorch-and-datasets-fe64f5590f2)\n",
    "1. [Effective Data Handling with Custom PyTorch Dataset Classes\n",
    "](https://dantokeefe.medium.com/effective-data-handling-with-custom-pytorch-dataset-classes-b141bcb87b41)\n",
    "1. [TRAINING WITH PYTORCH\n",
    "](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#:~:text=The%20Dataset%20and%20DataLoader%20classes,processing%20single%20instances%20of%20data.)\n",
    "1. [Training a PyTorch Model with DataLoader and Dataset\n",
    "](https://machinelearningmastery.com/training-a-pytorch-model-with-dataloader-and-dataset/)\n",
    "1. [Cross-Validation in Machine Learning\n",
    "](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f)\n",
    "1. [Automatic Mixed Precision for Deep Learning](https://developer.nvidia.com/automatic-mixed-precision)\n",
    "\n",
    "\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:left;display:fill;border-radius:5px;background-color:#7b6b59;overflow:hidden\">QA</div>\n",
    "\n",
    "\n",
    "1. Do I need to tokenize the text before TF-IDF? If not what's the default tokenization that it takes place during Vectorization ? How the words get splitted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd76f1",
   "metadata": {
    "papermill": {
     "duration": 0.038576,
     "end_time": "2024-01-18T17:12:46.979598",
     "exception": false,
     "start_time": "2024-01-18T17:12:46.941022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7516023,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 4005256,
     "sourceId": 6977472,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 248.039196,
   "end_time": "2024-01-18T17:12:49.844775",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-18T17:08:41.805579",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00f878e253874b969e8d8a9446b6ec2e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "019498fb48a9403c846dc9d3cb214d3a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "02d1af7c0abc4e3286d64d0aa2cce210": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3d0d65d9c19f40dab94ffee3b382f3b1",
        "IPY_MODEL_58678ff669d64549b38d7e10960fac6b",
        "IPY_MODEL_5634382bb55444a3b8093416b6606efe"
       ],
       "layout": "IPY_MODEL_09970eabd6994be782f60405f050f258"
      }
     },
     "0442dff3206b46c7aedaa88235a12ce4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_fecd32294faf446ebb99b227c2a0642d",
        "IPY_MODEL_a35134fd33c6401ab32bc5d772d2ebeb",
        "IPY_MODEL_3ae8cb96077b4040b78a9890c0ed45fe"
       ],
       "layout": "IPY_MODEL_8bec4237b5eb456a86a7759c34c7cb28"
      }
     },
     "07142ad01e5b415a90d3092e9abe25d0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "075d6023444e47f7a799e2e5068f4f45": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0988a334c55d427c803638983c80afcb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "09970eabd6994be782f60405f050f258": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "09bbb243b9344e588089ea968719e57d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "0e89d5b0b07e46d6b2bd9b2e279ed62b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_83a4ec8380b54f7d96755db7f7475530",
        "IPY_MODEL_27566a80c52543f1bbb28f7b76362fde",
        "IPY_MODEL_9946c3afea824af893d080b2e3432990"
       ],
       "layout": "IPY_MODEL_2304b53ec3e2416684937e0564a6693b"
      }
     },
     "105cbbf38c3646f49277de47a4070c2f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_52f11c6a14e044b8b8f0a85122f3f385",
        "IPY_MODEL_44010445ed3946bcae356c5d8cbb62bc",
        "IPY_MODEL_13193def8cd649b3b6e0488d7b51a579"
       ],
       "layout": "IPY_MODEL_2cae6b0ea8664cc5abb7f06b61f65758"
      }
     },
     "11de62578a96431a9f0e799bbba90633": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "13084f32fa4c4373bf4c1fc87c5dcae2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "13193def8cd649b3b6e0488d7b51a579": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e1e61a7a09d44b259c5c6db42aace91c",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_b2b7a3ab3f9445ca9114044c5696107f",
       "value": " 2.46M/2.46M [00:00&lt;00:00, 19.3MB/s]"
      }
     },
     "13d6544cf37b4adfb85047326917e476": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_17db5fe602e44eeabc3451a15b291119",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_68e40e19542f4d1fadd915dbee9e9622",
       "value": "spm.model: 100%"
      }
     },
     "14679db7d905465fa8ddfd22dc9a98b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "14997de0fed24748a532f4e912ebd780": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "16d9873d880046b8a6b1e47d7e7c5394": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1722926c076a42fdafffdada8289cc26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1782c5aa624e4abda3b68dfb5a9f473b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_2f58f0e3b1cf44139e1c0b24b74e228a",
       "max": 52,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_09bbb243b9344e588089ea968719e57d",
       "value": 52
      }
     },
     "17db5fe602e44eeabc3451a15b291119": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1b92f024a5e24eb9a5ac5f736eef9209": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1fe323c9c9814b24b48530009c37acac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1ff44d203717403eadc3acde45e8a604": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_482a614697fa4eaab07c4b6c0e79de52",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_bec92ec6bfb74d52b7fbd707784c2350",
       "value": " 371M/371M [00:06&lt;00:00, 61.5MB/s]"
      }
     },
     "2195eda59b8a4586aac98376bd71dfb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_33a3f4f2da6b40cdbb804bd80d75ac4a",
        "IPY_MODEL_29eedaf738cc4bf89f881715f8b2ac4f",
        "IPY_MODEL_68e8c3dc571a4c339a46a4340e5211ca"
       ],
       "layout": "IPY_MODEL_6503b4ddb7794762b7f72964a27cf18e"
      }
     },
     "226a42c65dee4de1a7b32cddfe11cf09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_40777b0aa701483892f62e101d947f9c",
        "IPY_MODEL_f3e120020cc34046a3ff6dc417fb1555",
        "IPY_MODEL_55bf9abab7254f3a87ab12171d310fc9"
       ],
       "layout": "IPY_MODEL_bacc47f8b42e448a9bbe9d280bb8f104"
      }
     },
     "2304b53ec3e2416684937e0564a6693b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "24651529233049d797e7fc6a383eab79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "25b7722e4b0f4a3c81cd6baf2e78565e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2621cf10339445d9925be1f53681e700": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ff20e5e12b734612a8399e177400b58a",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_1b92f024a5e24eb9a5ac5f736eef9209",
       "value": "100%"
      }
     },
     "27566a80c52543f1bbb28f7b76362fde": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_29644f50a3c44b64bb3bc87c31b9b00c",
       "max": 483,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_7c0f4f7898864fbf92228caf56e28b78",
       "value": 483
      }
     },
     "29644f50a3c44b64bb3bc87c31b9b00c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "29eedaf738cc4bf89f881715f8b2ac4f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a5d2f47d87fd4720a7d6706a92452c93",
       "max": 41,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d8e57627a84e46ffb79540979bcf3dc5",
       "value": 41
      }
     },
     "2cae6b0ea8664cc5abb7f06b61f65758": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2f58f0e3b1cf44139e1c0b24b74e228a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "33a3f4f2da6b40cdbb804bd80d75ac4a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7e2f84747fef4003a1c4d97883bd1d43",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_1fe323c9c9814b24b48530009c37acac",
       "value": "100%"
      }
     },
     "33a78fae2863452aa2322cbe53c97efd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_46ef589e1c8b468f8f5679472e38df6a",
        "IPY_MODEL_917ca8f71e0345409df7934ee277a849",
        "IPY_MODEL_5bb5577c7ea741e1bb26876c6bf54daa"
       ],
       "layout": "IPY_MODEL_00f878e253874b969e8d8a9446b6ec2e"
      }
     },
     "35437984dcfa461db7cd473f85bfbdb3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3a9169363557445ebfc33c75963c21e8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3ae8cb96077b4040b78a9890c0ed45fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_57d45ea2b8de45bc9d62eefb0a02b768",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_8465cea0b4864b98a10d77a45ebe4292",
       "value": " 28.0/28.0 [00:00&lt;00:00, 2.04kB/s]"
      }
     },
     "3be93b9ba4594436af9d372fc96af0bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3d0d65d9c19f40dab94ffee3b382f3b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d04ee5751a7a4131b62af4f0c6ddb4e9",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_b34d9de013f840ba93f7de6164f22a72",
       "value": "config.json: 100%"
      }
     },
     "3d838017cc1549e19d7cc0755ff6b0eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d303205de2894a05a66590456748724f",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_edd8c5daa5874a46ac378bc9eef415fc",
       "value": " 2.46M/2.46M [00:00&lt;00:00, 105MB/s]"
      }
     },
     "3f8142eae53a4a2ea0dc6efc9ec9a19c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c058f4c586a940088812d56b8f4d02b1",
       "max": 2464616,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_78d7b1335e714130b5abe3b59fa20bce",
       "value": 2464616
      }
     },
     "3f850808568c4c818409d85815eeb080": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "40777b0aa701483892f62e101d947f9c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_513f35a227464756b8e971bc97d3498a",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_24651529233049d797e7fc6a383eab79",
       "value": "tokenizer.json: 100%"
      }
     },
     "40aef2af7449492694ba59d8d92c5de4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_96993f795b9047ab8a8fb7a6c7afe369",
        "IPY_MODEL_6637d0ccf93b44d4966b30366669021e",
        "IPY_MODEL_1ff44d203717403eadc3acde45e8a604"
       ],
       "layout": "IPY_MODEL_a62f831c803d4c29af2e4acac86e80c9"
      }
     },
     "410c527fcbb842289fc09e212fbdd4c2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "41c655b53ca349e1bd028dd730cba4fc": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "44010445ed3946bcae356c5d8cbb62bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_459ae6363bdd417b81487d667b7f99ee",
       "max": 2464616,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_9728af09b9624736a2b895f0074fee5f",
       "value": 2464616
      }
     },
     "459ae6363bdd417b81487d667b7f99ee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "46ef589e1c8b468f8f5679472e38df6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_14679db7d905465fa8ddfd22dc9a98b4",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_a0a1f47ac21d46ca88ae0fa749b14a1c",
       "value": "config.json: 100%"
      }
     },
     "47cb655af7c94168832990f1cfea40aa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "482a614697fa4eaab07c4b6c0e79de52": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "48d7b022b4ff42a0b42b38108840e825": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c2e10e192dbf4f3ba3dbd6f5806392a0",
        "IPY_MODEL_d4ba1acc40c14a58ae61e108b0d1b8b9",
        "IPY_MODEL_938433f415764117b28be60ebb3aafbe"
       ],
       "layout": "IPY_MODEL_af22977dfe08444c9f14e8cf87bc8c46"
      }
     },
     "49040db2f8e3450f8f6d14cec927756b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f5c6d887e061435cabb76685b7845341",
        "IPY_MODEL_d8f69634f2c14c9891c63f2d62e3e300",
        "IPY_MODEL_5b95ce6c12904373aaa21b320cb26fb7"
       ],
       "layout": "IPY_MODEL_41c655b53ca349e1bd028dd730cba4fc"
      }
     },
     "4db395b6120d4606a5ade5fae68975f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4dca366dc81841768bc28f7368a1c535": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b1e62f1d2a7d44beb3f9bc32642905e8",
        "IPY_MODEL_1782c5aa624e4abda3b68dfb5a9f473b",
        "IPY_MODEL_774f9d9019814d7986f2c81f2a320646"
       ],
       "layout": "IPY_MODEL_d196b4a2eb704e23999d5b1174e1d850"
      }
     },
     "4dde08a95900484196d493facf8ce29e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2621cf10339445d9925be1f53681e700",
        "IPY_MODEL_a6001206b32c4fa5995e8b77d242bf8b",
        "IPY_MODEL_fa4a7907b90740dd9a0e9b5432b11cd9"
       ],
       "layout": "IPY_MODEL_8cbbc97c942b46bbaf9957c8d79ce9d5"
      }
     },
     "4f4cd155d93e4f9f9125a2bb7179197f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "513f35a227464756b8e971bc97d3498a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5203db1b0dd1443a907e9128c6242dd2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "52f11c6a14e044b8b8f0a85122f3f385": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_14997de0fed24748a532f4e912ebd780",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_3f850808568c4c818409d85815eeb080",
       "value": "spm.model: 100%"
      }
     },
     "55bf9abab7254f3a87ab12171d310fc9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a61906dc77e340ee92e2267f5fe1e913",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_df4b5d1337ca463dbd47f419edbbe4a5",
       "value": " 466k/466k [00:00&lt;00:00, 26.6MB/s]"
      }
     },
     "5634382bb55444a3b8093416b6606efe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c854d5166c5f416098c775521491a3ef",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_11de62578a96431a9f0e799bbba90633",
       "value": " 579/579 [00:00&lt;00:00, 45.7kB/s]"
      }
     },
     "57d45ea2b8de45bc9d62eefb0a02b768": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "58678ff669d64549b38d7e10960fac6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3be93b9ba4594436af9d372fc96af0bd",
       "max": 579,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_35437984dcfa461db7cd473f85bfbdb3",
       "value": 579
      }
     },
     "59409e3d5dfd4ecfb2a88f2a524e7eea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5b95ce6c12904373aaa21b320cb26fb7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6c5d6a7898254947bdda1440d9645a65",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_677c149b6c184a3b99352c876f604cdb",
       "value": " 232k/232k [00:00&lt;00:00, 5.13MB/s]"
      }
     },
     "5bb5577c7ea741e1bb26876c6bf54daa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_c6c3c4201a704bbfaaa0d4da0879639e",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_69e85a75a1bc49cd9eb689353f60ae58",
       "value": " 578/578 [00:00&lt;00:00, 38.3kB/s]"
      }
     },
     "5d758219c28d465f937402abc2913423": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "638f70fe8e2c4f0281f4ba1e7c1cf9e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_76faa11726084e84a38a884c3924d7ef",
        "IPY_MODEL_7df588603ad94c4d8040e2ffd49051ae",
        "IPY_MODEL_76f12122e32047c1a5d6972dae6d84ea"
       ],
       "layout": "IPY_MODEL_beef2807beff4a67ae3b6441e69056ae"
      }
     },
     "6503b4ddb7794762b7f72964a27cf18e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6637d0ccf93b44d4966b30366669021e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_75b1f96d13674738bbf5209ae68388d8",
       "max": 371146213,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e37dbb44a86d404da0bd1ea56dba64fa",
       "value": 371146213
      }
     },
     "677c149b6c184a3b99352c876f604cdb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "68e40e19542f4d1fadd915dbee9e9622": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "68e8c3dc571a4c339a46a4340e5211ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8003fa9f52ec4492a9872cb6544137b1",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_07142ad01e5b415a90d3092e9abe25d0",
       "value": " 41/41 [00:37&lt;00:00,  1.41ba/s]"
      }
     },
     "69e85a75a1bc49cd9eb689353f60ae58": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6c5d6a7898254947bdda1440d9645a65": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6c9441ca68ab441d8fa948572cd2487b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "6c97bcfc2ff94b5a84075628bedbf642": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6eb5edec80d7435f89779b0b00fdca71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "75b1f96d13674738bbf5209ae68388d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "76f12122e32047c1a5d6972dae6d84ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e8d3280752d54792945d44d1879c7684",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_b8de2aa77ec64c8babc3ed34a6f767f1",
       "value": " 241M/241M [00:00&lt;00:00, 279MB/s]"
      }
     },
     "76faa11726084e84a38a884c3924d7ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_47cb655af7c94168832990f1cfea40aa",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_7eb71f0d20e44cce9eae3fddb1952601",
       "value": "pytorch_model.bin: 100%"
      }
     },
     "774f9d9019814d7986f2c81f2a320646": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9e214e2e230b4fe09cfcc6c4ec6ebd8c",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_6c9441ca68ab441d8fa948572cd2487b",
       "value": " 52.0/52.0 [00:00&lt;00:00, 3.88kB/s]"
      }
     },
     "789ab542539143d1a870cf7bc2daba70": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_16d9873d880046b8a6b1e47d7e7c5394",
       "max": 267954768,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_5203db1b0dd1443a907e9128c6242dd2",
       "value": 267954768
      }
     },
     "78d7b1335e714130b5abe3b59fa20bce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7c0f4f7898864fbf92228caf56e28b78": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "7df588603ad94c4d8040e2ffd49051ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_929dff2632244353bebd01188b85032e",
       "max": 241453931,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_59409e3d5dfd4ecfb2a88f2a524e7eea",
       "value": 241453931
      }
     },
     "7e2f84747fef4003a1c4d97883bd1d43": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7eb71f0d20e44cce9eae3fddb1952601": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8003fa9f52ec4492a9872cb6544137b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "83a4ec8380b54f7d96755db7f7475530": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d614cbd0ed7d4a83a0b3adfe175ab121",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_9ae6e01d893341e795eb312ece327bdd",
       "value": "config.json: 100%"
      }
     },
     "8465cea0b4864b98a10d77a45ebe4292": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "853a635794c54570a964af1bba2ddfc5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "86a2611f43a64a89b9e3d4b562f27960": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8bec4237b5eb456a86a7759c34c7cb28": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8cbbc97c942b46bbaf9957c8d79ce9d5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8f149246050a43ffa8c0da47cbd3f229": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "917ca8f71e0345409df7934ee277a849": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_25b7722e4b0f4a3c81cd6baf2e78565e",
       "max": 578,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_daf11c48d4a74dbd96bad391ba548728",
       "value": 578
      }
     },
     "91fbaddc1b2044ed92863f115c286289": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "929dff2632244353bebd01188b85032e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "938433f415764117b28be60ebb3aafbe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b56302260890428b9f41fe5f112c4c7b",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_8f149246050a43ffa8c0da47cbd3f229",
       "value": " 52.0/52.0 [00:00&lt;00:00, 4.13kB/s]"
      }
     },
     "959e13350e234f58b9386d4aa6dc5666": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "96993f795b9047ab8a8fb7a6c7afe369": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4f4cd155d93e4f9f9125a2bb7179197f",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_be41491dc5634b939f7b5b2a99055c9b",
       "value": "pytorch_model.bin: 100%"
      }
     },
     "96f049ecb641489e974f7e5ca8c01fd0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b599838a4f6c4905a2aa7b21e34a24c3",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_86a2611f43a64a89b9e3d4b562f27960",
       "value": "model.safetensors: 100%"
      }
     },
     "9728af09b9624736a2b895f0074fee5f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9946c3afea824af893d080b2e3432990": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_019498fb48a9403c846dc9d3cb214d3a",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_f7055b667226446c88aedcee215f0a20",
       "value": " 483/483 [00:00&lt;00:00, 35.9kB/s]"
      }
     },
     "9ae6e01d893341e795eb312ece327bdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9e214e2e230b4fe09cfcc6c4ec6ebd8c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9ed6f1e1917142229be6501d0a7be1f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a0a1f47ac21d46ca88ae0fa749b14a1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "a35134fd33c6401ab32bc5d772d2ebeb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f7d5fd5cc54c41118d746ae87260c8f0",
       "max": 28,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4db395b6120d4606a5ade5fae68975f1",
       "value": 28
      }
     },
     "a5d2f47d87fd4720a7d6706a92452c93": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a6001206b32c4fa5995e8b77d242bf8b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6c97bcfc2ff94b5a84075628bedbf642",
       "max": 5,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f161545cb4e54ee2a68515567d9e564b",
       "value": 5
      }
     },
     "a61906dc77e340ee92e2267f5fe1e913": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a62f831c803d4c29af2e4acac86e80c9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "aa4904360c8449a7952af0116765bb24": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "af22977dfe08444c9f14e8cf87bc8c46": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1e62f1d2a7d44beb3f9bc32642905e8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6eb5edec80d7435f89779b0b00fdca71",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_d44ff5570b3045e595950b25561b4b17",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "b2b7a3ab3f9445ca9114044c5696107f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b34d9de013f840ba93f7de6164f22a72": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b56302260890428b9f41fe5f112c4c7b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b599838a4f6c4905a2aa7b21e34a24c3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8de2aa77ec64c8babc3ed34a6f767f1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b97ac91070c84fdab923e521088d210a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bacc47f8b42e448a9bbe9d280bb8f104": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be41491dc5634b939f7b5b2a99055c9b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bec92ec6bfb74d52b7fbd707784c2350": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "beef2807beff4a67ae3b6441e69056ae": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c0345a09810344a097bc95499ccb3cce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_96f049ecb641489e974f7e5ca8c01fd0",
        "IPY_MODEL_789ab542539143d1a870cf7bc2daba70",
        "IPY_MODEL_d0a0664091944f7384adf5f15e2b709e"
       ],
       "layout": "IPY_MODEL_f5a64d34498f4bd4b000c33c8b5058f1"
      }
     },
     "c058f4c586a940088812d56b8f4d02b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c2e10e192dbf4f3ba3dbd6f5806392a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9ed6f1e1917142229be6501d0a7be1f3",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_e661cd2d5f624f7f9c4d99d5344f50f4",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "c4e2ca1d844343cfa4b8f78ab7ff052c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_13d6544cf37b4adfb85047326917e476",
        "IPY_MODEL_3f8142eae53a4a2ea0dc6efc9ec9a19c",
        "IPY_MODEL_3d838017cc1549e19d7cc0755ff6b0eb"
       ],
       "layout": "IPY_MODEL_b97ac91070c84fdab923e521088d210a"
      }
     },
     "c6c3c4201a704bbfaaa0d4da0879639e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c854d5166c5f416098c775521491a3ef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cb21e25400214adfb8cf39a43b854af6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d04ee5751a7a4131b62af4f0c6ddb4e9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d0a0664091944f7384adf5f15e2b709e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1722926c076a42fdafffdada8289cc26",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_cb21e25400214adfb8cf39a43b854af6",
       "value": " 268M/268M [00:01&lt;00:00, 212MB/s]"
      }
     },
     "d196b4a2eb704e23999d5b1174e1d850": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d303205de2894a05a66590456748724f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d44ff5570b3045e595950b25561b4b17": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d4ba1acc40c14a58ae61e108b0d1b8b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5d758219c28d465f937402abc2913423",
       "max": 52,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_853a635794c54570a964af1bba2ddfc5",
       "value": 52
      }
     },
     "d614cbd0ed7d4a83a0b3adfe175ab121": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8e57627a84e46ffb79540979bcf3dc5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d8f69634f2c14c9891c63f2d62e3e300": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3a9169363557445ebfc33c75963c21e8",
       "max": 231508,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f1e69c4ed34a4214bd70d01d375ac848",
       "value": 231508
      }
     },
     "daf11c48d4a74dbd96bad391ba548728": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "de63afeba41744e299475fd29f76bbae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "df4b5d1337ca463dbd47f419edbbe4a5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e1e61a7a09d44b259c5c6db42aace91c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e37dbb44a86d404da0bd1ea56dba64fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e661cd2d5f624f7f9c4d99d5344f50f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e8d3280752d54792945d44d1879c7684": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "edd8c5daa5874a46ac378bc9eef415fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f161545cb4e54ee2a68515567d9e564b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f1e69c4ed34a4214bd70d01d375ac848": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f3e120020cc34046a3ff6dc417fb1555": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_959e13350e234f58b9386d4aa6dc5666",
       "max": 466062,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_410c527fcbb842289fc09e212fbdd4c2",
       "value": 466062
      }
     },
     "f5a64d34498f4bd4b000c33c8b5058f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5c6d887e061435cabb76685b7845341": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_13084f32fa4c4373bf4c1fc87c5dcae2",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_de63afeba41744e299475fd29f76bbae",
       "value": "vocab.txt: 100%"
      }
     },
     "f7055b667226446c88aedcee215f0a20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f7d5fd5cc54c41118d746ae87260c8f0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fa4a7907b90740dd9a0e9b5432b11cd9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_aa4904360c8449a7952af0116765bb24",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_91fbaddc1b2044ed92863f115c286289",
       "value": " 5/5 [00:05&lt;00:00,  1.07s/ba]"
      }
     },
     "fecd32294faf446ebb99b227c2a0642d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_075d6023444e47f7a799e2e5068f4f45",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_0988a334c55d427c803638983c80afcb",
       "value": "tokenizer_config.json: 100%"
      }
     },
     "ff20e5e12b734612a8399e177400b58a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
