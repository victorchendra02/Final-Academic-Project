{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\envs\\artofproblemsolving\\lib\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35.2\n",
      "2.10.1\n",
      "2.10.0\n",
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import tensorflow\n",
    "import keras\n",
    "import torch\n",
    "\n",
    "print(transformers.__version__)\n",
    "print(tensorflow.__version__)\n",
    "print(keras.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algebra       = 683\n",
      "Combinatorics = 565\n",
      "Geometry      = 962\n",
      "Number Theory = 661\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"imo.csv\")\n",
    "# Select not null data\n",
    "df_notna = df[df[\"label\"].notna()]\n",
    "# Reset index\n",
    "df_notna.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# DISPLAY INFORMATION DATASET\n",
    "algebra = df_notna[df_notna['label'] == 'Algebra'].shape[0]\n",
    "combinatorics = df_notna[df_notna['label'] == 'Combinatorics'].shape[0]\n",
    "geometry = df_notna[df_notna['label'] == 'Geometry'].shape[0]\n",
    "number_theory = df_notna[df_notna['label'] == 'Number Theory'].shape[0]\n",
    "\n",
    "print(f\"Algebra       = {algebra}\")\n",
    "print(f\"Combinatorics = {combinatorics}\")\n",
    "print(f\"Geometry      = {geometry}\")\n",
    "print(f\"Number Theory = {number_theory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_key</th>\n",
       "      <th>no</th>\n",
       "      <th>year</th>\n",
       "      <th>post_rendered</th>\n",
       "      <th>post_canonical</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023imo</td>\n",
       "      <td>Determine all composite integers &lt;img src=\"//l...</td>\n",
       "      <td>Determine all composite integers $n&gt;1$ that sa...</td>\n",
       "      <td>Number Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2023imo</td>\n",
       "      <td>Let &lt;img src=\"//latex.artofproblemsolving.com/...</td>\n",
       "      <td>Let $ABC$ be an acute-angled triangle with $AB...</td>\n",
       "      <td>Geometry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2023imo</td>\n",
       "      <td>For each integer &lt;span style=\"white-space:pre;...</td>\n",
       "      <td>For each integer $k \\geqslant 2$, determine al...</td>\n",
       "      <td>Number Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2023imo</td>\n",
       "      <td>Let &lt;img src=\"//latex.artofproblemsolving.com/...</td>\n",
       "      <td>Let $x_1,x_2,\\dots,x_{2023}$ be pairwise diffe...</td>\n",
       "      <td>Algebra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2023imo</td>\n",
       "      <td>Let &lt;img src=\"//latex.artofproblemsolving.com/...</td>\n",
       "      <td>Let $n$ be a positive integer. A Japanese tria...</td>\n",
       "      <td>Combinatorics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2866</th>\n",
       "      <td>10938</td>\n",
       "      <td>2</td>\n",
       "      <td>1997_tuymaada_olympiad</td>\n",
       "      <td>Solve in natural numbers the system of equatio...</td>\n",
       "      <td>Solve in natural numbers the system of equatio...</td>\n",
       "      <td>Number Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2867</th>\n",
       "      <td>10941</td>\n",
       "      <td>5</td>\n",
       "      <td>1997_tuymaada_olympiad</td>\n",
       "      <td>Prove the inequality &lt;img src=\"//latex.artofpr...</td>\n",
       "      <td>Prove the inequality $\\left(1+\\frac{1}{q}\\righ...</td>\n",
       "      <td>Algebra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>10943</td>\n",
       "      <td>7</td>\n",
       "      <td>1997_tuymaada_olympiad</td>\n",
       "      <td>It is known that every student of the class fo...</td>\n",
       "      <td>It is known that every student of the class fo...</td>\n",
       "      <td>Combinatorics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>10955</td>\n",
       "      <td>3</td>\n",
       "      <td>1995_tuymaada_olympiad</td>\n",
       "      <td>Prove that the equation  &lt;img src=\"//latex.art...</td>\n",
       "      <td>Prove that the equation $(\\sqrt5 +1)^{2x}+ (\\s...</td>\n",
       "      <td>Number Theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2870</th>\n",
       "      <td>10959</td>\n",
       "      <td>7</td>\n",
       "      <td>1995_tuymaada_olympiad</td>\n",
       "      <td>Find a continuous function &lt;img src=\"//latex.a...</td>\n",
       "      <td>Find a continuous function $f(x)$ satisfying t...</td>\n",
       "      <td>Algebra</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2871 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_key no                    year  \\\n",
       "0          1  1                 2023imo   \n",
       "1          2  2                 2023imo   \n",
       "2          3  3                 2023imo   \n",
       "3          4  4                 2023imo   \n",
       "4          5  5                 2023imo   \n",
       "...      ... ..                     ...   \n",
       "2866   10938  2  1997_tuymaada_olympiad   \n",
       "2867   10941  5  1997_tuymaada_olympiad   \n",
       "2868   10943  7  1997_tuymaada_olympiad   \n",
       "2869   10955  3  1995_tuymaada_olympiad   \n",
       "2870   10959  7  1995_tuymaada_olympiad   \n",
       "\n",
       "                                          post_rendered  \\\n",
       "0     Determine all composite integers <img src=\"//l...   \n",
       "1     Let <img src=\"//latex.artofproblemsolving.com/...   \n",
       "2     For each integer <span style=\"white-space:pre;...   \n",
       "3     Let <img src=\"//latex.artofproblemsolving.com/...   \n",
       "4     Let <img src=\"//latex.artofproblemsolving.com/...   \n",
       "...                                                 ...   \n",
       "2866  Solve in natural numbers the system of equatio...   \n",
       "2867  Prove the inequality <img src=\"//latex.artofpr...   \n",
       "2868  It is known that every student of the class fo...   \n",
       "2869  Prove that the equation  <img src=\"//latex.art...   \n",
       "2870  Find a continuous function <img src=\"//latex.a...   \n",
       "\n",
       "                                         post_canonical          label  \n",
       "0     Determine all composite integers $n>1$ that sa...  Number Theory  \n",
       "1     Let $ABC$ be an acute-angled triangle with $AB...       Geometry  \n",
       "2     For each integer $k \\geqslant 2$, determine al...  Number Theory  \n",
       "3     Let $x_1,x_2,\\dots,x_{2023}$ be pairwise diffe...        Algebra  \n",
       "4     Let $n$ be a positive integer. A Japanese tria...  Combinatorics  \n",
       "...                                                 ...            ...  \n",
       "2866  Solve in natural numbers the system of equatio...  Number Theory  \n",
       "2867  Prove the inequality $\\left(1+\\frac{1}{q}\\righ...        Algebra  \n",
       "2868  It is known that every student of the class fo...  Combinatorics  \n",
       "2869  Prove that the equation $(\\sqrt5 +1)^{2x}+ (\\s...  Number Theory  \n",
       "2870  Find a continuous function $f(x)$ satisfying t...        Algebra  \n",
       "\n",
       "[2871 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_18072\\2941989690.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_notna['label_encoded'] = le.fit_transform(df_notna['label'])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_key</th>\n",
       "      <th>no</th>\n",
       "      <th>year</th>\n",
       "      <th>post_rendered</th>\n",
       "      <th>post_canonical</th>\n",
       "      <th>label</th>\n",
       "      <th>label_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2023imo</td>\n",
       "      <td>Determine all composite integers &lt;img src=\"//l...</td>\n",
       "      <td>Determine all composite integers $n&gt;1$ that sa...</td>\n",
       "      <td>Number Theory</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2023imo</td>\n",
       "      <td>Let &lt;img src=\"//latex.artofproblemsolving.com/...</td>\n",
       "      <td>Let $ABC$ be an acute-angled triangle with $AB...</td>\n",
       "      <td>Geometry</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2023imo</td>\n",
       "      <td>For each integer &lt;span style=\"white-space:pre;...</td>\n",
       "      <td>For each integer $k \\geqslant 2$, determine al...</td>\n",
       "      <td>Number Theory</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2023imo</td>\n",
       "      <td>Let &lt;img src=\"//latex.artofproblemsolving.com/...</td>\n",
       "      <td>Let $x_1,x_2,\\dots,x_{2023}$ be pairwise diffe...</td>\n",
       "      <td>Algebra</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2023imo</td>\n",
       "      <td>Let &lt;img src=\"//latex.artofproblemsolving.com/...</td>\n",
       "      <td>Let $n$ be a positive integer. A Japanese tria...</td>\n",
       "      <td>Combinatorics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2866</th>\n",
       "      <td>10938</td>\n",
       "      <td>2</td>\n",
       "      <td>1997_tuymaada_olympiad</td>\n",
       "      <td>Solve in natural numbers the system of equatio...</td>\n",
       "      <td>Solve in natural numbers the system of equatio...</td>\n",
       "      <td>Number Theory</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2867</th>\n",
       "      <td>10941</td>\n",
       "      <td>5</td>\n",
       "      <td>1997_tuymaada_olympiad</td>\n",
       "      <td>Prove the inequality &lt;img src=\"//latex.artofpr...</td>\n",
       "      <td>Prove the inequality $\\left(1+\\frac{1}{q}\\righ...</td>\n",
       "      <td>Algebra</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>10943</td>\n",
       "      <td>7</td>\n",
       "      <td>1997_tuymaada_olympiad</td>\n",
       "      <td>It is known that every student of the class fo...</td>\n",
       "      <td>It is known that every student of the class fo...</td>\n",
       "      <td>Combinatorics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>10955</td>\n",
       "      <td>3</td>\n",
       "      <td>1995_tuymaada_olympiad</td>\n",
       "      <td>Prove that the equation  &lt;img src=\"//latex.art...</td>\n",
       "      <td>Prove that the equation $(\\sqrt5 +1)^{2x}+ (\\s...</td>\n",
       "      <td>Number Theory</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2870</th>\n",
       "      <td>10959</td>\n",
       "      <td>7</td>\n",
       "      <td>1995_tuymaada_olympiad</td>\n",
       "      <td>Find a continuous function &lt;img src=\"//latex.a...</td>\n",
       "      <td>Find a continuous function $f(x)$ satisfying t...</td>\n",
       "      <td>Algebra</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2871 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id_key no                    year  \\\n",
       "0          1  1                 2023imo   \n",
       "1          2  2                 2023imo   \n",
       "2          3  3                 2023imo   \n",
       "3          4  4                 2023imo   \n",
       "4          5  5                 2023imo   \n",
       "...      ... ..                     ...   \n",
       "2866   10938  2  1997_tuymaada_olympiad   \n",
       "2867   10941  5  1997_tuymaada_olympiad   \n",
       "2868   10943  7  1997_tuymaada_olympiad   \n",
       "2869   10955  3  1995_tuymaada_olympiad   \n",
       "2870   10959  7  1995_tuymaada_olympiad   \n",
       "\n",
       "                                          post_rendered  \\\n",
       "0     Determine all composite integers <img src=\"//l...   \n",
       "1     Let <img src=\"//latex.artofproblemsolving.com/...   \n",
       "2     For each integer <span style=\"white-space:pre;...   \n",
       "3     Let <img src=\"//latex.artofproblemsolving.com/...   \n",
       "4     Let <img src=\"//latex.artofproblemsolving.com/...   \n",
       "...                                                 ...   \n",
       "2866  Solve in natural numbers the system of equatio...   \n",
       "2867  Prove the inequality <img src=\"//latex.artofpr...   \n",
       "2868  It is known that every student of the class fo...   \n",
       "2869  Prove that the equation  <img src=\"//latex.art...   \n",
       "2870  Find a continuous function <img src=\"//latex.a...   \n",
       "\n",
       "                                         post_canonical          label  \\\n",
       "0     Determine all composite integers $n>1$ that sa...  Number Theory   \n",
       "1     Let $ABC$ be an acute-angled triangle with $AB...       Geometry   \n",
       "2     For each integer $k \\geqslant 2$, determine al...  Number Theory   \n",
       "3     Let $x_1,x_2,\\dots,x_{2023}$ be pairwise diffe...        Algebra   \n",
       "4     Let $n$ be a positive integer. A Japanese tria...  Combinatorics   \n",
       "...                                                 ...            ...   \n",
       "2866  Solve in natural numbers the system of equatio...  Number Theory   \n",
       "2867  Prove the inequality $\\left(1+\\frac{1}{q}\\righ...        Algebra   \n",
       "2868  It is known that every student of the class fo...  Combinatorics   \n",
       "2869  Prove that the equation $(\\sqrt5 +1)^{2x}+ (\\s...  Number Theory   \n",
       "2870  Find a continuous function $f(x)$ satisfying t...        Algebra   \n",
       "\n",
       "      label_encoded  \n",
       "0                 3  \n",
       "1                 2  \n",
       "2                 3  \n",
       "3                 0  \n",
       "4                 1  \n",
       "...             ...  \n",
       "2866              3  \n",
       "2867              0  \n",
       "2868              1  \n",
       "2869              3  \n",
       "2870              0  \n",
       "\n",
       "[2871 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace label with integer values (encode)\n",
    "display(df_notna[[\"id_key\", \"no\", \"year\", \"post_rendered\", \"post_canonical\", \"label\"]])\n",
    "\n",
    "le = LabelEncoder()\n",
    "df_notna['label_encoded'] = le.fit_transform(df_notna['label'])\n",
    "\n",
    "display(df_notna[[\"id_key\", \"no\", \"year\", \"post_rendered\", \"post_canonical\", \"label\", \"label_encoded\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Algebra': 0, 'Combinatorics': 1, 'Geometry': 2, 'Number Theory': 3}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to map encoded labels back to original labels\n",
    "label_map_rev = {label: i for i, label in enumerate(le.classes_)}\n",
    "label_map_rev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_18072\\1720640278.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_notna[\"post_canonical\"] = df_notna[\"post_canonical\"].str.lower()\n"
     ]
    }
   ],
   "source": [
    "# Preprocess text (optional)\n",
    "df_notna[\"post_canonical\"] = df_notna[\"post_canonical\"].str.lower()\n",
    "# Consider other preprocessing steps\n",
    "\n",
    "# Split X, y dataset\n",
    "X, y = df_notna[\"post_canonical\"], df_notna[\"label_encoded\"]\n",
    "\n",
    "# Split train & test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# DISPLAY DATA (observe column 'post_canonical') AFTER PREPROCESSED\n",
    "# display(df_notna[[\"id_key\", \"no\", \"year\", \"post_rendered\", \"post_canonical\", \"label\", \"label_encoded\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize inputs\n",
    "max_length = 128\n",
    "train_encodings = tokenizer(list(X_train), truncation=True, padding=True, max_length=max_length)\n",
    "test_encodings = tokenizer(list(X_test), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_input_ids = torch.tensor(train_encodings['input_ids'])\n",
    "train_attention_masks = torch.tensor(train_encodings['attention_mask'])\n",
    "train_labels = torch.tensor(y_train.values)\n",
    "\n",
    "test_input_ids = torch.tensor(test_encodings['input_ids'])\n",
    "test_attention_masks = torch.tensor(test_encodings['attention_mask'])\n",
    "test_labels = torch.tensor(y_test.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\anaconda3\\envs\\artofproblemsolving\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 3\n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "\"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m input_ids, attention_masks, labels \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device), attention_masks\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m----> 7\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_masks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m      9\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\artofproblemsolving\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\artofproblemsolving\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\artofproblemsolving\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1599\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1598\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m-> 1599\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1600\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mproblem_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulti_label_classification\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1601\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m BCEWithLogitsLoss()\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\artofproblemsolving\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\artofproblemsolving\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\artofproblemsolving\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\artofproblemsolving\\lib\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: \"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Int'"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch in train_loader:\n",
    "        input_ids, attention_masks, labels = batch\n",
    "        input_ids, attention_masks, labels = input_ids.to(device), attention_masks.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes=max(df_notna['label'])+1)\n",
    "y_test = to_categorical(y_test, num_classes=max(df_notna['label'])+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT and the preprocessing model from TF Hub.\n",
    "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Layers\n",
    "\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "print(outputs['pooled_output'].shape)\n",
    "\n",
    "# Neural Network Layers\n",
    "first = tf.keras.layers.Dropout(0.1,name='dropout')(outputs['pooled_output'])\n",
    "second = tf.keras.layers.Dense(4,activation='softmax', name='output')(first)\n",
    "\n",
    "# Construct the final model\n",
    "model = tf.keras.Model(inputs=[text_input], outputs=[second])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number Theory': 0, 'Geometry': 1, 'Algebra': 2, 'Combinatorics': 3}\n",
      "1423    2\n",
      "958     0\n",
      "605     3\n",
      "2561    3\n",
      "612     1\n",
      "       ..\n",
      "1638    1\n",
      "1095    1\n",
      "1130    0\n",
      "1294    2\n",
      "860     2\n",
      "Name: label, Length: 2296, dtype: int64\n",
      "729     3\n",
      "2067    1\n",
      "1288    2\n",
      "1738    0\n",
      "2078    0\n",
      "       ..\n",
      "2597    1\n",
      "163     1\n",
      "1913    2\n",
      "252     1\n",
      "1801    0\n",
      "Name: label, Length: 575, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", padding=True)\n",
    "\n",
    "# Create dataset classes with padding\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, labels):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.text[idx]\n",
    "        label = self.labels[idx]\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(), \n",
    "            'attention_mask': inputs['attention_mask'].squeeze()\n",
    "            }, label\n",
    "\n",
    "# Create datasets with padding\n",
    "train_dataset = TextDataset(X_train.tolist(), y_train.tolist())\n",
    "test_dataset = TextDataset(X_test.tolist(), y_test.tolist())\n",
    "\n",
    "# Encode labels (after splitting for consistency)\n",
    "label_encoder = {label: i for i, label in enumerate(df_notna[\"label\"].unique())}\n",
    "y_train = y_train.apply(lambda x: label_encoder[x])\n",
    "y_test = y_test.apply(lambda x: label_encoder[x])\n",
    "\n",
    "print(label_encoder)\n",
    "print(y_train)\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_encoder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mdata_collator)\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     12\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, labels\u001b[38;5;241m=\u001b[39mlabels)\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\artofproblemsolving\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\artofproblemsolving\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\artofproblemsolving\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\artofproblemsolving\\lib\\site-packages\\transformers\\data\\data_collator.py:249\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, features: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m--> 249\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[0;32m    257\u001b[0m         batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\victo\\anaconda3\\envs\\artofproblemsolving\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3020\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3016\u001b[0m \u001b[38;5;66;03m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[0;32m   3017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m encoded_inputs:\n\u001b[0;32m   3018\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3019\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 3020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat includes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but you provided \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[43mencoded_inputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3021\u001b[0m     )\n\u001b[0;32m   3023\u001b[0m required_input \u001b[38;5;241m=\u001b[39m encoded_inputs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_input_names[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m   3025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m required_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(required_input, Sized) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(required_input) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "# Set up training\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Define data collator for padding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Train model\n",
    "for epoch in range(3):  # Adjust number of epochs\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=data_collator)\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        inputs, labels = batch\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "model.eval()\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "predictions = []\n",
    "true_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        inputs, labels = batch\n",
    "        outputs = model(**inputs)\n",
    "        predicted_labels = torch.argmax(outputs.logits, dim=1)\n",
    "        predictions.extend(predicted_labels.tolist())\n",
    "        true_labels.extend(labels.tolist())\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Accuracy:\", accuracy_score(true_labels, predictions))\n",
    "print(\"F1 Score:\", f1_score(true_labels, predictions, average=\"weighted\"))\n",
    "print(\"Precision:\", precision_score(true_labels, predictions, average=\"weighted\"))\n",
    "print(\"Recall:\", recall_score(true_labels, predictions, average=\"weighted\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artofproblemsolving",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
